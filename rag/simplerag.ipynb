{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a28bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Life.. is so short Life only has only a little time Although it will continue to go on you cannot hold on and think that every day is promised to you We have all experienced great losses in our life. We have all felt that pain of losing a loved one, someone that we cared about.\\n\\nBut yet weâ€™re still here and now we must go on But what is the example that weâ€™re going to leave? What kind of leadership and what kind of leadership qualities do we have? How do we go on? How do we lead the next generation? Hate is not going to make it work Being afraid to be truthful to who you are will only limit who you truly are inside.\\n\\nDonâ€™t let anyone take that away from you donâ€™t let anyone take away who you are and how true you around what you matter in this world to so many other people Donâ€™t be afraid, to be honest. Donâ€™t be afraid to be truthful. Donâ€™t be afraid to be different. Even being different can be difficult for a lot of people.\\n\\nBut I guarantee you this, thereâ€™s nobody in the world that can do you. When the time comes what would you leave behind? What legacy will be left behind to remind others of your greatness, of your losses, of your victories, of your sorrows? This is a short life that we all have. And itâ€™s not easy, itâ€™s not easy living it every day.\\n\\nItâ€™s not easy to go through so many different circumstances, so many different challenges. Itâ€™s not easy getting that pink slip Knowing that this may be your last day on your job. Itâ€™s not easy knowing that you may lose your home because you got laid off your job.\\n\\nThis type of pain and these different circumstances many people are always going to come back and say, well thatâ€™s the life We cannot blame life Itâ€™s not the life that makes these challenges what they are today. It is the purpose and purpose never lies it will always tell you the truth.\\n\\nBut while you exist in this world right now I need you to hold on, I need you to hold on strong and donâ€™t give up, I need you to believe in every possibility that you have and understand that it is not over for you. I need you to understand life is always going to be good.\\n\\nBut always keep in mind sooner or later We all got to punch that clock. So carry on and donâ€™t give up and donâ€™t give in and do the best that you can to have the right attitude, to make your existence matter. Your existence is not by accident. Your existence has so much meaning Are you ready to take on the unknown? Are you prepared for it? You canâ€™t even understand in most cases how beautiful it is sometimes to not even know what's coming your way, sometimes itâ€™s not necessary to know everything, sometimes itâ€™s not necessary to understand everything. But are you ready to embrace the fact that someday life as you know it, life as I know it will soon come to an end?\\n\\nNow, what are you going to do about what youâ€™re doing with the life that you have right now? See I talk about life because life is such a great thing, life is such a beautiful thing Life has so much university to it. It is unexplained to some people, it is unimaginable to some people. Sometimes you may even ask yourself I donâ€™t even deserve this life.\\n\\nSometimes you may even ask yourself why am I suffering so much from this and that person over there is not suffering as much as I am? Ladies and gentlemen Life, your existence itâ€™s temporary, it wonâ€™t last forever, and if youâ€™re sitting around wasting it if youâ€™re sitting around being hateful if sitting around being jealous. If youâ€™re letting things outside of your circle control your possibilities, your uniqueness, your qualities your principles, who you really are inside then whatâ€™s going to happen next? There will come a time when you leave this world and theyâ€™re going to put you in that hole and guess what ladies and gentlemen? No one is going to jump in that hole with you.\\n\\nNo one is going to jump in there with you and celebrate that death that you just experienced. Yes, I know it may sound a little harsh. In fact, it may even scare you but you canâ€™t fake this, you canâ€™t hide from this. Life is short and the only thing that's beautiful about it is that when you live and you have strong possibilities, you have something that you can be doing to make it impactful for others to see. You canâ€™t stop, you canâ€™t stop living, you canâ€™t stop growing, you canâ€™t stop fighting.\\n\\nYou have to understand that yesterday and today and tomorrow are in three different universes. And the only universe you exist in right now Is the now, in this moment, is this reality. You canâ€™t fix anything thatâ€™s already been broken. Some things can mend in time. And somethings are better off left alone. There are a lot of negative people right now that exists in this world. there are people that are Reading to this message right now. And yet, theyâ€™ll still find something that is not making them happy. You must make peace within your heart. You must make peace within your spirit. Life.. is so short.\")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text = loader.load()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4ca85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f99acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Load chunk and indedx the content of html page\n",
    "loader = WebBaseLoader(web_path=(\"https://lilianweng.github.io/posts/2025-05-01-thinking/\"),\n",
    "                       bs_kwargs=dict(parse_only= bs4.SoupStrainer(class_=(\"post-header\",\"post-content\",\"post-header\"))))\n",
    "text_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b8eda0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/'}, page_content='\\n\\n      Why We Think\\n    \\nDate: May 1, 2025  |  Estimated Reading Time: 40 min  |  Author: Lilian Weng\\n\\n\\nSpecial thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.\\nMotivation#\\nEnabling models to think for longer can be motivated in a few different ways.\\nAnalogy to Psychology#\\nThe core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \"What\\'s 12345 times 56789?\". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\\n\\nFast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\\nSlow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.\\n\\nBecause System 1 thinking is fast and easy, it often ends up being the main decision driver, at the cost of accuracy and logic. It naturally relies on our brain’s mental shortcuts (i.e., heuristics) and can lead to errors and biases. By consciously slowing down and taking more time to reflect, improve and analyze, we can engage in System 2 thinking to challenge our instincts and make more rational choices.\\nComputation as a Resource#\\nOne view of deep learning, is that neural networks can be characterized by the amount of computation and storage they can access in a forward pass, and if we optimize them to solve problems using gradient descent, the optimization process will figure out how to use these resources–they’ll figure out how to organize these resources into circuits for calculation and information storage. From this view, if we design an architecture or system that can do more computation at test time, and we train it to effectively use this resource, it’ll work better.\\nIn Transformer models, the amount of computation (flops) that the model does for each generated token is roughly 2 times the number of parameters. For sparse models like mixture of experts (MoE), only a fraction of the parameters are used in each forward pass, so computation = 2 * parameters / sparsity, where sparsity is the fraction of experts active.\\nOn the other hand, CoT enables the model to perform far more flops of computation for each token of the answer that it is trying to compute. In fact, CoT has a nice property that it allows the model to use a variable amount of compute depending on the hardness of the problem.\\nLatent Variable Modeling#\\nA classic idea in machine learning is to define a probabilistic model with a latent (hidden) variable $z$ and a visible variable $y$, where $y$ is given to our learning algorithm. Marginalizing (summing) over the possible values of the latent variable allows us to express a rich distribution over the visible variables, $P(y) = \\\\sum_{z \\\\sim P(z)} P(y \\\\mid z)$. For example, we can model the distribution over math problems and solutions by letting $x$ denote a problem statement, $y$ be ground truth answer or proof, and $z$ as a free-form thought process that leads to the proof. The marginal probability distribution to optimize would be $P(y \\\\mid x) = \\\\sum_{z \\\\sim p(z\\\\mid x)} P(y \\\\mid x, z)$\\nThe latent variable perspective is particularly useful for understanding methods that involve collecting multiple parallel CoTs or searching over the CoT–these algorithms can be seen as sampling from the posterior $P(z \\\\mid x, y)$. This view also suggests the benefits of using the log loss $\\\\log P(y \\\\mid x)$ as the target objective to optimize, as the log loss objective has been so effective in pretraining.\\nThinking in Tokens#\\nThe strategy of generating intermediate steps before generating short answers, particularly for math problems, was explored by Ling, et al. 2017, who introduced the AQUA-RAT dataset, and then expanded by Cobbe et al. 2021, who introduced the Grade School Math (GSM) dataset. Cobbe et al. train a generator with supervised learning on human-written solutions and verifiers that predict the correctness of a candidate solution; they can then search over these solutions. Nye et al. (2021) experimented with intermediate thinking tokens as “scratchpads” and Wei et al. (2022) coined the now-standard term chain-of-thought (CoT).\\nEarly work on improving CoT reasoning involved doing supervised learning on human-written reasoning traces or model-written traces filtered for answer correctness, where the latter can be seen as a rudimentary form of reinforcement learning (RL). Some other work found that one could significantly boost math performance of instruction tuned models by prompting them appropriately, with \"think step by step\" (Kojima et al. 2022) or more complex prompting to encourage the model to reflect on related knowledge first (Yasunaga et al. 2023).\\nLater work found that the CoT reasoning capabilities can be significantly improved by doing reinforcement learning on a dataset of problems with automatically checkable solutions, such as STEM problems with short answers, or coding tasks that can be checked with unit tests (Zelikman et al. 2022, Wang et al., 2023, Liu et al., 2023). This approach rose to prominence with the announcement of o1-preview, o3, and the R1 tech report (DeepSeek-AI, 2025), which showed that a simple recipe where a policy gradient algorithm could lead to strong performance.\\n\\n\\nChain-of-thought prompting leads to higher success rate of solving math problems. Larger models benefit more from thinking time. (Image source: Wei et al. 2022)\\n\\nBranching and Editing#\\nThe fundamental intent of test-time compute is to adaptively modify the model’s output distribution at test time. There are various ways of utilizing test time resources for decoding to select better samples and thus alter the model’s predictions towards a more desired distribution. Two main approaches for improving the decoding process are parallel sampling and sequential revision.\\n\\nParallel sampling generates multiple outputs simultaneously, meanwhile providing guidance per step with process reward signals or using verifiers to judge the quality at the end. It is the most widely adopted decoding method to improve test time performance, such as best-of-$N$ or beam search. Self-consistency (Wang et al. 2023) is commonly used to select the answer with majority vote among multiple CoT rollouts when the ground truth is not available.\\nSequential revision adapts the model’s responses iteratively based on the output in the previous step, asking the model to intentionally reflect its existing response and correct mistakes. The revision process may have to rely on a fine-tuned model, as naively relying on the model’s intrinsic capability of self-correction without external feedback may not lead to improvement (Kamoi et al. 2024, Huang et al. 2024).\\n\\nParallel sampling is simple, intuitive and easier to implement, but bounded by the model capability of whether it can achieve the correct solution in one-go. Sequential explicitly asks the model to reflect on mistakes but it is slower and requires extra care during implementation as it does run the risk of correct predictions being modified to be incorrect or introducing other types of hallucinations. These two methods can be used together. Snell et al. (2024) showed that easier questions benefit from purely sequential test-time compute, whereas harder questions often perform best with an optimal ratio of sequential to parallel compute.\\n\\n\\nIllustration of parallel sampling vs sequential revision.\\n\\nParallel Sampling#\\nGiven a generative model and a scoring function that we can use to score full or partial samples, there are various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive, spending more sampling computation on more promising parts of the solution space.\\nBeam search maintains a set of promising partial sequences and alternates between extending them and pruning the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al. 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own generated reasoning step is correct, formatted as a multiple-choice question and found that per-step self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides, during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to determine how much each node should be expanded at each depth during beam search, according to the softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the difference between when the rationales is included in the context vs not. At inference time, RATIONALYST provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).\\n\\n\\nBeam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)\\n\\nInterestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting the model further with \"So the answer is\". The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.\\n\\n\\nTop-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang & Zhou, 2024)\\n\\nSequential Revision#\\nIf the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al. 2023).\\nSelf-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\\\theta(y \\\\mid y_0, x)$ given a fixed generator model $P_0(y_0 \\\\mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):\\n\\nSelf-correction learning first generates first generates multiple outputs per prompt in the data pool;\\nthen create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).\\nThese pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\\\\text{Similarity}(y, y’)$ to train the corrector model.\\nTo encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.\\n\\n\\n\\nIllustration of self-correction learning by matching model outputs for the same problem to form value-improving pairs to train a correction model. (Image source: Welleck et al. 2023)\\n\\nRecursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do both generation and self-correction.\\nSCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage the model to do self-correction by producing better answers at the second attempt than the one created at the first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and stage 2 further improves the results.\\n\\n\\nExplicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: Kumar et al. 2024)\\n\\nRL for Better Reasoning#\\nThere’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports from DeepSeek.\\nDeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training, enabling R1 to be good at both reasoning and non-reasoning tasks.\\n\\nCold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. Without this step, the model has issues of poor readability and language mixing.\\nReasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:\\n\\nFormat rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.\\nAccuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate whether test cases pass.\\n\\n\\nRejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA, and self-cognition, to retrain DeepSeek-V3-Base.\\n\\nFilter out CoTs with mixed languages, long paragraphs, and code blocks.\\nInclude non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.\\nFor certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by prompting. But for simpler queries like “hello”, CoT is not needed.\\nThen fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.\\n\\n\\nThe final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving helpfulness, harmlessness and reasoning.\\n\\n\\n\\nDeepSeek-R1 performs comparable to OpenAI o1-preview and o1-mini on several widely used reasoning benchmarks. DeepSeek-V3 is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)\\n\\nInterestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge, referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them. Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on math problems, as well as the emergent “aha moment”.\\n\\n\\nExamples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025; (right) Zeng et al. 2025)\\n\\nThe DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also failed due to the large search space for language model tokens, in comparison to, say, chess; and training the fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide unique insights and we would like to encourage the research community to share more about what did not work out.\\nExternal Tool Use#\\nDuring the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code or running mathematical calculations. Offloading that part of reasoning components into an external code interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution (Shinn, et al. 2023).\\n\\n\\nAn example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)\\n\\nReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of reasoning traces, such that reasoning paths can incorporate external knowledge.\\n\\n\\nAn example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an external tool to help with reasoning. (Image source: Yao et al. 2023)\\n\\no3 & o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.\\nThinking Faithfully#\\nDeep learning models are often treated as black boxes and various interpretability methods have been proposed. Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its actions. Second, it can help us determine whether the model is using a sound process to compute its answers. Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal process visible in natural language. This interpretability, however, rests on the assumption that the model truthfully describes its internal thought processes.\\nRecent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense intuitively, because thinking for longer should be especially useful when the model is presented with an unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make sense of the strange situation it’s been presented with.\\n\\n\\nThe experiment of asking the model to decide if another model tried to hack the unit tests in some way for coding questions given its thought process. We can monitor these reward hacking behavior during training with different types of monitor. The exit(0) coding hack is when the agent exploited a bug that allowed it to exit from the environment early without running all unit tests. The raise SkipTest hack is when the agent raises an exception from functions outside the testing framework in order to skip unit test evaluation. (Image source: Baker et al. 2025)\\n\\nDoes the Model Tell What it Thinks Faithfully#\\nIntuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples may contain mistakes. Thus we cannot by default assume CoT is always faithful .\\nLanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA, MMLU, ARC Challenge, TruthfulQA, HellaSwag):\\n\\n\\nMistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or language templates in the formation of CoT.\\n\\n\\nMistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may suffer performance drop slightly when compared to no CoT.\\n\\n\\nMistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting accuracy gains do not rely on human-readable reasoning.\\n\\n\\n\\n\\nIllustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al. 2023)\\n\\nInterestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not always increase with model size on multiple choice questions, but does increase with model size on addition tasks, implying that thinking time matters more for complex reasoning tasks.\\n\\n\\nThe dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al. 2023)\\n\\nAlternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths directly (Turpin et al. 2023, Chua & Evans, 2025, Chen et al. 2025).\\nOne method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to introduce biases.\\nAnother prompting technique inserts misleading hints into prompts, such as \"I think the answer is <random_label> but curious to hear what you think\". or \"A Stanford Professor thinks the answer is <random_label>\". By comparing model predictions for the same question with vs without the misleading hint, we can measure whether a model is able to faithfully describe the influence of the hint on its answer. Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly acknowledge the impact and admit the change of its answer is due to the hint.\\n\\n\\nBoth GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than \"Answer is always A\" type of bias. (Image source: Turpin et al. 2023)\\n\\nMultiple studies found that reasoning models describe the influence of the hint much more reliably than all the non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).\\n\\n\\nReasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al. 2025)\\n\\nSome evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT paths are optimized for producing the correct final answers, not to match human preference defined in RM, and thus intuitively are expected to be more faithful.\\nOptimization Pressure on CoT: Good or Bad?#\\nMonitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.\\nHowever, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward hacking where the model tries to hide its true intent within the CoT (Baker et al. 2025). With this direct optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.\\n\\n\\nWhen using the monitor to provide an RL reward signal to prevent reward hacking during training, the model still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)\\n\\nExperiments on CoT length RL signals by Yeo et al. (2025) confirmed a similar message as Baker et al. that new types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et al. further applied a n-gram repetition penalty to prevent this hacking behavior.\\n\\n\\nCareful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025)\\n\\nChen et al. (2025) experimented with a flawed RL environment, specifically using a grader with incorrect answers filled in for multiple-choice questions. The model learns to exploit the reward hack on >99% of the prompts, but almost never (<2%) verbalizes the reward hack in its CoT on more than half of their environments. Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.\\nRL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.\\nThinking in Continuous Space#\\nAdaptive Computation Time, introduced by Alex Graves in 2016, predated large language models but pioneered the same direction of enabling the model to dynamically decide the number of computational steps to take at the inference time, which can be viewed as enabling the model to “think more” in continuous space at test time. Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally via more sequential sampling steps.\\nRecurrent Architecture#\\nA number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling adaptive test time compute (Dehghani, et al. 2019, Hutchins, et al. 2022, Bulatov, et al. 2022). A deep dive into literature on this topic would make the post too long, so we will only review a few.\\nUniversal Transformer (Dehghani, et al. 2019) combines self-attention in Transformer with the recurrent mechanism in RNN, dynamically adjusting the number of steps using adaptive computation time (Graves, 2016). On a high level, it can be viewed as a recurrent function for learning the hidden state representation per token, and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.\\nA recent recurrent architecture design, proposed by Geiping et al. (2025), adds a recurrent block $R$ on top of the standard Transformer. Every iteration of this recurrent block takes the embedding $\\\\mathbf{e}$ and a random state $\\\\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned diffusion model, where the original input $\\\\mathbf{e}$ is provided in every recurrent step while a random Gaussian initialized state $\\\\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of their experiments of designs that resemble diffusion models more turned out to be bad.)\\n\\n$$  \\n\\\\begin{aligned}  \\n\\\\mathbf{e} &= P(\\\\mathbf{x}) & \\\\text{embedding} \\\\\\\\ \\\\mathbf{s}\\\\_0 &\\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\sigma^2 I\\\\_{n \\\\cdot h}) \\\\\\\\ \\\\mathbf{s}\\\\_i &= R(\\\\mathbf{e}, \\\\mathbf{s}\\\\_{i-1}) \\\\quad\\\\text{ for }i \\\\in {1, \\\\dots, r} & \\\\small{\\\\text{recurrent block; resembles a Transformer block}}\\\\\\\\ \\\\mathbf{p} &= C(\\\\mathbf{s}\\\\_r) & \\\\text{unembedding}  \\n\\\\end{aligned}\\n$$\\n\\nThe recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson distribution. The embedding block continues to receive gradient updates in every step since its output $\\\\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the same hidden state for every token; or the model may learn to ignore the incoming state $\\\\mathbf{s}$. To stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful tuning.\\n\\n\\nPlot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around $\\\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts. (Image source: Geiping et al. 2025)\\n\\nThinking Tokens#\\nThinking tokens refer to a set of implicit tokens introduced during training or inference that do not carry direct linguistic meaning. Instead, their role is to provide extra thinking time and compute power for the model to perform better.\\nHerel & Mikolov (2023) introduced the idea of inserting special thinking tokens (<T>) after each word in a sentence and training the model on such a dataset. Each thinking token buys extra time for the model to process and make better predictions. Training with thinking tokens on a toy model setup results in lower perplexity than baseline model trained without them. The benefits of thinking tokens are more pronounced for non-trivial reasoning tasks or sentences involving numbers.\\nSimilarly, pause tokens proposed by Goyal et al. (2024) delay the model’s outputs by appending dummy tokens (e.g. character like . or #) at the end of the input sequence, giving the model extra computation during inference. It is important to inject such pause tokens both during training and inference time, while only fine-tuning on pause tokens leads to limited gain. During training, multiple copies of pause tokens are inserted at uniformly random locations and the loss on pause tokens is ignored for training.\\n\\n\\nIllustration of how pause tokens are injected during training and inference in comparison to standard setup. (Image source: Goyal et al. 2024)\\n\\nInterestingly, thinking tokens or pause tokens in above experiments do not carry any extra information or add many new parameters. But why is it still helpful? On one hand, it helps expand the computation by introducing more inference loops, effectively increasing computational capacity. On the other hand, it can be seen as acting as a special, implicit form of CoTs. A drawback here is hat the model needs to be pretrained with respect to thinking tokens. Still, this strategy is an interesting way to further improve the capability of test time compute utilization on top of inference time CoTs.\\nQuiet-STaR (Zelikman et al. 2025) introduces token-level reasoning by training the model to generate rationales after every token to explain future text. It mixes the future-text predictions with and without rationales and uses learning to generate better rationales and uses REINFORCE to optimize the quality of rationale generation.\\n\\n\\nIllustration of Quiet-STaR. (Image source: Zelikman et al. 2025)\\n\\nQuiet-STaR consists of three stages:\\n\\n\\nThink: Predicting next tokens with rationales. Due to high computational cost demanded by token level reasoning, this process is designed to generate multiple rationales in parallel. A special attention map is used to enable all thought tokens to only pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text.\\n\\n\\nTalk: Next token prediction without rationale is mixed with post-rationale prediction. The mixing weight for two logits is learned by a special mixing head of a shallow MLP out of hidden output after each rationale. Correct next tokens can be selected via teacher forcing.\\n\\n\\nLearn: Train the model to generate better rationale via REINFORCE by learning from examples that increase the probability of correct next token while discarding those that hurt the prediction.\\n\\n\\nWithout dataset specific fine-tuning, Quiet-STaR improves zero-shot results on CommonsenseQA (36.3%→47.2%) and GSM8K (5.9%→10.9%), within experiments on Mistral 7B.\\nThinking as Latent Variables#\\nA latent variable model defines a probabilistic framework where observable data is explained through unobserved (latent) variables. These latent variables capture hidden structures or intermediate processes that generate the observable outcomes. Language models can be viewed as probabilistic latent variable models where test-time thinking and reasoning steps are latent thought variables (Zhou et al. 2020, Phan et al. 2023). Such a latent variable model defines a joint distribution of problems x_i, answers y_i and latent thought z_i. We would like to optimize the log-likelihood of answers given questions and a variety of CoTs as latent variables (N is the number of samples; $K$ is the number of CoTs per problem):\\n\\n$$ \\n\\\\begin{aligned}  \\n\\\\log \\\\mathcal{L}(\\\\theta)  \\n&= \\\\log p(y \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(y, z^{(k)} \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(z^{(k)} \\\\mid x)\\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n&= \\\\log \\\\mathbb{E}_{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)} \\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n\\\\end{aligned}\\n$$\\n\\nOur goal is to maximize the marginal likelihood of the correct answer, $p(y \\\\mid x)$, given a number of reasoning traces per problem, $\\\\{z^{(k)}\\\\}_{k=1}^K$.\\nExpectation-Maximization#\\nExpectation-Maximization is a commonly used iterative algorithm for optimizing parameters for a model with (hidden) latent variables, and thus can be applied to train better CoTs and then condition on that to generate better responses. Typically we iterate between E-step (Expectation) where we guess the missing information about latent variables (i.e. how to sample better CoTs), and M-step (Maximization) where we optimize the model parameters based on latent variables (i.e. how to sample better answers), until convergence.\\n\\n$$\\n\\\\log \\\\mathcal{L}(\\\\theta) = \\n\\\\log \\\\mathbb{E}_{\\\\underbrace{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)}_\\\\text{E-step}}\\\\;\\\\underbrace{p(y \\\\mid z^{(k)}, x)}_\\\\text{M-step}\\n$$\\n\\nBecause we cannot directly sample from the latent variable distribution $p(z \\\\mid x, y)$, researchers have explored methods relying on human annotated data (Zhou et al. 2020), Metropolis-Hastings MCMC (Phan et al. 2023) or Monte Carlo sampling with special importance weights (Ruan et al. 2025) to draw good CoT samples to update the model. Ruan et al. (2025) experimented with training a model on a large body of Web text with latent thoughts with the EM algorithm, where the latent thought is synthesized per chunk of observed data and then the model learns over both latent thought and data in an autoregressive manner.\\n\\n\\nIllustration of training on data corpus with latent thought injected. (Image source: Ruan et al. 2025)\\n\\nThey first prompt a LLM $\\\\tilde{q}$ to generate synthetic latent thought Z_i given observed data X_i:\\nYou are provided with a pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, step-by-step derivations or logical reasoning).\\nSpecial tokens like <StartOfLatent><Prior> ... <EndOfPrior> are used to insert the generated latent thought content into the raw data for training the joint distribution $p(z, x)$ or the approximate posterior $q(z \\\\mid x)$, depending on whether $z$ is inserted before or after $x$. However, since we are using a LLM $\\\\tilde{q}(z \\\\mid x)$ to generate the CoTs, it imposed a performance ceiling on how good the approximate $q(z \\\\mid x)$ can be. Ruan et al. introduced importance weights for selecting CoT samples at the E-step, formulated as:\\n\\n$$\\nw^{(k)}  \\n= \\\\frac{p(z^{(k)}, x)}{q(z^{(k)} \\\\mid x)}  \\n= \\\\frac{p(x \\\\mid z^{(k)}) \\\\; p(z^{(k)})}{q(z^{(k)} \\\\mid x)}  \\n$$\\n\\n, such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\\\mid z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low $q(z^{(k)} \\\\mid x)$).\\nIterative Learning#\\nSince pretrained models already possess the capability of generating chains of thought, it is intuitive to design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on rationales that lead to correct answers.\\nHowever, this straightforward design can fail because the model receives no learning signals for problems it fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the model is finetuned on correct solutions that either lead to correct outputs or are generated through rationalization.\\n\\n\\nThe algorithm of STaR. (Image source: Zelikman et al. 2022)\\n\\nWe can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the reward, $\\\\mathbb{1}[\\\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\\\sim p(z \\\\mid x)$ and then $y \\\\sim p(y \\\\mid x, z)$, since $p(y \\\\mid x) = \\\\sum_z p(z \\\\mid x) \\\\; p(y \\\\mid x, z)$.\\n\\n$$\\n\\\\begin{aligned}  \\n\\\\nabla_\\\\theta J(\\\\theta)  \\n&= \\\\nabla_\\\\theta \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\nabla_\\\\theta \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\frac{\\\\nabla_\\\\theta p(y_i, z_i \\\\mid x_i)}{p(y_i, z_i \\\\mid x_i)} & \\\\text{;log-derivative trick}\\\\\\\\  \\n&= \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; \\\\nabla_\\\\theta \\\\log p(y_i, z_i \\\\mid x_i) & \\\\text{;log-derivative trick}\\n\\\\end{aligned}\\n$$\\n\\nEach iteration is equivalent to first selecting the CoT samples according to $\\\\mathbb{1}[y=y^\\\\text{truth}]$ and then running supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR improves with more training iterations, and the “rationalization” process for generating better CoTs accelerates learning. They observed that sampling with high temperature increases the chance of getting correct answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth answers (Wang et al. 2022), making it possible to use synthetic samples for training.\\n\\n\\nA comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image source: Zelikman et al. 2022)\\n\\nScaling Laws for Thinking Time#\\nSo far we have seen much evidence that allowing models to spend additional compute on reasoning before producing final answers at inference time can significantly improve performance. Techniques like prompting the model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect before predicting next tokens, have been found to boost the model performance beyond the capability limit obtained during training. This essentially introduces a new dimension to tinker with for improving model intelligence, complementing established factors such as model size, training compute and data quantity, as defined in scaling laws (Kaplan et al. 2020).\\nRecent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can offer Pareto-optimal trade-offs in cost and performance.\\nSnell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1 exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are substantially fewer than pretraining ones. This indicates that developing a capable base model with enough pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in big model capability gaps.\\n\\n\\n(Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to pretraining tokens is << 1, ~=1 or >> 1 and the benefit of test time compute is clear only the ratio << 1. (Image source: Snell et al. 2024)\\n\\ns1 models (Muennighoff & Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation accuracy.\\n\\n\\nBoth parallel and sequential scaling methods of test-time compute shows positive correlation with the evaluation performance in s1 experiments. (Image Muennighoff & Yang, et al. 2025)\\n\\nWhen comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.\\n\\n\\n(Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval accuracy. (Image source: Muennighoff & Yang et al. 2025)\\n\\nWhat’s for Future#\\nThe exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical reflection, and error correction. Excitement with current progress invites us for more future research to improve and understand deeply not just how but why we—and our models—think.\\nAt the end, I would like to call for more research for the following open research questions on test time compute and chain-of-thought reasoning.\\n\\nCan we incentivize the model to produce human-readable, faithful reasoning paths during RL training while avoiding reward hacking behavior?\\nHow to define reward hacking? Can we capture reward hacking during RL training or inference without human intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?\\nSelf-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn RL. How can we train the model to correct itself without hallucination or regression when ground truth is not available?\\nHow to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to grade, such as creative writing, coaching, brainstorming?\\nWhen we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly translate the performance gain back into the base model with reduced inference time cost (e.g. via distillation)?\\nHow to make test time spending more adaptive according to the difficulty of the problem in hand?\\n\\nCitation#\\nPlease cite this work as:\\nWeng, Lilian. \"Why We Think\". Lil\\'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/\\nOr use the BibTex citation:\\n@article{weng2025think,\\n  title = {Why We Think},\\n  author = {Weng, Lilian},\\n  journal = {lilianweng.github.io},\\n  year = {2025},\\n  month = {May},\\n  url = \"https://lilianweng.github.io/posts/2025-05-01-thinking/\"\\n}\\nReferences#\\n[1] Alex Graves. “Adaptive Computation Time for Recurrent Neural Networks.”. arXiv preprint arXiv:1603.08983 (2016).\\n[2] Wang Ling, et al. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”. arXiv preprint arXiv:1705.04146 (2017).\\n[3] Karl Cobbe, et al. “Training Verifiers to Solve Math Word Problems.”. arXiv preprint arXiv:2110.14168 (2021).\\n[4] Jason Wei, et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.”. NeurIPS 2022.\\n[5] Maxwell Nye, et al. “Show Your Work: Scratchpads for Intermediate Computation with Language Models.”. arXiv preprint arXiv:2112.00114 (2021).\\n[6] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux (2013).\\n[7] Takeshi Kojima, et al. “Large Language Models are Zero-Shot Reasoners.”. NeurIPS 2022.\\n[8] Michihiro Yasunaga, et al. “Large Language Models as Analogical Reasoners”. arXiv preprint arXiv:2310.01714 (2023).\\n[9] Eric Zelikman, et al. “STaR: Bootstrapping Reasoning With Reasoning.”. NeurIPS 2022.\\n[10] Xuezhi Wang, et al. “Self-consistency Improves Chain of Thought Reasoning in Language Models.”. ACL 2023.\\n[11] Ryo Kamoi, et al. “When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.”. TACL 2024.\\n[12] Jie Huang, et al. “Large Language Models Cannot Self-Correct Reasoning Yet.”. ICLR 2024.\\n[13] Noah Shinn, et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.”. arXiv preprint arXiv:2303.11366 (2023).\\n[14] Yunxiang Zhang, et al. “Small Language Models Need Strong Verifiers to Self-Correct Reasoning.”. ACL Findings 2024.\\n[15] Hao Liu, et al. “Chain of Hindsight Aligns Language Models with Feedback.”. arXiv preprint arXiv:2302.02676 (2023).\\n[16] Sean Welleck, et al. “Generating Sequences by Learning to Self-Correct.”. arXiv preprint arXiv:2211.00053 (2023).\\n[17] Yuxiao Qu, et al. “Recursive Introspection: Teaching Language Model Agents How to Self-Improve.”. arXiv preprint arXiv:2407.18219 (2024).\\n[18] Aviral Kumar, et al. “Training Language Models to Self-Correct via Reinforcement Learning.”. arXiv preprint arXiv:2409.12917 (2024).\\n[19] Hunter Lightman, et al. “Let’s Verify Step by Step.”. arXiv preprint arXiv:2305.20050 (2023).\\n[20] Yuxi Xie, et al. “Self-Evaluation Guided Beam Search for Reasoning.”. NeurIPS 2023.\\n[21] Yangzhen Wu, et al. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models”. ICLR 2025.\\n[22] Dongwei Jiang, et al. “RATIONALYST: Pre-training Process-Supervision for Improving Reasoning”. arXiv preprint arXiv:2410.01044 (2024).\\n[23] Xuezhi Wang and Denny Zhou. “Chain-of-Thought Reasoning Without Prompting.”. arXiv preprint arXiv:2402.10200 (2024).\\n[24] DeepSeek-AI. “DeepSeek-V3 Technical Report.” arXiv preprint arXiv:2412.19437 (2024).\\n[25] DeepSeek-AI. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.”. arXiv preprint arXiv:2501.12948 (2025).\\n[26] Luyu Gao, Aman Madaan & Shuyan Zhou, et al. “PAL: Program-aided Language Models.”. ICML 2023.\\n[27] Shunyu Yao, et al. “ReAct: Synergizing Reasoning and Acting in Language Models.”. ICLR 2023.\\n[29] Bowen Baker, et al. “Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.”. arXiv preprint arXiv:2503.11926 (2025).\\n[30] Wojciech Zaremba, et al. “Trading Inference-Time Compute for Adversarial Robustness.”. arXiv preprint arXiv:2501.18841 (2025).\\n[31] Tamera Lanham, et al. “Measuring Faithfulness in Chain-of-Thought Reasoning”. arXiv preprint arXiv:2307.13702 (2023).\\n[32] Boshi Wang, et al. “Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.”. ACL 2023.\\n[33] Miles Turpin, et al. “Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.”. NeuriPS 2023.\\n[34] James Chua & Owain Evans. “Are DeepSeek R1 And Other Reasoning Models More Faithful?”. arXiv preprint arXiv:2501.08156 (2025).\\n[35] Yanda Chen et al. “Reasoning Models Don’t Always Say What They Think”. arXiv preprint arXiv:2505.05410 (2025).\\n[36] Edward Yeo, et al. “Demystifying Long Chain-of-Thought Reasoning in LLMs.”. arXiv preprint arXiv:2502.03373 (2025).\\n[37] Mostafa Dehghani, et al. “Universal Transformers.”. ICLR 2019.\\n[38] DeLesley Hutchins, et al. “Block-Recurrent Transformers.”. NeurIPS 2022.\\n[39] Aydar Bulatov, et al. “Recurrent Memory Transformers.”. NeuriPS 2022.\\n[40] Jonas Geiping, et al. “Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.”. arXiv preprint arXiv:2502.05171 (2025).\\n[41] Herel & Mikolov. “Thinking Tokens for Language Modeling.”. AITP 2023.\\n[42] Sachin Goyal et al. “Think before you speak: Training Language Models With Pause Tokens.”. ICLR 2024.\\n[43] Eric Zelikman, et al. “Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.”. arXiv preprint arXiv:2403.09629 (2025).\\n[44] Wangchunshu Zhou et al. “Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.”. NeurIPS 2020.\\n[45] Du Phan et al. “Training Chain-of-Thought via Latent-Variable Inference.”. NeurIPS 2023.\\n[46] Yangjun Ruan et al. “Reasoning to Learn from Latent Thoughts.”. arXiv preprint arXiv:2503.18866 (2025).\\n[47] Xuezhi Wang et al. “Rationale-Augmented Ensembles in Language Models.”. arXiv preprint arXiv:2207.00747 (2022).\\n[48] Jared Kaplan, et al. “Scaling Laws for Neural Language Models.”. arXiv preprint arXiv:2001.08361 (2020).\\n[49] Niklas Muennighoff & Zitong Yang, et al. “s1: Simple test-time scaling.”. arXiv preprint arXiv:2501.19393 (2025).\\n[50] Peiyi Wang, et al. “Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations” arXiv preprint arXiv:2312.08935 (2023).\\n[51] Yixin Liu, et al. “Improving Large Language Model Fine-tuning for Solving Math Problems.” arXiv preprint arXiv:2310.10047 (2023).\\n[52] Charlie Snell, et al. “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.”. arXiv preprint arXiv:2408.03314 (2024).\\n[53] OpenAI. o1-preview: “Learning to reason with LLMs.” Sep 12, 2024.\\n[54] OpenAI. o3: “Introducing OpenAI o3 and o4-mini.” Apr 16, 2025.\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b71f96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant\\nperformance boost. Since we combine region proposals\\nwith CNNs, we call our method R-CNN: Regions with CNN\\nfeatures. We also compare R-CNN to OverFeat, a recently\\nproposed sliding-window detector based on a similar CNN\\narchitecture. We ﬁnd that R-CNN outperforms OverFeat\\nby a large margin on the 200-class ILSVRC2013 detection\\ndataset. Source code for the complete system is available at\\nhttp://www.cs.berkeley.edu/˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [39] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\nOn the 200-class ILSVRC2013 detection dataset, R-CNN’s\\nmAP is 31.4% , a large improvement over OverFeat [34], which\\nhad the previous best result at 24.3%.\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training\\nalgorithm. Building on Rumelhart et al. [33], LeCun et\\nal. [26] showed that stochastic gradient descent via back-\\npropagation was effective for training convolutional neural\\nnetworks (CNNs), a class of models that extend the neocog-\\nnitron.\\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\\nfell out of fashion with the rise of support vector machines.\\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\\nby showing substantially higher image classiﬁcation accu-\\nracy on the ImageNet Large Scale Visual Recognition Chal-\\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\\ning a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\n1\\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='debated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question by bridging the gap between\\nimage classiﬁcation and object detection. This paper is the\\nﬁrst to show that a CNN can lead to dramatically higher ob-\\nject detection performance on PASCAL VOC as compared\\nto systems based on simpler HOG-like features. To achieve\\nthis result, we focused on two problems: localizing objects\\nwith a deep network and training a high-capacity model\\nwith only a small quantity of annotated detection data.\\nUnlike image classiﬁcation, detection requires localiz-\\ning (likely many) objects within an image. One approach\\nframes localization as a regression problem. However, work\\nfrom Szegedy et al. [38], concurrent with our own, indi-\\ncates that this strategy may not fare well in practice (they\\nreport a mAP of 30.5% on VOC 2007 compared to the\\n58.5% achieved by our method). An alternative is to build a\\nsliding-window detector. CNNs have been used in this way\\nfor at least two decades, typically on constrained object cat-\\negories, such as faces [32, 40] and pedestrians [35]. In order\\nto maintain high spatial resolution, these CNNs typically\\nonly have two convolutional and pooling layers. We also\\nconsidered adopting a sliding-window approach. However,\\nunits high up in our network, which has ﬁve convolutional\\nlayers, have very large receptive ﬁelds ( 195 ×195 pixels)\\nand strides (32×32 pixels) in the input image, which makes\\nprecise localization within the sliding-window paradigm an\\nopen technical challenge.\\nInstead, we solve the CNN localization problem by oper-\\nating within the “recognition using regions” paradigm [21],\\nwhich has been successful for both object detection [39] and\\nsemantic segmentation [5]. At test time, our method gener-\\nates around 2000 category-independent region proposals for\\nthe input image, extracts a ﬁxed-length feature vector from\\neach proposal using a CNN, and then classiﬁes each region\\nwith category-speciﬁc linear SVMs. We use a simple tech-\\nnique (afﬁne image warping) to compute a ﬁxed-size CNN\\ninput from each region proposal, regardless of the region’s\\nshape. Figure 1 presents an overview of our method and\\nhighlights some of our results. Since our system combines\\nregion proposals with CNNs, we dub the method R-CNN:\\nRegions with CNN features.\\nIn this updated version of this paper, we provide a head-\\nto-head comparison of R-CNN and the recently proposed\\nOverFeat [34] detection system by running R-CNN on the\\n200-class ILSVRC2013 detection dataset. OverFeat uses a\\nsliding-window CNN for detection and until now was the\\nbest performing method on ILSVRC2013 detection. We\\nshow that R-CNN signiﬁcantly outperforms OverFeat, with\\na mAP of 31.4% versus 24.3%.\\nA second challenge faced in detection is that labeled data\\nis scarce and the amount currently available is insufﬁcient\\nfor training a large CNN. The conventional solution to this\\nproblem is to useunsupervised pre-training, followed by su-\\npervised ﬁne-tuning (e.g., [35]). The second principle con-\\ntribution of this paper is to show thatsupervised pre-training\\non a large auxiliary dataset (ILSVRC), followed by domain-\\nspeciﬁc ﬁne-tuning on a small dataset (PASCAL), is an\\neffective paradigm for learning high-capacity CNNs when\\ndata is scarce. In our experiments, ﬁne-tuning for detection\\nimproves mAP performance by 8 percentage points. After\\nﬁne-tuning, our system achieves a mAP of 54% on VOC\\n2010 compared to 33% for the highly-tuned, HOG-based\\ndeformable part model (DPM) [17, 20]. We also point read-\\ners to contemporaneous work by Donahue et al. [12], who\\nshow that Krizhevsky’s CNN can be used (without ﬁne-\\ntuning) as a blackbox feature extractor, yielding excellent\\nperformance on several recognition tasks including scene\\nclassiﬁcation, ﬁne-grained sub-categorization, and domain\\nadaptation.\\nOur system is also quite efﬁcient. The only class-speciﬁc\\ncomputations are a reasonably small matrix-vector product\\nand greedy non-maximum suppression. This computational\\nproperty follows from features that are shared across all cat-\\negories and that are also two orders of magnitude lower-\\ndimensional than previously used region features (cf. [39]).\\nUnderstanding the failure modes of our approach is also\\ncritical for improving it, and so we report results from the\\ndetection analysis tool of Hoiem et al. [23]. As an im-\\nmediate consequence of this analysis, we demonstrate that\\na simple bounding-box regression method signiﬁcantly re-\\nduces mislocalizations, which are the dominant error mode.\\nBefore developing technical details, we note that because\\nR-CNN operates on regions it is natural to extend it to the\\ntask of semantic segmentation. With minor modiﬁcations,\\nwe also achieve competitive results on the PASCAL VOC\\nsegmentation task, with an average segmentation accuracy\\nof 47.9% on the VOC 2011 test set.\\n2. Object detection with R-CNN\\nOur object detection system consists of three modules.\\nThe ﬁrst generates category-independent region proposals.\\nThese proposals deﬁne the set of candidate detections avail-\\nable to our detector. The second module is a large convo-\\nlutional neural network that extracts a ﬁxed-length feature\\nvector from each region. The third module is a set of class-\\nspeciﬁc linear SVMs. In this section, we present our design\\ndecisions for each module, describe their test-time usage,\\ndetail how their parameters are learned, and show detection\\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\\n2.1. Module design\\nRegion proposals. A variety of recent papers offer meth-\\nods for generating category-independent region proposals.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='aeroplane bicycle bird car\\nFigure 2: Warped training samples from VOC 2007 train.\\nExamples include: objectness [1], selective search [39],\\ncategory-independent object proposals [14], constrained\\nparametric min-cuts (CPMC) [5], multi-scale combinatorial\\ngrouping [3], and Cires ¸an et al. [6], who detect mitotic cells\\nby applying a CNN to regularly-spaced square crops, which\\nare a special case of region proposals. While R-CNN is ag-\\nnostic to the particular region proposal method, we use se-\\nlective search to enable a controlled comparison with prior\\ndetection work (e.g., [39, 41]).\\nFeature extraction. We extract a 4096-dimensional fea-\\nture vector from each region proposal using the Caffe [24]\\nimplementation of the CNN described by Krizhevsky et\\nal. [25]. Features are computed by forward propagating\\na mean-subtracted 227 ×227 RGB image through ﬁve con-\\nvolutional layers and two fully connected layers. We refer\\nreaders to [24, 25] for more network architecture details.\\nIn order to compute features for a region proposal, we\\nmust ﬁrst convert the image data in that region into a form\\nthat is compatible with the CNN (its architecture requires\\ninputs of a ﬁxed 227 ×227 pixel size). Of the many possi-\\nble transformations of our arbitrary-shaped regions, we opt\\nfor the simplest. Regardless of the size or aspect ratio of the\\ncandidate region, we warp all pixels in a tight bounding box\\naround it to the required size. Prior to warping, we dilate the\\ntight bounding box so that at the warped size there are ex-\\nactly ppixels of warped image context around the original\\nbox (we use p = 16). Figure 2 shows a random sampling\\nof warped training regions. Alternatives to warping are dis-\\ncussed in Appendix A.\\n2.2. Test-time detection\\nAt test time, we run selective search on the test image\\nto extract around 2000 region proposals (we use selective\\nsearch’s “fast mode” in all experiments). We warp each\\nproposal and forward propagate it through the CNN in or-\\nder to compute features. Then, for each class, we score\\neach extracted feature vector using the SVM trained for that\\nclass. Given all scored regions in an image, we apply a\\ngreedy non-maximum suppression (for each class indepen-\\ndently) that rejects a region if it has an intersection-over-\\nunion (IoU) overlap with a higher scoring selected region\\nlarger than a learned threshold.\\nRun-time analysis. Two properties make detection efﬁ-\\ncient. First, all CNN parameters are shared across all cate-\\ngories. Second, the feature vectors computed by the CNN\\nare low-dimensional when compared to other common ap-\\nproaches, such as spatial pyramids with bag-of-visual-word\\nencodings. The features used in the UV A detection system\\n[39], for example, are two orders of magnitude larger than\\nours (360k vs. 4k-dimensional).\\nThe result of such sharing is that the time spent com-\\nputing region proposals and features (13s/image on a GPU\\nor 53s/image on a CPU) is amortized over all classes. The\\nonly class-speciﬁc computations are dot products between\\nfeatures and SVM weights and non-maximum suppression.\\nIn practice, all dot products for an image are batched into\\na single matrix-matrix product. The feature matrix is typi-\\ncally 2000×4096 and the SVM weight matrix is4096×N,\\nwhere N is the number of classes.\\nThis analysis shows that R-CNN can scale to thousands\\nof object classes without resorting to approximate tech-\\nniques, such as hashing. Even if there were 100k classes,\\nthe resulting matrix multiplication takes only 10 seconds on\\na modern multi-core CPU. This efﬁciency is not merely the\\nresult of using region proposals and shared features. The\\nUV A system, due to its high-dimensional features, would\\nbe two orders of magnitude slower while requiring 134GB\\nof memory just to store 100k linear predictors, compared to\\njust 1.5GB for our lower-dimensional features.\\nIt is also interesting to contrast R-CNN with the recent\\nwork from Dean et al. on scalable detection using DPMs\\nand hashing [8]. They report a mAP of around 16% on VOC\\n2007 at a run-time of 5 minutes per image when introducing\\n10k distractor classes. With our approach, 10k detectors can\\nrun in about a minute on a CPU, and because no approxi-\\nmations are made mAP would remain at 59% (Section 3.2).\\n2.3. Training\\nSupervised pre-training. We discriminatively pre-trained\\nthe CNN on a large auxiliary dataset (ILSVRC2012 clas-\\nsiﬁcation) using image-level annotations only (bounding-\\nbox labels are not available for this data). Pre-training\\nwas performed using the open source Caffe CNN library\\n[24]. In brief, our CNN nearly matches the performance\\nof Krizhevsky et al. [25], obtaining a top-1 error rate 2.2\\npercentage points higher on the ILSVRC2012 classiﬁcation\\nvalidation set. This discrepancy is due to simpliﬁcations in\\nthe training process.\\nDomain-speciﬁc ﬁne-tuning. To adapt our CNN to the\\nnew task (detection) and the new domain (warped proposal\\nwindows), we continue stochastic gradient descent (SGD)\\ntraining of the CNN parameters using only warped region\\nproposals. Aside from replacing the CNN’s ImageNet-\\nspeciﬁc 1000-way classiﬁcation layer with a randomly ini-\\ntialized (N + 1)-way classiﬁcation layer (where N is the\\nnumber of object classes, plus 1 for background), the CNN\\narchitecture is unchanged. For VOC, N = 20 and for\\nILSVRC2013, N = 200. We treat all region proposals with\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='≥0.5 IoU overlap with a ground-truth box as positives for\\nthat box’s class and the rest as negatives. We start SGD at\\na learning rate of 0.001 (1/10th of the initial pre-training\\nrate), which allows ﬁne-tuning to make progress while not\\nclobbering the initialization. In each SGD iteration, we uni-\\nformly sample 32 positive windows (over all classes) and\\n96 background windows to construct a mini-batch of size\\n128. We bias the sampling towards positive windows be-\\ncause they are extremely rare compared to background.\\nObject category classiﬁers. Consider training a binary\\nclassiﬁer to detect cars. It’s clear that an image region\\ntightly enclosing a car should be a positive example. Simi-\\nlarly, it’s clear that a background region, which has nothing\\nto do with cars, should be a negative example. Less clear\\nis how to label a region that partially overlaps a car. We re-\\nsolve this issue with an IoU overlap threshold, below which\\nregions are deﬁned as negatives. The overlap threshold,0.3,\\nwas selected by a grid search over {0,0.1,..., 0.5}on a\\nvalidation set. We found that selecting this threshold care-\\nfully is important. Setting it to 0.5, as in [39], decreased\\nmAP by 5 points. Similarly, setting it to 0 decreased mAP\\nby 4 points. Positive examples are deﬁned simply to be the\\nground-truth bounding boxes for each class.\\nOnce features are extracted and training labels are ap-\\nplied, we optimize one linear SVM per class. Since the\\ntraining data is too large to ﬁt in memory, we adopt the\\nstandard hard negative mining method [17, 37]. Hard neg-\\native mining converges quickly and in practice mAP stops\\nincreasing after only a single pass over all images.\\nIn Appendix B we discuss why the positive and negative\\nexamples are deﬁned differently in ﬁne-tuning versus SVM\\ntraining. We also discuss the trade-offs involved in training\\ndetection SVMs rather than simply using the outputs from\\nthe ﬁnal softmax layer of the ﬁne-tuned CNN.\\n2.4. Results on PASCAL VOC 2010-12\\nFollowing the PASCAL VOC best practices [15], we\\nvalidated all design decisions and hyperparameters on the\\nVOC 2007 dataset (Section 3.2). For ﬁnal results on the\\nVOC 2010-12 datasets, we ﬁne-tuned the CNN on VOC\\n2012 train and optimized our detection SVMs on VOC 2012\\ntrainval. We submitted test results to the evaluation server\\nonly once for each of the two major algorithm variants (with\\nand without bounding-box regression).\\nTable 1 shows complete results on VOC 2010. We com-\\npare our method against four strong baselines, including\\nSegDPM [18], which combines DPM detectors with the\\noutput of a semantic segmentation system [4] and uses ad-\\nditional inter-detector context and image-classiﬁer rescor-\\ning. The most germane comparison is to the UV A system\\nfrom Uijlings et al. [39], since our systems use the same re-\\ngion proposal algorithm. To classify regions, their method\\nbuilds a four-level spatial pyramid and populates it with\\ndensely sampled SIFT, Extended OpponentSIFT, and RGB-\\nSIFT descriptors, each vector quantized with 4000-word\\ncodebooks. Classiﬁcation is performed with a histogram\\nintersection kernel SVM. Compared to their multi-feature,\\nnon-linear kernel SVM approach, we achieve a large im-\\nprovement in mAP, from 35.1% to 53.7% mAP, while also\\nbeing much faster (Section 2.2). Our method achieves sim-\\nilar performance (53.3% mAP) on VOC 2011/12 test.\\n2.5. Results on ILSVRC2013 detection\\nWe ran R-CNN on the 200-class ILSVRC2013 detection\\ndataset using the same system hyperparameters that we used\\nfor PASCAL VOC. We followed the same protocol of sub-\\nmitting test results to the ILSVRC2013 evaluation server\\nonly twice, once with and once without bounding-box re-\\ngression.\\nFigure 3 compares R-CNN to the entries in the ILSVRC\\n2013 competition and to the post-competition OverFeat re-\\nsult [34]. R-CNN achieves a mAP of 31.4%, which is sig-\\nniﬁcantly ahead of the second-best result of 24.3% from\\nOverFeat. To give a sense of the AP distribution over\\nclasses, box plots are also presented and a table of per-\\nclass APs follows at the end of the paper in Table 8. Most\\nof the competing submissions (OverFeat, NEC-MU, UvA-\\nEuvision, Toronto A, and UIUC-IFP) used convolutional\\nneural networks, indicating that there is signiﬁcant nuance\\nin how CNNs can be applied to object detection, leading to\\ngreatly varying outcomes.\\nIn Section 4, we give an overview of the ILSVRC2013\\ndetection dataset and provide details about choices that we\\nmade when running R-CNN on it.\\n3. Visualization, ablation, and modes of error\\n3.1. Visualizing learned features\\nFirst-layer ﬁlters can be visualized directly and are easy\\nto understand [25]. They capture oriented edges and oppo-\\nnent colors. Understanding the subsequent layers is more\\nchallenging. Zeiler and Fergus present a visually attrac-\\ntive deconvolutional approach in [42]. We propose a simple\\n(and complementary) non-parametric method that directly\\nshows what the network learned.\\nThe idea is to single out a particular unit (feature) in the\\nnetwork and use it as if it were an object detector in its own\\nright. That is, we compute the unit’s activations on a large\\nset of held-out region proposals (about 10 million), sort the\\nproposals from highest to lowest activation, perform non-\\nmaximum suppression, and then display the top-scoring re-\\ngions. Our method lets the selected unit “speak for itself”\\nby showing exactly which inputs it ﬁres on. We avoid aver-\\naging in order to see different visual modes and gain insight\\ninto the invariances computed by the unit.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='VOC 2010 testaero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nDPM v5 [20]† 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4\\nUV A [39] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1\\nRegionlets [41]65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7\\nSegDPM [18]† 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4\\nR-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2\\nR-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7\\nTable 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UV A and Regionlets since all\\nmethods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM\\nwas the top-performer on the PASCAL VOC leaderboard. †DPM and SegDPM use context rescoring not used by the other methods.\\n0 20 40 60 80 100\\nUIUC−IFP \\nDelta \\nGPU_UCLA \\nSYSU_Vision \\nToronto A \\n*OverFeat (1) \\n*NEC−MU \\nUvA−Euvision \\n*OverFeat (2) \\n*R−CNN BB \\nmean average precision (mAP) in %\\nILSVRC2013 detection test set mAP\\n \\n \\n1.0%\\n6.1%\\n9.8%\\n10.5%\\n11.5%\\n19.4%\\n20.9%\\n22.6%\\n24.3%\\n31.4%\\ncompetition result\\npost competition result\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n*R−CNN BB\\nUvA−Euvision\\n*NEC−MU\\n*OverFeat (1)\\nToronto A\\nSYSU_Vision\\nGPU_UCLA\\nDelta\\nUIUC−IFP\\naverage precision (AP) in %\\nILSVRC2013 detection test set class AP box plots\\nFigure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data\\n(images and labels from the ILSVRC classiﬁcation dataset in all cases). (Right) Box plots for the 200 average precision values per\\nmethod. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for\\nR-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; seeR-CNN-ILSVRC2013-APs.txt). The red\\nline marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each\\nmethod. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).\\n1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9\\n1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\nFigure 4: Top regions for six pool5 units. Receptive ﬁelds and activation values are drawn in white. Some units are aligned to concepts,\\nsuch as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reﬂections (6).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='VOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN pool5 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2\\nR-CNN fc6 59.3 61.8 43.1 34.0 25.1 53.1 60.6 52.8 21.7 47.8 42.7 47.8 52.5 58.5 44.6 25.6 48.3 34.0 53.1 58.0 46.2\\nR-CNN fc7 57.6 57.9 38.5 31.8 23.7 51.2 58.9 51.4 20.0 50.5 40.9 46.0 51.6 55.9 43.3 23.3 48.1 35.3 51.0 57.4 44.7\\nR-CNN FT pool5 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3\\nR-CNN FT fc6 63.5 66.0 47.9 37.7 29.9 62.5 70.2 60.2 32.0 57.9 47.0 53.5 60.1 64.2 52.2 31.3 55.0 50.0 57.7 63.0 53.1\\nR-CNN FT fc7 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nDPM v5 [20] 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7\\nDPM ST [28] 23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1\\nDPM HSC [31] 32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3\\nTable 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without ﬁne-tuning. Rows 4-6 show\\nresults for the CNN pre-trained on ILSVRC 2012 and then ﬁne-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box\\nregression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The ﬁrst uses\\nonly HOG, while the next two use different feature learning approaches to augment or replace HOG.\\nVOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN T-Net 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN T-Net BB68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nR-CNN O-Net 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2\\nR-CNN O-Net BB73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nTable 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The ﬁrst two rows are results from\\nTable 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\\nand Zisserman (O-Net) [43].\\nWe visualize units from layer pool 5, which is the max-\\npooled output of the network’s ﬁfth and ﬁnal convolutional\\nlayer. The pool 5 feature map is 6 ×6 ×256 = 9216-\\ndimensional. Ignoring boundary effects, each pool5 unit has\\na receptive ﬁeld of195×195 pixels in the original227×227\\npixel input. A central pool 5 unit has a nearly global view,\\nwhile one near the edge has a smaller, clipped support.\\nEach row in Figure 4 displays the top 16 activations for\\na pool5 unit from a CNN that we ﬁne-tuned on VOC 2007\\ntrainval. Six of the 256 functionally unique units are visu-\\nalized (Appendix D includes more). These units were se-\\nlected to show a representative sample of what the network\\nlearns. In the second row, we see a unit that ﬁres on dog\\nfaces and dot arrays. The unit corresponding to the third row\\nis a red blob detector. There are also detectors for human\\nfaces and more abstract patterns such as text and triangular\\nstructures with windows. The network appears to learn a\\nrepresentation that combines a small number of class-tuned\\nfeatures together with a distributed representation of shape,\\ntexture, color, and material properties. The subsequent fully\\nconnected layer fc 6 has the ability to model a large set of\\ncompositions of these rich features.\\n3.2. Ablation studies\\nPerformance layer-by-layer, without ﬁne-tuning. To un-\\nderstand which layers are critical for detection performance,\\nwe analyzed results on the VOC 2007 dataset for each of the\\nCNN’s last three layers. Layer pool5 was brieﬂy described\\nin Section 3.1. The ﬁnal two layers are summarized below.\\nLayer fc6 is fully connected to pool 5. To compute fea-\\ntures, it multiplies a4096×9216 weight matrix by the pool5\\nfeature map (reshaped as a 9216-dimensional vector) and\\nthen adds a vector of biases. This intermediate vector is\\ncomponent-wise half-wave rectiﬁed (x←max(0,x)).\\nLayer fc7 is the ﬁnal layer of the network. It is imple-\\nmented by multiplying the features computed by fc 6 by a\\n4096 ×4096 weight matrix, and similarly adding a vector\\nof biases and applying half-wave rectiﬁcation.\\nWe start by looking at results from the CNN without\\nﬁne-tuning on PASCAL, i.e. all CNN parameters were\\npre-trained on ILSVRC 2012 only. Analyzing performance\\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\\nfc7 generalize worse than features from fc 6. This means\\nthat 29%, or about 16.8 million, of the CNN’s parameters\\ncan be removed without degrading mAP. More surprising is\\nthat removing both fc7 and fc6 produces quite good results\\neven though pool5 features are computed using only 6% of\\nthe CNN’s parameters. Much of the CNN’s representational\\npower comes from its convolutional layers, rather than from\\nthe much larger densely connected layers. This ﬁnding sug-\\ngests potential utility in computing a dense feature map, in\\nthe sense of HOG, of an arbitrary-sized image by using only\\nthe convolutional layers of the CNN. This representation\\nwould enable experimentation with sliding-window detec-\\ntors, including DPM, on top of pool5 features.\\nPerformance layer-by-layer, with ﬁne-tuning. We now\\nlook at results from our CNN after having ﬁne-tuned its pa-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='rameters on VOC 2007 trainval. The improvement is strik-\\ning (Table 2 rows 4-6): ﬁne-tuning increases mAP by 8.0\\npercentage points to 54.2%. The boost from ﬁne-tuning is\\nmuch larger for fc6 and fc7 than for pool5, which suggests\\nthat the pool 5 features learned from ImageNet are general\\nand that most of the improvement is gained from learning\\ndomain-speciﬁc non-linear classiﬁers on top of them.\\nComparison to recent feature learning methods. Rela-\\ntively few feature learning methods have been tried on PAS-\\nCAL VOC detection. We look at two recent approaches that\\nbuild on deformable part models. For reference, we also in-\\nclude results for the standard HOG-based DPM [20].\\nThe ﬁrst DPM feature learning method, DPM ST [28],\\naugments HOG features with histograms of “sketch token”\\nprobabilities. Intuitively, a sketch token is a tight distri-\\nbution of contours passing through the center of an image\\npatch. Sketch token probabilities are computed at each pixel\\nby a random forest that was trained to classify35×35 pixel\\npatches into one of 150 sketch tokens or background.\\nThe second method, DPM HSC [31], replaces HOG with\\nhistograms of sparse codes (HSC). To compute an HSC,\\nsparse code activations are solved for at each pixel using\\na learned dictionary of 100 7 ×7 pixel (grayscale) atoms.\\nThe resulting activations are rectiﬁed in three ways (full and\\nboth half-waves), spatially pooled, unit ℓ2 normalized, and\\nthen power transformed (x←sign(x)|x|α).\\nAll R-CNN variants strongly outperform the three DPM\\nbaselines (Table 2 rows 8-10), including the two that use\\nfeature learning. Compared to the latest version of DPM,\\nwhich uses only HOG features, our mAP is more than 20\\npercentage points higher: 54.2% vs. 33.7%— a 61% rela-\\ntive improvement. The combination of HOG and sketch to-\\nkens yields 2.5 mAP points over HOG alone, while HSC\\nimproves over HOG by 4 mAP points (when compared\\ninternally to their private DPM baselines—both use non-\\npublic implementations of DPM that underperform the open\\nsource version [20]). These methods achieve mAPs of\\n29.1% and 34.3%, respectively.\\n3.3. Network architectures\\nMost results in this paper use the network architecture\\nfrom Krizhevsky et al. [25]. However, we have found that\\nthe choice of architecture has a large effect on R-CNN de-\\ntection performance. In Table 3 we show results on VOC\\n2007 test using the 16-layer deep network recently proposed\\nby Simonyan and Zisserman [43]. This network was one of\\nthe top performers in the recent ILSVRC 2014 classiﬁca-\\ntion challenge. The network has a homogeneous structure\\nconsisting of 13 layers of 3 ×3 convolution kernels, with\\nﬁve max pooling layers interspersed, and topped with three\\nfully-connected layers. We refer to this network as “O-Net”\\nfor OxfordNet and the baseline as “T-Net” for TorontoNet.\\nTo use O-Net in R-CNN, we downloaded the pub-\\nlicly available pre-trained network weights for the\\nVGG ILSVRC 16 layers model from the Caffe Model\\nZoo.1 We then ﬁne-tuned the network using the same pro-\\ntocol as we used for T-Net. The only difference was to use\\nsmaller minibatches (24 examples) as required in order to\\nﬁt within GPU memory. The results in Table 3 show that R-\\nCNN with O-Net substantially outperforms R-CNN with T-\\nNet, increasing mAP from 58.5% to 66.0%. However there\\nis a considerable drawback in terms of compute time, with\\nthe forward pass of O-Net taking roughly 7 times longer\\nthan T-Net.\\n3.4. Detection error analysis\\nWe applied the excellent detection analysis tool from\\nHoiem et al. [23] in order to reveal our method’s error\\nmodes, understand how ﬁne-tuning changes them, and to\\nsee how our error types compare with DPM. A full sum-\\nmary of the analysis tool is beyond the scope of this pa-\\nper and we encourage readers to consult [23] to understand\\nsome ﬁner details (such as “normalized AP”). Since the\\nanalysis is best absorbed in the context of the associated\\nplots, we present the discussion within the captions of Fig-\\nure 5 and Figure 6.\\n3.5. Bounding-box regression\\nBased on the error analysis, we implemented a sim-\\nple method to reduce localization errors. Inspired by the\\nbounding-box regression employed in DPM [17], we train a\\nlinear regression model to predict a new detection window\\ngiven the pool 5 features for a selective search region pro-\\nposal. Full details are given in Appendix C. Results in Ta-\\nble 1, Table 2, and Figure 5 show that this simple approach\\nﬁxes a large number of mislocalized detections, boosting\\nmAP by 3 to 4 points.\\n3.6. Qualitative results\\nQualitative detection results on ILSVRC2013 are pre-\\nsented in Figure 8 and Figure 9 at the end of the paper. Each\\nimage was sampled randomly from the val 2 set and all de-\\ntections from all detectors with a precision greater than 0.5\\nare shown. Note that these are not curated and give a re-\\nalistic impression of the detectors in action. More qualita-\\ntive results are presented in Figure 10 and Figure 11, but\\nthese have been curated. We selected each image because it\\ncontained interesting, surprising, or amusing results. Here,\\nalso, all detections at precision greater than 0.5 are shown.\\n4. The ILSVRC2013 detection dataset\\nIn Section 2 we presented results on the ILSVRC2013\\ndetection dataset. This dataset is less homogeneous than\\n1https://github.com/BVLC/caffe/wiki/Model-Zoo\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='occ trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.212\\n0.612\\n0.420\\n0.557\\n0.201\\n0.720\\n0.344\\n0.606\\n0.351\\n0.677\\n0.244\\n0.609\\n0.516\\nnormalized AP\\nR−CNN fc6: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.179\\n0.701\\n0.498\\n0.634\\n0.335\\n0.766\\n0.442\\n0.672\\n0.429\\n0.723\\n0.325\\n0.685\\n0.593\\nnormalized AP\\nR−CNN FT fc7: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.211\\n0.731\\n0.542\\n0.676\\n0.385\\n0.786\\n0.484\\n0.709\\n0.453\\n0.779\\n0.368\\n0.720\\n0.633\\nnormalized AP\\nR−CNN FT fc7 BB: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.132\\n0.339\\n0.216\\n0.347\\n0.056\\n0.487\\n0.126\\n0.453\\n0.137\\n0.391\\n0.094\\n0.388\\n0.297\\nnormalized AP\\nDPM voc−release5: sensitivity and impact\\nFigure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and\\nlowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part\\nvisibility). We show plots for our method (R-CNN) with and without ﬁne-tuning (FT) and bounding-box regression (BB) as well as for\\nDPM voc-release5. Overall, ﬁne-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve\\nboth the highest and lowest performing subsets for nearly all characteristics. This indicates that ﬁne-tuning does more than simply improve\\nthe lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.\\nInstead, ﬁne-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\nFigure 5: Distribution of top-ranked false positive (FP) types.\\nEach plot shows the evolving distribution of FP types as more FPs\\nare considered in order of decreasing score. Each FP is catego-\\nrized into 1 of 4 types: Loc—poor localization (a detection with\\nan IoU overlap with the correct class between 0.1 and 0.5, or a du-\\nplicate); Sim—confusion with a similar category; Oth—confusion\\nwith a dissimilar object category; BG—a FP that ﬁred on back-\\nground. Compared with DPM (see [23]), signiﬁcantly more of\\nour errors result from poor localization, rather than confusion with\\nbackground or other object classes, indicating that the CNN fea-\\ntures are much more discriminative than HOG. Loose localiza-\\ntion likely results from our use of bottom-up region proposals and\\nthe positional invariance learned from pre-training the CNN for\\nwhole-image classiﬁcation. Column three shows how our simple\\nbounding-box regression method ﬁxes many localization errors.\\nPASCAL VOC, requiring choices about how to use it. Since\\nthese decisions are non-trivial, we cover them in this sec-\\ntion.\\n4.1. Dataset overview\\nThe ILSVRC2013 detection dataset is split into three\\nsets: train (395,918), val (20,121), and test (40,152), where\\nthe number of images in each set is in parentheses. The\\nval and test splits are drawn from the same image distribu-\\ntion. These images are scene-like and similar in complexity\\n(number of objects, amount of clutter, pose variability, etc.)\\nto PASCAL VOC images. The val and test splits are exhaus-\\ntively annotated, meaning that in each image all instances\\nfrom all 200 classes are labeled with bounding boxes. The\\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\\nsiﬁcation image distribution. These images have more vari-\\nable complexity with a skew towards images of a single cen-\\ntered object. Unlike val and test, the train images (due to\\ntheir large number) are not exhaustively annotated. In any\\ngiven train image, instances from the 200 classes may or\\nmay not be labeled. In addition to these image sets, each\\nclass has an extra set of negative images. Negative images\\nare manually checked to validate that they do not contain\\nany instances of their associated class. The negative im-\\nage sets were not used in this work. More information on\\nhow ILSVRC was collected and annotated can be found in\\n[11, 36].\\nThe nature of these splits presents a number of choices\\nfor training R-CNN. The train images cannot be used for\\nhard negative mining, because annotations are not exhaus-\\ntive. Where should negative examples come from? Also,\\nthe train images have different statistics than val and test.\\nShould the train images be used at all, and if so, to what\\nextent? While we have not thoroughly evaluated a large\\nnumber of choices, we present what seemed like the most\\nobvious path based on previous experience.\\nOur general strategy is to rely heavily on the val set and\\nuse some of the train images as an auxiliary source of pos-\\nitive examples. To use val for both training and valida-\\ntion, we split it into roughly equally sized “val1” and “val2”\\nsets. Since some classes have very few examples in val (the\\nsmallest has only 31 and half have fewer than 110), it is\\nimportant to produce an approximately class-balanced par-\\ntition. To do this, a large number of candidate splits were\\ngenerated and the one with the smallest maximum relative\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='class imbalance was selected. 2 Each candidate split was\\ngenerated by clustering val images using their class counts\\nas features, followed by a randomized local search that may\\nimprove the split balance. The particular split used here has\\na maximum relative imbalance of about 11% and a median\\nrelative imbalance of 4%. The val1/val2 split and code used\\nto produce them will be publicly available to allow other re-\\nsearchers to compare their methods on the val splits used in\\nthis report.\\n4.2. Region proposals\\nWe followed the same region proposal approach that was\\nused for detection on PASCAL. Selective search [39] was\\nrun in “fast mode” on each image in val1, val2, and test (but\\nnot on images in train). One minor modiﬁcation was re-\\nquired to deal with the fact that selective search is not scale\\ninvariant and so the number of regions produced depends\\non the image resolution. ILSVRC image sizes range from\\nvery small to a few that are several mega-pixels, and so we\\nresized each image to a ﬁxed width (500 pixels) before run-\\nning selective search. On val, selective search resulted in an\\naverage of 2403 region proposals per image with a 91.6%\\nrecall of all ground-truth bounding boxes (at 0.5 IoU thresh-\\nold). This recall is notably lower than in PASCAL, where\\nit is approximately 98%, indicating signiﬁcant room for im-\\nprovement in the region proposal stage.\\n4.3. Training data\\nFor training data, we formed a set of images and boxes\\nthat includes all selective search and ground-truth boxes\\nfrom val 1 together with up to N ground-truth boxes per\\nclass from train (if a class has fewer than N ground-truth\\nboxes in train, then we take all of them). We’ll call this\\ndataset of images and boxes val 1+trainN. In an ablation\\nstudy, we show mAP on val2 for N ∈{0,500,1000}(Sec-\\ntion 4.5).\\nTraining data is required for three procedures in R-CNN:\\n(1) CNN ﬁne-tuning, (2) detector SVM training, and (3)\\nbounding-box regressor training. CNN ﬁne-tuning was run\\nfor 50k SGD iteration on val1+trainN using the exact same\\nsettings as were used for PASCAL. Fine-tuning on a sin-\\ngle NVIDIA Tesla K20 took 13 hours using Caffe. For\\nSVM training, all ground-truth boxes from val 1+trainN\\nwere used as positive examples for their respective classes.\\nHard negative mining was performed on a randomly se-\\nlected subset of 5000 images from val 1. An initial experi-\\nment indicated that mining negatives from all of val1, versus\\na 5000 image subset (roughly half of it), resulted in only a\\n0.5 percentage point drop in mAP, while cutting SVM train-\\ning time in half. No negative examples were taken from\\n2Relative imbalance is measured as |a −b|/(a + b) where a and b are\\nclass counts in each half of the split.\\ntrain because the annotations are not exhaustive. The ex-\\ntra sets of veriﬁed negative images were not used. The\\nbounding-box regressors were trained on val1.\\n4.4. Validation and evaluation\\nBefore submitting results to the evaluation server, we\\nvalidated data usage choices and the effect of ﬁne-tuning\\nand bounding-box regression on the val2 set using the train-\\ning data described above. All system hyperparameters (e.g.,\\nSVM C hyperparameters, padding used in region warp-\\ning, NMS thresholds, bounding-box regression hyperpa-\\nrameters) were ﬁxed at the same values used for PAS-\\nCAL. Undoubtedly some of these hyperparameter choices\\nare slightly suboptimal for ILSVRC, however the goal of\\nthis work was to produce a preliminary R-CNN result on\\nILSVRC without extensive dataset tuning. After selecting\\nthe best choices on val 2, we submitted exactly two result\\nﬁles to the ILSVRC2013 evaluation server. The ﬁrst sub-\\nmission was without bounding-box regression and the sec-\\nond submission was with bounding-box regression. For\\nthese submissions, we expanded the SVM and bounding-\\nbox regressor training sets to use val +train1k and val, re-\\nspectively. We used the CNN that was ﬁne-tuned on\\nval1+train1k to avoid re-running ﬁne-tuning and feature\\ncomputation.\\n4.5. Ablation study\\nTable 4 shows an ablation study of the effects of differ-\\nent amounts of training data, ﬁne-tuning, and bounding-\\nbox regression. A ﬁrst observation is that mAP on val 2\\nmatches mAP on test very closely. This gives us conﬁ-\\ndence that mAP on val 2 is a good indicator of test set per-\\nformance. The ﬁrst result, 20.9%, is what R-CNN achieves\\nusing a CNN pre-trained on the ILSVRC2012 classiﬁca-\\ntion dataset (no ﬁne-tuning) and given access to the small\\namount of training data in val1 (recall that half of the classes\\nin val 1 have between 15 and 55 examples). Expanding\\nthe training set to val 1+trainN improves performance to\\n24.1%, with essentially no difference between N = 500\\nand N = 1000. Fine-tuning the CNN using examples from\\njust val1 gives a modest improvement to 26.5%, however\\nthere is likely signiﬁcant overﬁtting due to the small number\\nof positive training examples. Expanding the ﬁne-tuning\\nset to val1+train1k, which adds up to 1000 positive exam-\\nples per class from the train set, helps signiﬁcantly, boosting\\nmAP to 29.7%. Bounding-box regression improves results\\nto 31.0%, which is a smaller relative gain that what was ob-\\nserved in PASCAL.\\n4.6. Relationship to OverFeat\\nThere is an interesting relationship between R-CNN and\\nOverFeat: OverFeat can be seen (roughly) as a special case\\nof R-CNN. If one were to replace selective search region\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='test set val2 val2 val2 val2 val2 val2 test test\\nSVM training set val1 val1+train.5k val1+train1k val1+train1k val1+train1k val1+train1k val+train1k val+train1k\\nCNN ﬁne-tuning set n/a n/a n/a val 1 val1+train1k val1+train1k val1+train1k val1+train1k\\nbbox reg set n/a n/a n/a n/a n/a val 1 n/a val\\nCNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7\\nmAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4\\nmedian AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3\\nTable 4: ILSVRC2013 ablation study of data usage choices, ﬁne-tuning, and bounding-box regression.\\nproposals with a multi-scale pyramid of regular square re-\\ngions and change the per-class bounding-box regressors to\\na single bounding-box regressor, then the systems would\\nbe very similar (modulo some potentially signiﬁcant differ-\\nences in how they are trained: CNN detection ﬁne-tuning,\\nusing SVMs, etc.). It is worth noting that OverFeat has\\na signiﬁcant speed advantage over R-CNN: it is about 9x\\nfaster, based on a ﬁgure of 2 seconds per image quoted from\\n[34]. This speed comes from the fact that OverFeat’s slid-\\ning windows (i.e., region proposals) are not warped at the\\nimage level and therefore computation can be easily shared\\nbetween overlapping windows. Sharing is implemented by\\nrunning the entire network in a convolutional fashion over\\narbitrary-sized inputs. Speeding up R-CNN should be pos-\\nsible in a variety of ways and remains as future work.\\n5. Semantic segmentation\\nRegion classiﬁcation is a standard technique for seman-\\ntic segmentation, allowing us to easily apply R-CNN to the\\nPASCAL VOC segmentation challenge. To facilitate a di-\\nrect comparison with the current leading semantic segmen-\\ntation system (called O 2P for “second-order pooling”) [4],\\nwe work within their open source framework. O 2P uses\\nCPMC to generate 150 region proposals per image and then\\npredicts the quality of each region, for each class, using\\nsupport vector regression (SVR). The high performance of\\ntheir approach is due to the quality of the CPMC regions\\nand the powerful second-order pooling of multiple feature\\ntypes (enriched variants of SIFT and LBP). We also note\\nthat Farabet et al. [16] recently demonstrated good results\\non several dense scene labeling datasets (not including PAS-\\nCAL) using a CNN as a multi-scale per-pixel classiﬁer.\\nWe follow [2, 4] and extend the PASCAL segmentation\\ntraining set to include the extra annotations made available\\nby Hariharan et al. [22]. Design decisions and hyperparam-\\neters were cross-validated on the VOC 2011 validation set.\\nFinal test results were evaluated only once.\\nCNN features for segmentation. We evaluate three strate-\\ngies for computing features on CPMC regions, all of which\\nbegin by warping the rectangular window around the re-\\ngion to 227 ×227. The ﬁrst strategy ( full) ignores the re-\\ngion’s shape and computes CNN features directly on the\\nwarped window, exactly as we did for detection. However,\\nthese features ignore the non-rectangular shape of the re-\\ngion. Two regions might have very similar bounding boxes\\nwhile having very little overlap. Therefore, the second strat-\\negy (fg) computes CNN features only on a region’s fore-\\nground mask. We replace the background with the mean\\ninput so that background regions are zero after mean sub-\\ntraction. The third strategy ( full+fg) simply concatenates\\nthe full and fg features; our experiments validate their com-\\nplementarity.\\nfull R-CNN fg R-CNN full+fg R-CNN\\nO2P [4] fc6 fc7 fc6 fc7 fc6 fc7\\n46.4 43.0 42.5 43.7 42.1 47.9 45.8\\nTable 5: Segmentation mean accuracy (%) on VOC 2011 vali-\\ndation. Column 1 presents O2P; 2-7 use our CNN pre-trained on\\nILSVRC 2012.\\nResults on VOC 2011. Table 5 shows a summary of our\\nresults on the VOC 2011 validation set compared with O2P.\\n(See Appendix E for complete per-category results.) Within\\neach feature computation strategy, layer fc6 always outper-\\nforms fc7 and the following discussion refers to the fc6 fea-\\ntures. The fg strategy slightly outperforms full, indicating\\nthat the masked region shape provides a stronger signal,\\nmatching our intuition. However, full+fg achieves an aver-\\nage accuracy of 47.9%, our best result by a margin of 4.2%\\n(also modestly outperforming O2P), indicating that the con-\\ntext provided by the full features is highly informative even\\ngiven the fg features. Notably, training the 20 SVRs on our\\nfull+fg features takes an hour on a single core, compared to\\n10+ hours for training on O2P features.\\nIn Table 6 we present results on the VOC 2011 test\\nset, comparing our best-performing method, fc 6 (full+fg),\\nagainst two strong baselines. Our method achieves the high-\\nest segmentation accuracy for 11 out of 21 categories, and\\nthe highest overall segmentation accuracy of 47.9%, aver-\\naged across categories (but likely ties with the O 2P result\\nunder any reasonable margin of error). Still better perfor-\\nmance could likely be achieved by ﬁne-tuning.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='VOC 2011 test bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nR&P [2] 83.446.8 18.9 36.6 31.2 42.7 57.3 47.4 44.1 8.1 39.436.136.3 49.5 48.3 50.7 26.3 47.2 22.1 42.0 43.2 40.8\\nO2P [4] 85.469.722.3 45.244.4 46.9 66.7 57.8 56.213.5 46.132.3 41.259.1 55.3 51.0 36.2 50.4 27.846.944.6 47.6\\nours(full+fgR-CNN fc6) 84.266.923.7 58.337.4 55.4 73.3 58.7 56.59.7 45.5 29.549.3 40.1 57.8 53.9 33.8 60.7 22.747.141.3 47.9\\nTable 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the “Regions and Parts” (R&P)\\nmethod of [2] and the second-order pooling (O 2P) method of [4]. Without any ﬁne-tuning, our CNN achieves top segmentation perfor-\\nmance, outperforming R&P and roughly matching O2P.\\n6. Conclusion\\nIn recent years, object detection performance had stag-\\nnated. The best performing systems were complex en-\\nsembles combining multiple low-level image features with\\nhigh-level context from object detectors and scene classi-\\nﬁers. This paper presents a simple and scalable object de-\\ntection algorithm that gives a 30% relative improvement\\nover the best previous results on PASCAL VOC 2012.\\nWe achieved this performance through two insights. The\\nﬁrst is to apply high-capacity convolutional neural net-\\nworks to bottom-up region proposals in order to localize\\nand segment objects. The second is a paradigm for train-\\ning large CNNs when labeled training data is scarce. We\\nshow that it is highly effective to pre-train the network—\\nwith supervision—for a auxiliary task with abundant data\\n(image classiﬁcation) and then to ﬁne-tune the network for\\nthe target task where data is scarce (detection). We conjec-\\nture that the “supervised pre-training/domain-speciﬁc ﬁne-\\ntuning” paradigm will be highly effective for a variety of\\ndata-scarce vision problems.\\nWe conclude by noting that it is signiﬁcant that we\\nachieved these results by using a combination of classi-\\ncal tools from computer vision and deep learning (bottom-\\nup region proposals and convolutional neural networks).\\nRather than opposing lines of scientiﬁc inquiry, the two are\\nnatural and inevitable partners.\\nAcknowledgments. This research was supported in part\\nby DARPA Mind’s Eye and MSEE programs, by NSF\\nawards IIS-0905647, IIS-1134072, and IIS-1212798,\\nMURI N000014-10-1-0933, and by support from Toyota.\\nThe GPUs used in this research were generously donated\\nby the NVIDIA Corporation.\\nAppendix\\nA. Object proposal transformations\\nThe convolutional neural network used in this work re-\\nquires a ﬁxed-size input of 227 ×227 pixels. For detec-\\ntion, we consider object proposals that are arbitrary image\\nrectangles. We evaluated two approaches for transforming\\nobject proposals into valid CNN inputs.\\nThe ﬁrst method (“tightest square with context”) en-\\ncloses each object proposal inside the tightest square and\\n(A) (B) (C) (D)\\n (A) (B) (C) (D)\\nFigure 7: Different object proposal transformations. (A) the\\noriginal object proposal at its actual scale relative to the trans-\\nformed CNN inputs; (B) tightest square with context; (C) tight-\\nest square without context; (D) warp. Within each column and\\nexample proposal, the top row corresponds top = 0pixels of con-\\ntext padding while the bottom row has p = 16 pixels of context\\npadding.\\nthen scales (isotropically) the image contained in that\\nsquare to the CNN input size. Figure 7 column (B) shows\\nthis transformation. A variant on this method (“tightest\\nsquare without context”) excludes the image content that\\nsurrounds the original object proposal. Figure 7 column\\n(C) shows this transformation. The second method (“warp”)\\nanisotropically scales each object proposal to the CNN in-\\nput size. Figure 7 column (D) shows the warp transforma-\\ntion.\\nFor each of these transformations, we also consider in-\\ncluding additional image context around the original object\\nproposal. The amount of context padding (p) is deﬁned as a\\nborder size around the original object proposal in the trans-\\nformed input coordinate frame. Figure 7 shows p = 0pix-\\nels in the top row of each example and p = 16 pixels in\\nthe bottom row. In all methods, if the source rectangle ex-\\ntends beyond the image, the missing data is replaced with\\nthe image mean (which is then subtracted before inputing\\nthe image into the CNN). A pilot set of experiments showed\\nthat warping with context padding ( p = 16pixels) outper-\\nformed the alternatives by a large margin (3-5 mAP points).\\nObviously more alternatives are possible, including using\\nreplication instead of mean padding. Exhaustive evaluation\\nof these alternatives is left as future work.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='B. Positive vs. negative examples and softmax\\nTwo design choices warrant further discussion. The ﬁrst\\nis: Why are positive and negative examples deﬁned differ-\\nently for ﬁne-tuning the CNN versus training the object de-\\ntection SVMs? To review the deﬁnitions brieﬂy, for ﬁne-\\ntuning we map each object proposal to the ground-truth in-\\nstance with which it has maximum IoU overlap (if any) and\\nlabel it as a positive for the matched ground-truth class if the\\nIoU is at least 0.5. All other proposals are labeled “back-\\nground” (i.e., negative examples for all classes). For train-\\ning SVMs, in contrast, we take only the ground-truth boxes\\nas positive examples for their respective classes and label\\nproposals with less than 0.3 IoU overlap with all instances\\nof a class as a negative for that class. Proposals that fall\\ninto the grey zone (more than 0.3 IoU overlap, but are not\\nground truth) are ignored.\\nHistorically speaking, we arrived at these deﬁnitions be-\\ncause we started by training SVMs on features computed\\nby the ImageNet pre-trained CNN, and so ﬁne-tuning was\\nnot a consideration at that point in time. In that setup, we\\nfound that our particular label deﬁnition for training SVMs\\nwas optimal within the set of options we evaluated (which\\nincluded the setting we now use for ﬁne-tuning). When we\\nstarted using ﬁne-tuning, we initially used the same positive\\nand negative example deﬁnition as we were using for SVM\\ntraining. However, we found that results were much worse\\nthan those obtained using our current deﬁnition of positives\\nand negatives.\\nOur hypothesis is that this difference in how positives\\nand negatives are deﬁned is not fundamentally important\\nand arises from the fact that ﬁne-tuning data is limited.\\nOur current scheme introduces many “jittered” examples\\n(those proposals with overlap between 0.5 and 1, but not\\nground truth), which expands the number of positive exam-\\nples by approximately 30x. We conjecture that this large\\nset is needed when ﬁne-tuning the entire network to avoid\\noverﬁtting. However, we also note that using these jittered\\nexamples is likely suboptimal because the network is not\\nbeing ﬁne-tuned for precise localization.\\nThis leads to the second issue: Why, after ﬁne-tuning,\\ntrain SVMs at all? It would be cleaner to simply apply the\\nlast layer of the ﬁne-tuned network, which is a 21-way soft-\\nmax regression classiﬁer, as the object detector. We tried\\nthis and found that performance on VOC 2007 dropped\\nfrom 54.2% to 50.9% mAP. This performance drop likely\\narises from a combination of several factors including that\\nthe deﬁnition of positive examples used in ﬁne-tuning does\\nnot emphasize precise localization and the softmax classi-\\nﬁer was trained on randomly sampled negative examples\\nrather than on the subset of “hard negatives” used for SVM\\ntraining.\\nThis result shows that it’s possible to obtain close to\\nthe same level of performance without training SVMs af-\\nter ﬁne-tuning. We conjecture that with some additional\\ntweaks to ﬁne-tuning the remaining performance gap may\\nbe closed. If true, this would simplify and speed up R-CNN\\ntraining with no loss in detection performance.\\nC. Bounding-box regression\\nWe use a simple bounding-box regression stage to im-\\nprove localization performance. After scoring each selec-\\ntive search proposal with a class-speciﬁc detection SVM,\\nwe predict a new bounding box for the detection using a\\nclass-speciﬁc bounding-box regressor. This is similar in\\nspirit to the bounding-box regression used in deformable\\npart models [17]. The primary difference between the two\\napproaches is that here we regress from features computed\\nby the CNN, rather than from geometric features computed\\non the inferred DPM part locations.\\nThe input to our training algorithm is a set of N train-\\ning pairs {(Pi,Gi)}i=1,...,N, where Pi = (Pi\\nx,Pi\\ny,Pi\\nw,Pi\\nh)\\nspeciﬁes the pixel coordinates of the center of proposalPi’s\\nbounding box together with Pi’s width and height in pixels.\\nHence forth, we drop the superscript iunless it is needed.\\nEach ground-truth bounding box Gis speciﬁed in the same\\nway: G = (Gx,Gy,Gw,Gh). Our goal is to learn a trans-\\nformation that maps a proposed boxP to a ground-truth box\\nG.\\nWe parameterize the transformation in terms of four\\nfunctions dx(P), dy(P), dw(P), and dh(P). The ﬁrst\\ntwo specify a scale-invariant translation of the center of\\nP’s bounding box, while the second two specify log-space\\ntranslations of the width and height of P’s bounding box.\\nAfter learning these functions, we can transform an input\\nproposal P into a predicted ground-truth box ˆGby apply-\\ning the transformation\\nˆGx = Pwdx(P) +Px (1)\\nˆGy = Phdy(P) +Py (2)\\nˆGw = Pwexp(dw(P)) (3)\\nˆGh = Phexp(dh(P)). (4)\\nEach function d⋆(P) (where ⋆ is one of x,y,h,w ) is\\nmodeled as a linear function of the pool 5 features of pro-\\nposal P, denoted by φ5(P). (The dependence of φ5(P)\\non the image data is implicitly assumed.) Thus we have\\nd⋆(P) = wT\\n⋆φ5(P), where w⋆ is a vector of learnable\\nmodel parameters. We learn w⋆ by optimizing the regu-\\nlarized least squares objective (ridge regression):\\nw⋆ = argmin\\nˆw⋆\\nN∑\\ni\\n(ti\\n⋆ −ˆwT\\n⋆φ5(Pi))2 + λ∥ˆw⋆∥2 . (5)\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='The regression targets t⋆ for the training pair (P,G) are de-\\nﬁned as\\ntx = (Gx −Px)/Pw (6)\\nty = (Gy −Py)/Ph (7)\\ntw = log(Gw/Pw) (8)\\nth = log(Gh/Ph). (9)\\nAs a standard regularized least squares problem, this can be\\nsolved efﬁciently in closed form.\\nWe found two subtle issues while implementing\\nbounding-box regression. The ﬁrst is that regularization\\nis important: we set λ = 1000 based on a validation set.\\nThe second issue is that care must be taken when selecting\\nwhich training pairs (P,G) to use. Intuitively, if P is far\\nfrom all ground-truth boxes, then the task of transforming\\nP to a ground-truth box Gdoes not make sense. Using ex-\\namples like P would lead to a hopeless learning problem.\\nTherefore, we only learn from a proposal P if it is nearby\\nat least one ground-truth box. We implement “nearness” by\\nassigning P to the ground-truth box G with which it has\\nmaximum IoU overlap (in case it overlaps more than one) if\\nand only if the overlap is greater than a threshold (which we\\nset to 0.6 using a validation set). All unassigned proposals\\nare discarded. We do this once for each object class in order\\nto learn a set of class-speciﬁc bounding-box regressors.\\nAt test time, we score each proposal and predict its new\\ndetection window only once. In principle, we could iterate\\nthis procedure (i.e., re-score the newly predicted bounding\\nbox, and then predict a new bounding box from it, and so\\non). However, we found that iterating does not improve\\nresults.\\nD. Additional feature visualizations\\nFigure 12 shows additional visualizations for 20 pool 5\\nunits. For each unit, we show the 24 region proposals that\\nmaximally activate that unit out of the full set of approxi-\\nmately 10 million regions in all of VOC 2007 test.\\nWe label each unit by its (y, x, channel) position in the\\n6 ×6 ×256 dimensional pool5 feature map. Within each\\nchannel, the CNN computes exactly the same function of\\nthe input region, with the (y, x) position changing only the\\nreceptive ﬁeld.\\nE. Per-category segmentation results\\nIn Table 7 we show the per-category segmentation ac-\\ncuracy on VOC 2011 val for each of our six segmentation\\nmethods in addition to the O 2P method [4]. These results\\nshow which methods are strongest across each of the 20\\nPASCAL classes, plus the background class.\\nF. Analysis of cross-dataset redundancy\\nOne concern when training on an auxiliary dataset is that\\nthere might be redundancy between it and the test set. Even\\nthough the tasks of object detection and whole-image clas-\\nsiﬁcation are substantially different, making such cross-set\\nredundancy much less worrisome, we still conducted a thor-\\nough investigation that quantiﬁes the extent to which PAS-\\nCAL test images are contained within the ILSVRC 2012\\ntraining and validation sets. Our ﬁndings may be useful to\\nresearchers who are interested in using ILSVRC 2012 as\\ntraining data for the PASCAL image classiﬁcation task.\\nWe performed two checks for duplicate (and near-\\nduplicate) images. The ﬁrst test is based on exact matches\\nof ﬂickr image IDs, which are included in the VOC 2007\\ntest annotations (these IDs are intentionally kept secret for\\nsubsequent PASCAL test sets). All PASCAL images, and\\nabout half of ILSVRC, were collected from ﬂickr.com. This\\ncheck turned up 31 matches out of 4952 (0.63%).\\nThe second check uses GIST [30] descriptor matching,\\nwhich was shown in [13] to have excellent performance at\\nnear-duplicate image detection in large (>1 million) image\\ncollections. Following [13], we computed GIST descrip-\\ntors on warped 32 ×32 pixel versions of all ILSVRC 2012\\ntrainval and PASCAL 2007 test images.\\nEuclidean distance nearest-neighbor matching of GIST\\ndescriptors revealed 38 near-duplicate images (including all\\n31 found by ﬂickr ID matching). The matches tend to vary\\nslightly in JPEG compression level and resolution, and to a\\nlesser extent cropping. These ﬁndings show that the overlap\\nis small, less than 1%. For VOC 2012, because ﬂickr IDs\\nare not available, we used the GIST matching method only.\\nBased on GIST matches, 1.5% of VOC 2012 test images\\nare in ILSVRC 2012 trainval. The slightly higher rate for\\nVOC 2012 is likely due to the fact that the two datasets\\nwere collected closer together in time than VOC 2007 and\\nILSVRC 2012 were.\\nG. Document changelog\\nThis document tracks the progress of R-CNN. To help\\nreaders understand how it has changed over time, here’s a\\nbrief changelog describing the revisions.\\nv1 Initial version.\\nv2 CVPR 2014 camera-ready revision. Includes substan-\\ntial improvements in detection performance brought about\\nby (1) starting ﬁne-tuning from a higher learning rate (0.001\\ninstead of 0.0001), (2) using context padding when prepar-\\ning CNN inputs, and (3) bounding-box regression to ﬁx lo-\\ncalization errors.\\nv3 Results on the ILSVRC2013 detection dataset and com-\\nparison with OverFeat were integrated into several sections\\n(primarily Section 2 and Section 4).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='VOC 2011 val bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nO2P [4] 84.069.021.7 47.7 42.2 42.464.7 65.857.4 12.9 37.4 20.5 43.7 35.7 52.7 51.0 35.8 51.0 28.4 59.8 49.746.4\\nfullR-CNN fc6 81.356.2 23.9 42.9 40.7 38.8 59.2 56.5 53.2 11.4 34.6 16.7 48.1 37.0 51.4 46.0 31.5 44.0 24.3 53.7 51.143.0\\nfullR-CNN fc7 81.052.825.143.8 40.5 42.7 55.4 57.7 51.3 8.7 32.5 11.5 48.1 37.0 50.5 46.4 30.2 42.1 21.2 57.7 56.0 42.5\\nfgR-CNN fc6 81.454.1 21.1 40.6 38.753.6 59.9 57.2 52.5 9.1 36.5 23.6 46.4 38.1 53.2 51.3 32.2 38.7 29.053.0 47.543.7\\nfgR-CNN fc7 80.950.1 20.0 40.2 34.1 40.9 59.7 59.8 52.7 7.3 32.1 14.3 48.8 42.9 54.0 48.6 28.9 42.6 24.9 52.2 48.8 42.1\\nfull+fgR-CNN fc6 83.160.4 23.2 48.447.3 52.6 61.6 60.659.1 10.8 45.8 20.9 57.7 43.3 57.4 52.9 34.7 48.7 28.1 60.0 48.647.9\\nfull+fgR-CNN fc7 82.356.7 20.649.944.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.155.2 45.7\\nTable 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.\\nv4 The softmax vs. SVM results in Appendix B contained\\nan error, which has been ﬁxed. We thank Sergio Guadar-\\nrama for helping to identify this issue.\\nv5 Added results using the new 16-layer network architec-\\nture from Simonyan and Zisserman [43] to Section 3.3 and\\nTable 3.\\nReferences\\n[1] B. Alexe, T. Deselaers, and V . Ferrari. Measuring the object-\\nness of image windows. TPAMI, 2012. 2\\n[2] P. Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\\nJ. Malik. Semantic segmentation using regions and parts. In\\nCVPR, 2012. 10, 11\\n[3] P. Arbel ´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-\\nlik. Multiscale combinatorial grouping. In CVPR, 2014. 3\\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\\nmantic segmentation with second-order pooling. In ECCV,\\n2012. 4, 10, 11, 13, 14\\n[5] J. Carreira and C. Sminchisescu. CPMC: Automatic ob-\\nject segmentation using constrained parametric min-cuts.\\nTPAMI, 2012. 2, 3\\n[6] D. Cires ¸an, A. Giusti, L. Gambardella, and J. Schmidhu-\\nber. Mitosis detection in breast cancer histology images with\\ndeep neural networks. In MICCAI, 2013. 3\\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In CVPR, 2005. 1\\n[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, and J. Yagnik. Fast, accurate detection of\\n100,000 object classes on a single machine. In CVPR, 2013.\\n3\\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\\nFei. ImageNet Large Scale Visual Recognition Competition\\n2012 (ILSVRC2012). http://www.image-net.org/\\nchallenges/LSVRC/2012/. 1\\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. ImageNet: A large-scale hierarchical image database.\\nIn CVPR, 2009. 1\\n[11] J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. C.\\nBerg, and L. Fei-Fei. Scalable multi-label annotation. In\\nCHI, 2014. 8\\n[12] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. InICML,\\n2014. 2\\n[13] M. Douze, H. J ´egou, H. Sandhawalia, L. Amsaleg, and\\nC. Schmid. Evaluation of gist descriptors for web-scale im-\\nage search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\\nChallenge. IJCV, 2010. 1, 4\\n[16] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. TPAMI, 2013. 10\\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. TPAMI, 2010. 2, 4, 7, 12\\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up\\nsegmentation for top-down detection. In CVPR, 2013. 4, 5\\n[19] K. Fukushima. Neocognitron: A self-organizing neu-\\nral network model for a mechanism of pattern recogni-\\ntion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-\\nnatively trained deformable part models, release 5. http:\\n//www.cs.berkeley.edu/˜rbg/latent-v5/. 2,\\n5, 6, 7\\n[21] C. Gu, J. J. Lim, P. Arbel ´aez, and J. Malik. Recognition\\nusing regions. In CVPR, 2009. 2\\n[22] B. Hariharan, P. Arbel ´aez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In ICCV, 2011.\\n10\\n[23] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In ECCV. 2012. 2, 7, 8\\n[24] Y . Jia. Caffe: An open source convolutional archi-\\ntecture for fast feature embedding. http://caffe.\\nberkeleyvision.org/, 2013. 3\\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\\nsiﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 3, 4, 7\\n[26] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\\nW. Hubbard, and L. Jackel. Backpropagation applied to\\nhandwritten zip code recognition. Neural Comp., 1989. 1\\n[27] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. of the\\nIEEE, 1998. 1\\n[28] J. J. Lim, C. L. Zitnick, and P. Doll ´ar. Sketch tokens: A\\nlearned mid-level representation for contour and object de-\\ntection. In CVPR, 2013. 6, 7\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='class AP class AP class AP class AP class AP\\naccordion 50.8 centipede 30.4 hair spray 13.8 pencil box 11.4 snowplow 69.2\\nairplane 50.0 chain saw 14.1 hamburger 34.2 pencil sharpener 9.0 soap dispenser 16.8\\nant 31.8 chair 19.5 hammer 9.9 perfume 32.8 soccer ball 43.7\\nantelope 53.8 chime 24.6 hamster 46.0 person 41.7 sofa 16.3\\napple 30.9 cocktail shaker 46.2 harmonica 12.6 piano 20.5 spatula 6.8\\narmadillo 54.0 coffee maker 21.5 harp 50.4 pineapple 22.6 squirrel 31.3\\nartichoke 45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball 21.0 starﬁsh 45.1\\naxe 11.8 computer mouse 21.2 head cabbage 17.4 pitcher 19.2 stethoscope 18.3\\nbaby bed 42.0 corkscrew 24.2 helmet 33.4 pizza 43.7 stove 8.1\\nbackpack 2.8 cream 29.9 hippopotamus 38.0 plastic bag 6.4 strainer 9.9\\nbagel 37.5 croquet ball 30.0 horizontal bar 7.0 plate rack 15.2 strawberry 26.8\\nbalance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2\\nbanana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8\\nband aid 17.4 cup or mug 34.0 iPod 59.2 porcupine 37.2 swimming trunks 9.1\\nbanjo 55.3 diaper 10.1 isopod 19.5 power drill 7.9 swine 45.3\\nbaseball 41.8 digital clock 18.5 jellyﬁsh 23.7 pretzel 24.8 syringe 5.7\\nbasketball 65.3 dishwasher 19.9 koala bear 44.3 printer 21.3 table 21.7\\nbathing cap 37.2 dog 76.8 ladle 3.0 puck 14.1 tape player 21.4\\nbeaker 11.3 domestic cat 44.1 ladybug 58.4 punching bag 29.4 tennis ball 59.1\\nbear 62.7 dragonﬂy 27.8 lamp 9.1 purse 8.0 tick 42.6\\nbee 52.9 drum 19.9 laptop 35.4 rabbit 71.0 tie 24.6\\nbell pepper 38.8 dumbbell 14.1 lemon 33.3 racket 16.2 tiger 61.8\\nbench 12.7 electric fan 35.0 lion 51.3 ray 41.1 toaster 29.2\\nbicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 trafﬁc light 24.7\\nbinder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8\\nbird 70.9 ﬁg 44.5 lobster 32.4 remote control 41.6 trombone 13.8\\nbookshelf 19.3 ﬁling cabinet 20.6 maillot 31.0 rubber eraser 2.5 trumpet 14.4\\nbow tie 38.8 ﬂower pot 20.2 maraca 30.1 rugby ball 34.5 turtle 59.1\\nbow 9.0 ﬂute 4.9 microphone 4.0 ruler 11.5 tv or monitor 41.7\\nbowl 26.7 fox 59.3 microwave 40.1 salt or pepper shaker 24.6 unicycle 27.2\\nbrassiere 31.2 french horn 24.2 milk can 33.3 saxophone 40.8 vacuum 19.5\\nburrito 25.7 frog 64.1 miniskirt 14.9 scorpion 57.3 violin 13.7\\nbus 57.5 frying pan 21.5 monkey 49.6 screwdriver 10.6 volleyball 59.7\\nbutterﬂy 88.5 giant panda 42.5 motorcycle 42.2 seal 20.9 wafﬂe iron 24.0\\ncamel 37.6 goldﬁsh 28.6 mushroom 31.8 sheep 48.9 washer 39.8\\ncan opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1\\ncar 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9\\ncart 48.0 guacamole 32.3 oboe 27.5 snail 36.2 whale 48.6\\ncattle 32.3 guitar 33.1 orange 38.8 snake 33.8 wine bottle 31.2\\ncello 28.9 hair dryer 13.0 otter 22.2 snowmobile 58.8 zebra 49.6\\nTable 8: Per-class average precision (%) on the ILSVRC2013 detection test set.\\n[29] D. Lowe. Distinctive image features from scale-invariant\\nkeypoints. IJCV, 2004. 1\\n[30] A. Oliva and A. Torralba. Modeling the shape of the scene:\\nA holistic representation of the spatial envelope.IJCV, 2001.\\n13\\n[31] X. Ren and D. Ramanan. Histograms of sparse codes for\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='lemon 0.79\\nlemon 0.70\\nlemon 0.56lemon 0.50\\nperson 0.88\\nperson 0.72\\ncocktail shaker 0.56\\ndog 0.97dog 0.85 dog 0.57\\nbird 0.63\\ndog 0.97dog 0.95\\ndog 0.64\\nhelmet 0.65\\nhelmet 0.52\\nmotorcycle 0.65\\nperson 0.75\\nperson 0.58\\nsnowmobile 0.83\\nsnowmobile 0.83\\nbow tie 0.86\\nperson 0.82\\nbird 0.61\\ndog 0.66\\ndog 0.61\\ndomestic cat 0.57\\nbird 0.96\\ndog 0.91\\ndog 0.77\\nsofa 0.71\\ndog 0.95\\ndog 0.55\\nladybug 1.00\\nperson 0.87\\ncar 0.96 car 0.66car 0.63\\nbird 0.98\\nperson 0.65\\nwatercraft 1.00\\nwatercraft 0.69\\npretzel 0.78\\ncar 0.96\\nperson 0.65person 0.58person 0.52\\nperson 0.52\\nbird 0.99 bird 0.91\\nbird 0.75\\ndog 0.98\\nflower pot 0.62\\ndog 0.97dog 0.56\\ntrain 1.00\\ntrain 0.53\\narmadillo 1.00\\narmadillo 0.56\\nbird 0.93\\ndog 0.92\\nswine 0.88\\nbird 1.00\\nbutterfly 0.96\\nperson 0.90\\nflower pot 0.62\\nsnake 0.70\\nturtle 0.54\\nbell pepper 0.81\\nbell pepper 0.62\\nbell pepper 0.54\\nruler 1.00\\nantelope 0.53\\nmushroom 0.93\\ntv or monitor 0.82\\ntv or monitor 0.76tv or monitor 0.54\\nbird 0.89\\nlipstick 0.80\\nlipstick 0.61\\nperson 0.58\\ndog 0.97\\nsoccer ball 0.90\\nFigure 8: Example detections on the val2 set from the conﬁguration that achieved 31.0% mAP on val2. Each image was sampled randomly\\n(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the\\nprecision value of that detection from the detector’s precision-recall curve. Viewing digitally with zoom is recommended.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17'}, page_content='baby bed 0.55helmet 0.51\\npitcher 0.57\\ndog 0.98\\nhat with a wide brim 0.78\\nperson 0.86\\nbird 0.52table 0.60\\nmonkey 0.97\\ntable 0.68\\nwatercraft 0.55\\nperson 0.88\\ncar 0.61\\nperson 0.87\\nperson 0.51\\nsunglasses 0.51\\ndog 0.94dog 0.55\\nbird 0.52\\nmonkey 0.87\\nmonkey 0.81\\nswine 0.50\\ndog 0.97\\nhat with a wide brim 0.96\\nsnake 0.74\\ndog 0.93\\nperson 0.77\\ndog 0.97\\nguacamole 0.64\\npretzel 0.69\\ntable 0.54\\ndog 0.71\\nperson 0.85\\nladybug 0.90\\nperson 0.52\\nzebra 0.83 zebra 0.80\\nzebra 0.55\\nzebra 0.52\\ndog 0.98\\nhat with a wide brim 0.60person 0.85\\nperson 0.81 person 0.73\\nelephant 1.00\\nbird 0.99\\nperson 0.58\\ndog 0.98\\ncart 1.00\\nchair 0.79chair 0.64\\nperson 0.91person 0.87 person 0.57\\nperson 0.52\\ncomputer keyboard 0.52\\ndog 0.97 dog 0.92\\nperson 0.77\\nbird 0.94\\nbutterfly 0.98\\nperson 0.73\\nperson 0.61\\nbird 1.00\\nbird 0.78\\nperson 0.91 person 0.75\\nstethoscope 0.83\\nbird 0.83\\nFigure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='person 0.81\\nperson 0.57\\nperson 0.53\\nmotorcycle 0.64\\nperson 0.73\\nperson 0.51\\nbagel 0.57\\npineapple 1.00\\nbowl 0.63\\nguacamole 1.00tennis ball 0.60\\nlemon 0.88\\nlemon 0.86lemon 0.80\\nlemon 0.78\\norange 0.78\\norange 0.73\\norange 0.71\\ngolf ball 1.00\\ngolf ball 1.00\\ngolf ball 0.89\\ngolf ball 0.81\\ngolf ball 0.79\\ngolf ball 0.76golf ball 0.60\\ngolf ball 0.60\\ngolf ball 0.51\\nlemon 0.53\\nsoccer ball 0.67\\nlamp 0.61\\ntable 0.59\\nbee 0.85\\njellyfish 0.71\\nbowl 0.54\\nhamburger 0.78\\ndumbbell 1.00person 0.52\\nmicrophone 1.00\\nperson 0.85\\nhead cabbage 0.83\\nhead cabbage 0.75\\ndog 0.74\\ngoldfish 0.76\\nperson 0.57\\nguitar 1.00\\nguitar 1.00\\nguitar 0.88\\ntable 0.63\\ncomputer keyboard 0.78\\nmicrowave 0.60\\ntable 0.53\\ntick 0.64\\nlemon 0.80\\ntennis ball 0.67\\nrabbit 1.00\\ndog 0.98\\nperson 0.81\\nperson 0.92\\nsunglasses 0.52\\nwatercraft 0.86\\nmilk can 1.00\\nmilk can 1.00\\nbookshelf 0.50\\nchair 0.86\\ngiant panda 0.61\\nperson 0.87\\nantelope 0.74\\ncattle 0.81\\ndog 0.87\\nhorse 0.78\\npomegranate 1.00\\nchair 0.86\\ntv or monitor 0.52\\nantelope 0.68\\nbird 0.94\\nsnake 0.60\\ndog 0.98\\ndog 0.88\\nperson 0.79\\nsnake 0.76\\ntable 0.62\\ntv or monitor 0.80\\ntv or monitor 0.58\\ntv or monitor 0.54\\nlamp 0.86lamp 0.65\\ntable 0.83\\nmonkey 1.00monkey 1.00\\nmonkey 0.90\\nmonkey 0.88\\nmonkey 0.52\\ndog 0.88fox 1.00\\nfox 0.81\\nperson 0.88\\nwatercraft 0.91\\nwatercraft 0.56\\nbird 0.95\\nbird 0.78\\nisopod 0.56\\nbird 0.69\\nstarfish 0.67\\ndragonfly 0.70\\ndragonfly 0.60\\nhamburger 0.72\\nhamburger 0.60\\ncup or mug 0.72\\nelectric fan 1.00\\nelectric fan 0.83\\nelectric fan 0.78helmet 0.64\\nsoccer ball 0.63\\nFigure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing\\ndigitally with zoom is recommended.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='object detection. In CVPR, 2013. 6, 7\\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\\nbased face detection. TPAMI, 1998. 2\\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\\ning internal representations by error propagation. Parallel\\nDistributed Processing, 1:318–362, 1986. 1\\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\\ntion and Detection using Convolutional Networks. In ICLR,\\n2014. 1, 2, 4, 10\\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. In CVPR, 2013. 2\\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\\nfor visual object detection. In AAAI Technical Report, 4th\\nHuman Computation Workshop, 2012. 8\\n[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4\\n[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\\nfor object detection. In NIPS, 2013. 2\\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\\nSelective search for object recognition. IJCV, 2013. 1, 2, 3,\\n4, 5, 9\\n[40] R. Vaillant, C. Monrocq, and Y . LeCun. Original approach\\nfor the localisation of objects in images. IEE Proc on Vision,\\nImage, and Signal Processing, 1994. 2\\n[41] X. Wang, M. Yang, S. Zhu, and Y . Lin. Regionlets for generic\\nobject detection. In ICCV, 2013. 3, 5\\n[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-\\ntional networks for mid and high level feature learning. In\\nCVPR, 2011. 4\\n[43] K. Simonyan and A. Zisserman. Very Deep Convolu-\\ntional Networks for Large-Scale Image Recognition. arXiv\\npreprint, arXiv:1409.1556, 2014. 6, 7, 14\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='person 0.82\\nsnake 0.76\\nfrog 0.78\\nbird 0.79\\ngoldfish 0.76\\ngoldfish 0.76\\ngoldfish 0.58\\nperson 0.94\\nstethoscope 0.56\\nperson 0.95person 0.92person 0.67\\nperson 0.60\\ntable 0.81\\njellyfish 0.67\\nlemon 0.52\\nperson 0.78\\nperson 0.65\\nwatercraft 0.55\\nbaseball 1.00\\nperson 0.94\\nperson 0.82\\nperson 0.80\\nperson 0.61\\nperson 0.55\\nperson 0.52\\ncomputer keyboard 0.81\\ndog 0.60 person 0.88\\nperson 0.79\\nperson 0.68\\nperson 0.59\\ntv or monitor 0.82\\nlizard 0.58\\nchair 0.50\\nperson 0.74\\ntable 0.82\\nperson 0.94\\nperson 0.94\\nperson 0.95\\nperson 0.81person 0.69\\nrugby ball 0.91\\nperson 0.84 person 0.59\\nvolleyball 0.70\\npineapple 1.00\\nbrassiere 0.71\\nperson 0.95 person 0.94person 0.94\\nperson 0.81 person 0.80person 0.80\\nperson 0.79\\nperson 0.79\\nperson 0.69\\nperson 0.66\\nperson 0.58\\nperson 0.56person 0.54\\nswimming trunks 0.56\\nbaseball 0.86\\nhelmet 0.74\\nperson 0.75\\nminiskirt 0.64\\nperson 0.92\\nvacuum 1.00\\ndog 0.98\\ndog 0.93\\nperson 0.94 person 0.75\\nperson 0.65\\nperson 0.53\\nski 0.80 ski 0.80\\nbird 0.55\\ntiger 1.00\\ntiger 0.67\\ntiger 0.59\\nbird 0.56\\nwhale 1.00\\nchair 0.53\\nperson 0.92\\nperson 0.92\\nperson 0.82person 0.78\\nbowl 0.52\\nstrawberry 0.79strawberry 0.70\\nburrito 0.54\\ncroquet ball 0.91croquet ball 0.91croquet ball 0.91 croquet ball 0.91\\nmushroom 0.57\\nwatercraft 0.91\\nwatercraft 0.87\\nwatercraft 0.58\\nplastic bag 0.62\\nplastic bag 0.62\\nwhale 0.88\\ncar 0.70\\ndog 0.94\\ntv or monitor 0.57\\ncart 0.80\\nperson 0.79\\nperson 0.53\\nhat with a wide brim 0.89person 0.88\\nperson 0.82\\nperson 0.79\\nperson 0.56\\nperson 0.54\\ntraffic light 0.79\\nbird 0.59\\ncucumber 0.53\\ncucumber 0.52\\nantelope 1.00\\nantelope 1.00\\nantelope 0.94\\nantelope 0.73\\nantelope 0.63\\nantelope 0.63\\nfox 0.57\\nbalance beam 0.50horizontal bar 1.00\\nperson 0.80\\nperson 0.90\\nsnake 0.64\\ndog 0.98\\ndog 0.97\\nhelmet 0.69\\nhorse 0.92\\nhorse 0.69\\nperson 0.82\\nperson 0.72\\norange 0.79\\norange 0.71\\norange 0.66\\norange 0.66\\norange 0.59\\norange 0.56\\nbird 0.97\\nbird 0.96\\nbird 0.96\\nbird 0.94\\nbird 0.89\\nbird 0.64\\nbird 0.56\\nbird 0.53bird 0.52\\nguitar 1.00\\nperson 0.82\\nbicycle 0.92\\nperson 0.90\\nperson 0.83\\ncar 1.00 car 0.97\\ndog 0.98dog 0.86\\ndog 0.85\\ndog 0.65dog 0.50\\nperson 0.83\\nperson 0.80\\nperson 0.74person 0.54\\nelephant 0.60\\nFigure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,1) (top 1 − 24)\\n1.0 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,2) (top 1 − 24)\\n1.0 0.9 0.9 0.9 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,3) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,4) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,5) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,6) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,7) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,8) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,9) (top 1 − 24)\\n0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,10) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.5 0.5\\npool5 feature: (3,3,11) (top 1 − 24)\\n0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,12) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,13) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\npool5 feature: (3,3,14) (top 1 − 24)\\n0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,15) (top 1 − 24)\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,16) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,17) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,18) (top 1 − 24)\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,19) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,20) (top 1 − 24)\\n1.0 0.9 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\nFigure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly\\nactivate each of 20 units. Each montage is labeled by the unit’s (y, x, channel) position in the6 ×6 ×256 dimensional pool5 feature map.\\nEach image region is drawn with an overlay of the unit’s receptive ﬁeld in white. The activation value (which we normalize by dividing by\\nthe max activation value over all units in a channel) is shown in the receptive ﬁeld’s upper-left corner. Best viewed digitally with zoom.\\n21')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"1311.2524v5.pdf\")\n",
    "document=loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6b952d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant\\nperformance boost. Since we combine region proposals\\nwith CNNs, we call our method R-CNN: Regions with CNN\\nfeatures. We also compare R-CNN to OverFeat, a recently\\nproposed sliding-window detector based on a similar CNN\\narchitecture. We ﬁnd that R-CNN outperforms OverFeat\\nby a large margin on the 200-class ILSVRC2013 detection\\ndataset. Source code for the complete system is available at\\nhttp://www.cs.berkeley.edu/˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='a representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [39] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='On the 200-class ILSVRC2013 detection dataset, R-CNN’s\\nmAP is 31.4% , a large improvement over OverFeat [34], which\\nhad the previous best result at 24.3%.\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training\\nalgorithm. Building on Rumelhart et al. [33], LeCun et\\nal. [26] showed that stochastic gradient descent via back-\\npropagation was effective for training convolutional neural\\nnetworks (CNNs), a class of models that extend the neocog-\\nnitron.\\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\\nfell out of fashion with the rise of support vector machines.\\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\\nby showing substantially higher image classiﬁcation accu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='racy on the ImageNet Large Scale Visual Recognition Chal-\\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\\ning a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\n1\\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='debated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question by bridging the gap between\\nimage classiﬁcation and object detection. This paper is the\\nﬁrst to show that a CNN can lead to dramatically higher ob-\\nject detection performance on PASCAL VOC as compared\\nto systems based on simpler HOG-like features. To achieve\\nthis result, we focused on two problems: localizing objects\\nwith a deep network and training a high-capacity model\\nwith only a small quantity of annotated detection data.\\nUnlike image classiﬁcation, detection requires localiz-\\ning (likely many) objects within an image. One approach\\nframes localization as a regression problem. However, work\\nfrom Szegedy et al. [38], concurrent with our own, indi-\\ncates that this strategy may not fare well in practice (they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='report a mAP of 30.5% on VOC 2007 compared to the\\n58.5% achieved by our method). An alternative is to build a\\nsliding-window detector. CNNs have been used in this way\\nfor at least two decades, typically on constrained object cat-\\negories, such as faces [32, 40] and pedestrians [35]. In order\\nto maintain high spatial resolution, these CNNs typically\\nonly have two convolutional and pooling layers. We also\\nconsidered adopting a sliding-window approach. However,\\nunits high up in our network, which has ﬁve convolutional\\nlayers, have very large receptive ﬁelds ( 195 ×195 pixels)\\nand strides (32×32 pixels) in the input image, which makes\\nprecise localization within the sliding-window paradigm an\\nopen technical challenge.\\nInstead, we solve the CNN localization problem by oper-\\nating within the “recognition using regions” paradigm [21],\\nwhich has been successful for both object detection [39] and\\nsemantic segmentation [5]. At test time, our method gener-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='ates around 2000 category-independent region proposals for\\nthe input image, extracts a ﬁxed-length feature vector from\\neach proposal using a CNN, and then classiﬁes each region\\nwith category-speciﬁc linear SVMs. We use a simple tech-\\nnique (afﬁne image warping) to compute a ﬁxed-size CNN\\ninput from each region proposal, regardless of the region’s\\nshape. Figure 1 presents an overview of our method and\\nhighlights some of our results. Since our system combines\\nregion proposals with CNNs, we dub the method R-CNN:\\nRegions with CNN features.\\nIn this updated version of this paper, we provide a head-\\nto-head comparison of R-CNN and the recently proposed\\nOverFeat [34] detection system by running R-CNN on the\\n200-class ILSVRC2013 detection dataset. OverFeat uses a\\nsliding-window CNN for detection and until now was the\\nbest performing method on ILSVRC2013 detection. We\\nshow that R-CNN signiﬁcantly outperforms OverFeat, with\\na mAP of 31.4% versus 24.3%.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='A second challenge faced in detection is that labeled data\\nis scarce and the amount currently available is insufﬁcient\\nfor training a large CNN. The conventional solution to this\\nproblem is to useunsupervised pre-training, followed by su-\\npervised ﬁne-tuning (e.g., [35]). The second principle con-\\ntribution of this paper is to show thatsupervised pre-training\\non a large auxiliary dataset (ILSVRC), followed by domain-\\nspeciﬁc ﬁne-tuning on a small dataset (PASCAL), is an\\neffective paradigm for learning high-capacity CNNs when\\ndata is scarce. In our experiments, ﬁne-tuning for detection\\nimproves mAP performance by 8 percentage points. After\\nﬁne-tuning, our system achieves a mAP of 54% on VOC\\n2010 compared to 33% for the highly-tuned, HOG-based\\ndeformable part model (DPM) [17, 20]. We also point read-\\ners to contemporaneous work by Donahue et al. [12], who\\nshow that Krizhevsky’s CNN can be used (without ﬁne-\\ntuning) as a blackbox feature extractor, yielding excellent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='performance on several recognition tasks including scene\\nclassiﬁcation, ﬁne-grained sub-categorization, and domain\\nadaptation.\\nOur system is also quite efﬁcient. The only class-speciﬁc\\ncomputations are a reasonably small matrix-vector product\\nand greedy non-maximum suppression. This computational\\nproperty follows from features that are shared across all cat-\\negories and that are also two orders of magnitude lower-\\ndimensional than previously used region features (cf. [39]).\\nUnderstanding the failure modes of our approach is also\\ncritical for improving it, and so we report results from the\\ndetection analysis tool of Hoiem et al. [23]. As an im-\\nmediate consequence of this analysis, we demonstrate that\\na simple bounding-box regression method signiﬁcantly re-\\nduces mislocalizations, which are the dominant error mode.\\nBefore developing technical details, we note that because\\nR-CNN operates on regions it is natural to extend it to the\\ntask of semantic segmentation. With minor modiﬁcations,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='we also achieve competitive results on the PASCAL VOC\\nsegmentation task, with an average segmentation accuracy\\nof 47.9% on the VOC 2011 test set.\\n2. Object detection with R-CNN\\nOur object detection system consists of three modules.\\nThe ﬁrst generates category-independent region proposals.\\nThese proposals deﬁne the set of candidate detections avail-\\nable to our detector. The second module is a large convo-\\nlutional neural network that extracts a ﬁxed-length feature\\nvector from each region. The third module is a set of class-\\nspeciﬁc linear SVMs. In this section, we present our design\\ndecisions for each module, describe their test-time usage,\\ndetail how their parameters are learned, and show detection\\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\\n2.1. Module design\\nRegion proposals. A variety of recent papers offer meth-\\nods for generating category-independent region proposals.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='aeroplane bicycle bird car\\nFigure 2: Warped training samples from VOC 2007 train.\\nExamples include: objectness [1], selective search [39],\\ncategory-independent object proposals [14], constrained\\nparametric min-cuts (CPMC) [5], multi-scale combinatorial\\ngrouping [3], and Cires ¸an et al. [6], who detect mitotic cells\\nby applying a CNN to regularly-spaced square crops, which\\nare a special case of region proposals. While R-CNN is ag-\\nnostic to the particular region proposal method, we use se-\\nlective search to enable a controlled comparison with prior\\ndetection work (e.g., [39, 41]).\\nFeature extraction. We extract a 4096-dimensional fea-\\nture vector from each region proposal using the Caffe [24]\\nimplementation of the CNN described by Krizhevsky et\\nal. [25]. Features are computed by forward propagating\\na mean-subtracted 227 ×227 RGB image through ﬁve con-\\nvolutional layers and two fully connected layers. We refer\\nreaders to [24, 25] for more network architecture details.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='In order to compute features for a region proposal, we\\nmust ﬁrst convert the image data in that region into a form\\nthat is compatible with the CNN (its architecture requires\\ninputs of a ﬁxed 227 ×227 pixel size). Of the many possi-\\nble transformations of our arbitrary-shaped regions, we opt\\nfor the simplest. Regardless of the size or aspect ratio of the\\ncandidate region, we warp all pixels in a tight bounding box\\naround it to the required size. Prior to warping, we dilate the\\ntight bounding box so that at the warped size there are ex-\\nactly ppixels of warped image context around the original\\nbox (we use p = 16). Figure 2 shows a random sampling\\nof warped training regions. Alternatives to warping are dis-\\ncussed in Appendix A.\\n2.2. Test-time detection\\nAt test time, we run selective search on the test image\\nto extract around 2000 region proposals (we use selective\\nsearch’s “fast mode” in all experiments). We warp each\\nproposal and forward propagate it through the CNN in or-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='der to compute features. Then, for each class, we score\\neach extracted feature vector using the SVM trained for that\\nclass. Given all scored regions in an image, we apply a\\ngreedy non-maximum suppression (for each class indepen-\\ndently) that rejects a region if it has an intersection-over-\\nunion (IoU) overlap with a higher scoring selected region\\nlarger than a learned threshold.\\nRun-time analysis. Two properties make detection efﬁ-\\ncient. First, all CNN parameters are shared across all cate-\\ngories. Second, the feature vectors computed by the CNN\\nare low-dimensional when compared to other common ap-\\nproaches, such as spatial pyramids with bag-of-visual-word\\nencodings. The features used in the UV A detection system\\n[39], for example, are two orders of magnitude larger than\\nours (360k vs. 4k-dimensional).\\nThe result of such sharing is that the time spent com-\\nputing region proposals and features (13s/image on a GPU\\nor 53s/image on a CPU) is amortized over all classes. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='only class-speciﬁc computations are dot products between\\nfeatures and SVM weights and non-maximum suppression.\\nIn practice, all dot products for an image are batched into\\na single matrix-matrix product. The feature matrix is typi-\\ncally 2000×4096 and the SVM weight matrix is4096×N,\\nwhere N is the number of classes.\\nThis analysis shows that R-CNN can scale to thousands\\nof object classes without resorting to approximate tech-\\nniques, such as hashing. Even if there were 100k classes,\\nthe resulting matrix multiplication takes only 10 seconds on\\na modern multi-core CPU. This efﬁciency is not merely the\\nresult of using region proposals and shared features. The\\nUV A system, due to its high-dimensional features, would\\nbe two orders of magnitude slower while requiring 134GB\\nof memory just to store 100k linear predictors, compared to\\njust 1.5GB for our lower-dimensional features.\\nIt is also interesting to contrast R-CNN with the recent\\nwork from Dean et al. on scalable detection using DPMs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='and hashing [8]. They report a mAP of around 16% on VOC\\n2007 at a run-time of 5 minutes per image when introducing\\n10k distractor classes. With our approach, 10k detectors can\\nrun in about a minute on a CPU, and because no approxi-\\nmations are made mAP would remain at 59% (Section 3.2).\\n2.3. Training\\nSupervised pre-training. We discriminatively pre-trained\\nthe CNN on a large auxiliary dataset (ILSVRC2012 clas-\\nsiﬁcation) using image-level annotations only (bounding-\\nbox labels are not available for this data). Pre-training\\nwas performed using the open source Caffe CNN library\\n[24]. In brief, our CNN nearly matches the performance\\nof Krizhevsky et al. [25], obtaining a top-1 error rate 2.2\\npercentage points higher on the ILSVRC2012 classiﬁcation\\nvalidation set. This discrepancy is due to simpliﬁcations in\\nthe training process.\\nDomain-speciﬁc ﬁne-tuning. To adapt our CNN to the\\nnew task (detection) and the new domain (warped proposal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='windows), we continue stochastic gradient descent (SGD)\\ntraining of the CNN parameters using only warped region\\nproposals. Aside from replacing the CNN’s ImageNet-\\nspeciﬁc 1000-way classiﬁcation layer with a randomly ini-\\ntialized (N + 1)-way classiﬁcation layer (where N is the\\nnumber of object classes, plus 1 for background), the CNN\\narchitecture is unchanged. For VOC, N = 20 and for\\nILSVRC2013, N = 200. We treat all region proposals with\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='≥0.5 IoU overlap with a ground-truth box as positives for\\nthat box’s class and the rest as negatives. We start SGD at\\na learning rate of 0.001 (1/10th of the initial pre-training\\nrate), which allows ﬁne-tuning to make progress while not\\nclobbering the initialization. In each SGD iteration, we uni-\\nformly sample 32 positive windows (over all classes) and\\n96 background windows to construct a mini-batch of size\\n128. We bias the sampling towards positive windows be-\\ncause they are extremely rare compared to background.\\nObject category classiﬁers. Consider training a binary\\nclassiﬁer to detect cars. It’s clear that an image region\\ntightly enclosing a car should be a positive example. Simi-\\nlarly, it’s clear that a background region, which has nothing\\nto do with cars, should be a negative example. Less clear\\nis how to label a region that partially overlaps a car. We re-\\nsolve this issue with an IoU overlap threshold, below which\\nregions are deﬁned as negatives. The overlap threshold,0.3,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='was selected by a grid search over {0,0.1,..., 0.5}on a\\nvalidation set. We found that selecting this threshold care-\\nfully is important. Setting it to 0.5, as in [39], decreased\\nmAP by 5 points. Similarly, setting it to 0 decreased mAP\\nby 4 points. Positive examples are deﬁned simply to be the\\nground-truth bounding boxes for each class.\\nOnce features are extracted and training labels are ap-\\nplied, we optimize one linear SVM per class. Since the\\ntraining data is too large to ﬁt in memory, we adopt the\\nstandard hard negative mining method [17, 37]. Hard neg-\\native mining converges quickly and in practice mAP stops\\nincreasing after only a single pass over all images.\\nIn Appendix B we discuss why the positive and negative\\nexamples are deﬁned differently in ﬁne-tuning versus SVM\\ntraining. We also discuss the trade-offs involved in training\\ndetection SVMs rather than simply using the outputs from\\nthe ﬁnal softmax layer of the ﬁne-tuned CNN.\\n2.4. Results on PASCAL VOC 2010-12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='Following the PASCAL VOC best practices [15], we\\nvalidated all design decisions and hyperparameters on the\\nVOC 2007 dataset (Section 3.2). For ﬁnal results on the\\nVOC 2010-12 datasets, we ﬁne-tuned the CNN on VOC\\n2012 train and optimized our detection SVMs on VOC 2012\\ntrainval. We submitted test results to the evaluation server\\nonly once for each of the two major algorithm variants (with\\nand without bounding-box regression).\\nTable 1 shows complete results on VOC 2010. We com-\\npare our method against four strong baselines, including\\nSegDPM [18], which combines DPM detectors with the\\noutput of a semantic segmentation system [4] and uses ad-\\nditional inter-detector context and image-classiﬁer rescor-\\ning. The most germane comparison is to the UV A system\\nfrom Uijlings et al. [39], since our systems use the same re-\\ngion proposal algorithm. To classify regions, their method\\nbuilds a four-level spatial pyramid and populates it with\\ndensely sampled SIFT, Extended OpponentSIFT, and RGB-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='SIFT descriptors, each vector quantized with 4000-word\\ncodebooks. Classiﬁcation is performed with a histogram\\nintersection kernel SVM. Compared to their multi-feature,\\nnon-linear kernel SVM approach, we achieve a large im-\\nprovement in mAP, from 35.1% to 53.7% mAP, while also\\nbeing much faster (Section 2.2). Our method achieves sim-\\nilar performance (53.3% mAP) on VOC 2011/12 test.\\n2.5. Results on ILSVRC2013 detection\\nWe ran R-CNN on the 200-class ILSVRC2013 detection\\ndataset using the same system hyperparameters that we used\\nfor PASCAL VOC. We followed the same protocol of sub-\\nmitting test results to the ILSVRC2013 evaluation server\\nonly twice, once with and once without bounding-box re-\\ngression.\\nFigure 3 compares R-CNN to the entries in the ILSVRC\\n2013 competition and to the post-competition OverFeat re-\\nsult [34]. R-CNN achieves a mAP of 31.4%, which is sig-\\nniﬁcantly ahead of the second-best result of 24.3% from\\nOverFeat. To give a sense of the AP distribution over'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='classes, box plots are also presented and a table of per-\\nclass APs follows at the end of the paper in Table 8. Most\\nof the competing submissions (OverFeat, NEC-MU, UvA-\\nEuvision, Toronto A, and UIUC-IFP) used convolutional\\nneural networks, indicating that there is signiﬁcant nuance\\nin how CNNs can be applied to object detection, leading to\\ngreatly varying outcomes.\\nIn Section 4, we give an overview of the ILSVRC2013\\ndetection dataset and provide details about choices that we\\nmade when running R-CNN on it.\\n3. Visualization, ablation, and modes of error\\n3.1. Visualizing learned features\\nFirst-layer ﬁlters can be visualized directly and are easy\\nto understand [25]. They capture oriented edges and oppo-\\nnent colors. Understanding the subsequent layers is more\\nchallenging. Zeiler and Fergus present a visually attrac-\\ntive deconvolutional approach in [42]. We propose a simple\\n(and complementary) non-parametric method that directly\\nshows what the network learned.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='The idea is to single out a particular unit (feature) in the\\nnetwork and use it as if it were an object detector in its own\\nright. That is, we compute the unit’s activations on a large\\nset of held-out region proposals (about 10 million), sort the\\nproposals from highest to lowest activation, perform non-\\nmaximum suppression, and then display the top-scoring re-\\ngions. Our method lets the selected unit “speak for itself”\\nby showing exactly which inputs it ﬁres on. We avoid aver-\\naging in order to see different visual modes and gain insight\\ninto the invariances computed by the unit.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='VOC 2010 testaero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nDPM v5 [20]† 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4\\nUV A [39] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1\\nRegionlets [41]65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7\\nSegDPM [18]† 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4\\nR-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2\\nR-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7\\nTable 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UV A and Regionlets since all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='methods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM\\nwas the top-performer on the PASCAL VOC leaderboard. †DPM and SegDPM use context rescoring not used by the other methods.\\n0 20 40 60 80 100\\nUIUC−IFP \\nDelta \\nGPU_UCLA \\nSYSU_Vision \\nToronto A \\n*OverFeat (1) \\n*NEC−MU \\nUvA−Euvision \\n*OverFeat (2) \\n*R−CNN BB \\nmean average precision (mAP) in %\\nILSVRC2013 detection test set mAP\\n \\n \\n1.0%\\n6.1%\\n9.8%\\n10.5%\\n11.5%\\n19.4%\\n20.9%\\n22.6%\\n24.3%\\n31.4%\\ncompetition result\\npost competition result\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n*R−CNN BB\\nUvA−Euvision\\n*NEC−MU\\n*OverFeat (1)\\nToronto A\\nSYSU_Vision\\nGPU_UCLA\\nDelta\\nUIUC−IFP\\naverage precision (AP) in %\\nILSVRC2013 detection test set class AP box plots\\nFigure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='(images and labels from the ILSVRC classiﬁcation dataset in all cases). (Right) Box plots for the 200 average precision values per\\nmethod. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for\\nR-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; seeR-CNN-ILSVRC2013-APs.txt). The red\\nline marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each\\nmethod. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).\\n1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9\\n1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\nFigure 4: Top regions for six pool5 units. Receptive ﬁelds and activation values are drawn in white. Some units are aligned to concepts,\\nsuch as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reﬂections (6).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='VOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN pool5 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2\\nR-CNN fc6 59.3 61.8 43.1 34.0 25.1 53.1 60.6 52.8 21.7 47.8 42.7 47.8 52.5 58.5 44.6 25.6 48.3 34.0 53.1 58.0 46.2\\nR-CNN fc7 57.6 57.9 38.5 31.8 23.7 51.2 58.9 51.4 20.0 50.5 40.9 46.0 51.6 55.9 43.3 23.3 48.1 35.3 51.0 57.4 44.7\\nR-CNN FT pool5 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3\\nR-CNN FT fc6 63.5 66.0 47.9 37.7 29.9 62.5 70.2 60.2 32.0 57.9 47.0 53.5 60.1 64.2 52.2 31.3 55.0 50.0 57.7 63.0 53.1\\nR-CNN FT fc7 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='DPM v5 [20] 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7\\nDPM ST [28] 23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1\\nDPM HSC [31] 32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3\\nTable 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without ﬁne-tuning. Rows 4-6 show\\nresults for the CNN pre-trained on ILSVRC 2012 and then ﬁne-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box\\nregression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The ﬁrst uses\\nonly HOG, while the next two use different feature learning approaches to augment or replace HOG.\\nVOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='R-CNN T-Net 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN T-Net BB68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nR-CNN O-Net 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2\\nR-CNN O-Net BB73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nTable 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The ﬁrst two rows are results from\\nTable 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\\nand Zisserman (O-Net) [43].\\nWe visualize units from layer pool 5, which is the max-\\npooled output of the network’s ﬁfth and ﬁnal convolutional\\nlayer. The pool 5 feature map is 6 ×6 ×256 = 9216-\\ndimensional. Ignoring boundary effects, each pool5 unit has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='a receptive ﬁeld of195×195 pixels in the original227×227\\npixel input. A central pool 5 unit has a nearly global view,\\nwhile one near the edge has a smaller, clipped support.\\nEach row in Figure 4 displays the top 16 activations for\\na pool5 unit from a CNN that we ﬁne-tuned on VOC 2007\\ntrainval. Six of the 256 functionally unique units are visu-\\nalized (Appendix D includes more). These units were se-\\nlected to show a representative sample of what the network\\nlearns. In the second row, we see a unit that ﬁres on dog\\nfaces and dot arrays. The unit corresponding to the third row\\nis a red blob detector. There are also detectors for human\\nfaces and more abstract patterns such as text and triangular\\nstructures with windows. The network appears to learn a\\nrepresentation that combines a small number of class-tuned\\nfeatures together with a distributed representation of shape,\\ntexture, color, and material properties. The subsequent fully'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='connected layer fc 6 has the ability to model a large set of\\ncompositions of these rich features.\\n3.2. Ablation studies\\nPerformance layer-by-layer, without ﬁne-tuning. To un-\\nderstand which layers are critical for detection performance,\\nwe analyzed results on the VOC 2007 dataset for each of the\\nCNN’s last three layers. Layer pool5 was brieﬂy described\\nin Section 3.1. The ﬁnal two layers are summarized below.\\nLayer fc6 is fully connected to pool 5. To compute fea-\\ntures, it multiplies a4096×9216 weight matrix by the pool5\\nfeature map (reshaped as a 9216-dimensional vector) and\\nthen adds a vector of biases. This intermediate vector is\\ncomponent-wise half-wave rectiﬁed (x←max(0,x)).\\nLayer fc7 is the ﬁnal layer of the network. It is imple-\\nmented by multiplying the features computed by fc 6 by a\\n4096 ×4096 weight matrix, and similarly adding a vector\\nof biases and applying half-wave rectiﬁcation.\\nWe start by looking at results from the CNN without'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='ﬁne-tuning on PASCAL, i.e. all CNN parameters were\\npre-trained on ILSVRC 2012 only. Analyzing performance\\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\\nfc7 generalize worse than features from fc 6. This means\\nthat 29%, or about 16.8 million, of the CNN’s parameters\\ncan be removed without degrading mAP. More surprising is\\nthat removing both fc7 and fc6 produces quite good results\\neven though pool5 features are computed using only 6% of\\nthe CNN’s parameters. Much of the CNN’s representational\\npower comes from its convolutional layers, rather than from\\nthe much larger densely connected layers. This ﬁnding sug-\\ngests potential utility in computing a dense feature map, in\\nthe sense of HOG, of an arbitrary-sized image by using only\\nthe convolutional layers of the CNN. This representation\\nwould enable experimentation with sliding-window detec-\\ntors, including DPM, on top of pool5 features.\\nPerformance layer-by-layer, with ﬁne-tuning. We now'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='look at results from our CNN after having ﬁne-tuned its pa-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='rameters on VOC 2007 trainval. The improvement is strik-\\ning (Table 2 rows 4-6): ﬁne-tuning increases mAP by 8.0\\npercentage points to 54.2%. The boost from ﬁne-tuning is\\nmuch larger for fc6 and fc7 than for pool5, which suggests\\nthat the pool 5 features learned from ImageNet are general\\nand that most of the improvement is gained from learning\\ndomain-speciﬁc non-linear classiﬁers on top of them.\\nComparison to recent feature learning methods. Rela-\\ntively few feature learning methods have been tried on PAS-\\nCAL VOC detection. We look at two recent approaches that\\nbuild on deformable part models. For reference, we also in-\\nclude results for the standard HOG-based DPM [20].\\nThe ﬁrst DPM feature learning method, DPM ST [28],\\naugments HOG features with histograms of “sketch token”\\nprobabilities. Intuitively, a sketch token is a tight distri-\\nbution of contours passing through the center of an image\\npatch. Sketch token probabilities are computed at each pixel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='by a random forest that was trained to classify35×35 pixel\\npatches into one of 150 sketch tokens or background.\\nThe second method, DPM HSC [31], replaces HOG with\\nhistograms of sparse codes (HSC). To compute an HSC,\\nsparse code activations are solved for at each pixel using\\na learned dictionary of 100 7 ×7 pixel (grayscale) atoms.\\nThe resulting activations are rectiﬁed in three ways (full and\\nboth half-waves), spatially pooled, unit ℓ2 normalized, and\\nthen power transformed (x←sign(x)|x|α).\\nAll R-CNN variants strongly outperform the three DPM\\nbaselines (Table 2 rows 8-10), including the two that use\\nfeature learning. Compared to the latest version of DPM,\\nwhich uses only HOG features, our mAP is more than 20\\npercentage points higher: 54.2% vs. 33.7%— a 61% rela-\\ntive improvement. The combination of HOG and sketch to-\\nkens yields 2.5 mAP points over HOG alone, while HSC\\nimproves over HOG by 4 mAP points (when compared\\ninternally to their private DPM baselines—both use non-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='public implementations of DPM that underperform the open\\nsource version [20]). These methods achieve mAPs of\\n29.1% and 34.3%, respectively.\\n3.3. Network architectures\\nMost results in this paper use the network architecture\\nfrom Krizhevsky et al. [25]. However, we have found that\\nthe choice of architecture has a large effect on R-CNN de-\\ntection performance. In Table 3 we show results on VOC\\n2007 test using the 16-layer deep network recently proposed\\nby Simonyan and Zisserman [43]. This network was one of\\nthe top performers in the recent ILSVRC 2014 classiﬁca-\\ntion challenge. The network has a homogeneous structure\\nconsisting of 13 layers of 3 ×3 convolution kernels, with\\nﬁve max pooling layers interspersed, and topped with three\\nfully-connected layers. We refer to this network as “O-Net”\\nfor OxfordNet and the baseline as “T-Net” for TorontoNet.\\nTo use O-Net in R-CNN, we downloaded the pub-\\nlicly available pre-trained network weights for the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='VGG ILSVRC 16 layers model from the Caffe Model\\nZoo.1 We then ﬁne-tuned the network using the same pro-\\ntocol as we used for T-Net. The only difference was to use\\nsmaller minibatches (24 examples) as required in order to\\nﬁt within GPU memory. The results in Table 3 show that R-\\nCNN with O-Net substantially outperforms R-CNN with T-\\nNet, increasing mAP from 58.5% to 66.0%. However there\\nis a considerable drawback in terms of compute time, with\\nthe forward pass of O-Net taking roughly 7 times longer\\nthan T-Net.\\n3.4. Detection error analysis\\nWe applied the excellent detection analysis tool from\\nHoiem et al. [23] in order to reveal our method’s error\\nmodes, understand how ﬁne-tuning changes them, and to\\nsee how our error types compare with DPM. A full sum-\\nmary of the analysis tool is beyond the scope of this pa-\\nper and we encourage readers to consult [23] to understand\\nsome ﬁner details (such as “normalized AP”). Since the\\nanalysis is best absorbed in the context of the associated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='plots, we present the discussion within the captions of Fig-\\nure 5 and Figure 6.\\n3.5. Bounding-box regression\\nBased on the error analysis, we implemented a sim-\\nple method to reduce localization errors. Inspired by the\\nbounding-box regression employed in DPM [17], we train a\\nlinear regression model to predict a new detection window\\ngiven the pool 5 features for a selective search region pro-\\nposal. Full details are given in Appendix C. Results in Ta-\\nble 1, Table 2, and Figure 5 show that this simple approach\\nﬁxes a large number of mislocalized detections, boosting\\nmAP by 3 to 4 points.\\n3.6. Qualitative results\\nQualitative detection results on ILSVRC2013 are pre-\\nsented in Figure 8 and Figure 9 at the end of the paper. Each\\nimage was sampled randomly from the val 2 set and all de-\\ntections from all detectors with a precision greater than 0.5\\nare shown. Note that these are not curated and give a re-\\nalistic impression of the detectors in action. More qualita-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='tive results are presented in Figure 10 and Figure 11, but\\nthese have been curated. We selected each image because it\\ncontained interesting, surprising, or amusing results. Here,\\nalso, all detections at precision greater than 0.5 are shown.\\n4. The ILSVRC2013 detection dataset\\nIn Section 2 we presented results on the ILSVRC2013\\ndetection dataset. This dataset is less homogeneous than\\n1https://github.com/BVLC/caffe/wiki/Model-Zoo\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='occ trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.212\\n0.612\\n0.420\\n0.557\\n0.201\\n0.720\\n0.344\\n0.606\\n0.351\\n0.677\\n0.244\\n0.609\\n0.516\\nnormalized AP\\nR−CNN fc6: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.179\\n0.701\\n0.498\\n0.634\\n0.335\\n0.766\\n0.442\\n0.672\\n0.429\\n0.723\\n0.325\\n0.685\\n0.593\\nnormalized AP\\nR−CNN FT fc7: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.211\\n0.731\\n0.542\\n0.676\\n0.385\\n0.786\\n0.484\\n0.709\\n0.453\\n0.779\\n0.368\\n0.720\\n0.633\\nnormalized AP\\nR−CNN FT fc7 BB: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.132\\n0.339\\n0.216\\n0.347\\n0.056\\n0.487\\n0.126\\n0.453\\n0.137\\n0.391\\n0.094\\n0.388\\n0.297\\nnormalized AP\\nDPM voc−release5: sensitivity and impact\\nFigure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and\\nlowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='visibility). We show plots for our method (R-CNN) with and without ﬁne-tuning (FT) and bounding-box regression (BB) as well as for\\nDPM voc-release5. Overall, ﬁne-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve\\nboth the highest and lowest performing subsets for nearly all characteristics. This indicates that ﬁne-tuning does more than simply improve\\nthe lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.\\nInstead, ﬁne-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: animals'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\nFigure 5: Distribution of top-ranked false positive (FP) types.\\nEach plot shows the evolving distribution of FP types as more FPs\\nare considered in order of decreasing score. Each FP is catego-\\nrized into 1 of 4 types: Loc—poor localization (a detection with\\nan IoU overlap with the correct class between 0.1 and 0.5, or a du-\\nplicate); Sim—confusion with a similar category; Oth—confusion\\nwith a dissimilar object category; BG—a FP that ﬁred on back-\\nground. Compared with DPM (see [23]), signiﬁcantly more of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='our errors result from poor localization, rather than confusion with\\nbackground or other object classes, indicating that the CNN fea-\\ntures are much more discriminative than HOG. Loose localiza-\\ntion likely results from our use of bottom-up region proposals and\\nthe positional invariance learned from pre-training the CNN for\\nwhole-image classiﬁcation. Column three shows how our simple\\nbounding-box regression method ﬁxes many localization errors.\\nPASCAL VOC, requiring choices about how to use it. Since\\nthese decisions are non-trivial, we cover them in this sec-\\ntion.\\n4.1. Dataset overview\\nThe ILSVRC2013 detection dataset is split into three\\nsets: train (395,918), val (20,121), and test (40,152), where\\nthe number of images in each set is in parentheses. The\\nval and test splits are drawn from the same image distribu-\\ntion. These images are scene-like and similar in complexity\\n(number of objects, amount of clutter, pose variability, etc.)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='to PASCAL VOC images. The val and test splits are exhaus-\\ntively annotated, meaning that in each image all instances\\nfrom all 200 classes are labeled with bounding boxes. The\\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\\nsiﬁcation image distribution. These images have more vari-\\nable complexity with a skew towards images of a single cen-\\ntered object. Unlike val and test, the train images (due to\\ntheir large number) are not exhaustively annotated. In any\\ngiven train image, instances from the 200 classes may or\\nmay not be labeled. In addition to these image sets, each\\nclass has an extra set of negative images. Negative images\\nare manually checked to validate that they do not contain\\nany instances of their associated class. The negative im-\\nage sets were not used in this work. More information on\\nhow ILSVRC was collected and annotated can be found in\\n[11, 36].\\nThe nature of these splits presents a number of choices\\nfor training R-CNN. The train images cannot be used for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='hard negative mining, because annotations are not exhaus-\\ntive. Where should negative examples come from? Also,\\nthe train images have different statistics than val and test.\\nShould the train images be used at all, and if so, to what\\nextent? While we have not thoroughly evaluated a large\\nnumber of choices, we present what seemed like the most\\nobvious path based on previous experience.\\nOur general strategy is to rely heavily on the val set and\\nuse some of the train images as an auxiliary source of pos-\\nitive examples. To use val for both training and valida-\\ntion, we split it into roughly equally sized “val1” and “val2”\\nsets. Since some classes have very few examples in val (the\\nsmallest has only 31 and half have fewer than 110), it is\\nimportant to produce an approximately class-balanced par-\\ntition. To do this, a large number of candidate splits were\\ngenerated and the one with the smallest maximum relative\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='class imbalance was selected. 2 Each candidate split was\\ngenerated by clustering val images using their class counts\\nas features, followed by a randomized local search that may\\nimprove the split balance. The particular split used here has\\na maximum relative imbalance of about 11% and a median\\nrelative imbalance of 4%. The val1/val2 split and code used\\nto produce them will be publicly available to allow other re-\\nsearchers to compare their methods on the val splits used in\\nthis report.\\n4.2. Region proposals\\nWe followed the same region proposal approach that was\\nused for detection on PASCAL. Selective search [39] was\\nrun in “fast mode” on each image in val1, val2, and test (but\\nnot on images in train). One minor modiﬁcation was re-\\nquired to deal with the fact that selective search is not scale\\ninvariant and so the number of regions produced depends\\non the image resolution. ILSVRC image sizes range from\\nvery small to a few that are several mega-pixels, and so we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='resized each image to a ﬁxed width (500 pixels) before run-\\nning selective search. On val, selective search resulted in an\\naverage of 2403 region proposals per image with a 91.6%\\nrecall of all ground-truth bounding boxes (at 0.5 IoU thresh-\\nold). This recall is notably lower than in PASCAL, where\\nit is approximately 98%, indicating signiﬁcant room for im-\\nprovement in the region proposal stage.\\n4.3. Training data\\nFor training data, we formed a set of images and boxes\\nthat includes all selective search and ground-truth boxes\\nfrom val 1 together with up to N ground-truth boxes per\\nclass from train (if a class has fewer than N ground-truth\\nboxes in train, then we take all of them). We’ll call this\\ndataset of images and boxes val 1+trainN. In an ablation\\nstudy, we show mAP on val2 for N ∈{0,500,1000}(Sec-\\ntion 4.5).\\nTraining data is required for three procedures in R-CNN:\\n(1) CNN ﬁne-tuning, (2) detector SVM training, and (3)\\nbounding-box regressor training. CNN ﬁne-tuning was run'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='for 50k SGD iteration on val1+trainN using the exact same\\nsettings as were used for PASCAL. Fine-tuning on a sin-\\ngle NVIDIA Tesla K20 took 13 hours using Caffe. For\\nSVM training, all ground-truth boxes from val 1+trainN\\nwere used as positive examples for their respective classes.\\nHard negative mining was performed on a randomly se-\\nlected subset of 5000 images from val 1. An initial experi-\\nment indicated that mining negatives from all of val1, versus\\na 5000 image subset (roughly half of it), resulted in only a\\n0.5 percentage point drop in mAP, while cutting SVM train-\\ning time in half. No negative examples were taken from\\n2Relative imbalance is measured as |a −b|/(a + b) where a and b are\\nclass counts in each half of the split.\\ntrain because the annotations are not exhaustive. The ex-\\ntra sets of veriﬁed negative images were not used. The\\nbounding-box regressors were trained on val1.\\n4.4. Validation and evaluation\\nBefore submitting results to the evaluation server, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='validated data usage choices and the effect of ﬁne-tuning\\nand bounding-box regression on the val2 set using the train-\\ning data described above. All system hyperparameters (e.g.,\\nSVM C hyperparameters, padding used in region warp-\\ning, NMS thresholds, bounding-box regression hyperpa-\\nrameters) were ﬁxed at the same values used for PAS-\\nCAL. Undoubtedly some of these hyperparameter choices\\nare slightly suboptimal for ILSVRC, however the goal of\\nthis work was to produce a preliminary R-CNN result on\\nILSVRC without extensive dataset tuning. After selecting\\nthe best choices on val 2, we submitted exactly two result\\nﬁles to the ILSVRC2013 evaluation server. The ﬁrst sub-\\nmission was without bounding-box regression and the sec-\\nond submission was with bounding-box regression. For\\nthese submissions, we expanded the SVM and bounding-\\nbox regressor training sets to use val +train1k and val, re-\\nspectively. We used the CNN that was ﬁne-tuned on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='val1+train1k to avoid re-running ﬁne-tuning and feature\\ncomputation.\\n4.5. Ablation study\\nTable 4 shows an ablation study of the effects of differ-\\nent amounts of training data, ﬁne-tuning, and bounding-\\nbox regression. A ﬁrst observation is that mAP on val 2\\nmatches mAP on test very closely. This gives us conﬁ-\\ndence that mAP on val 2 is a good indicator of test set per-\\nformance. The ﬁrst result, 20.9%, is what R-CNN achieves\\nusing a CNN pre-trained on the ILSVRC2012 classiﬁca-\\ntion dataset (no ﬁne-tuning) and given access to the small\\namount of training data in val1 (recall that half of the classes\\nin val 1 have between 15 and 55 examples). Expanding\\nthe training set to val 1+trainN improves performance to\\n24.1%, with essentially no difference between N = 500\\nand N = 1000. Fine-tuning the CNN using examples from\\njust val1 gives a modest improvement to 26.5%, however\\nthere is likely signiﬁcant overﬁtting due to the small number\\nof positive training examples. Expanding the ﬁne-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='set to val1+train1k, which adds up to 1000 positive exam-\\nples per class from the train set, helps signiﬁcantly, boosting\\nmAP to 29.7%. Bounding-box regression improves results\\nto 31.0%, which is a smaller relative gain that what was ob-\\nserved in PASCAL.\\n4.6. Relationship to OverFeat\\nThere is an interesting relationship between R-CNN and\\nOverFeat: OverFeat can be seen (roughly) as a special case\\nof R-CNN. If one were to replace selective search region\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='test set val2 val2 val2 val2 val2 val2 test test\\nSVM training set val1 val1+train.5k val1+train1k val1+train1k val1+train1k val1+train1k val+train1k val+train1k\\nCNN ﬁne-tuning set n/a n/a n/a val 1 val1+train1k val1+train1k val1+train1k val1+train1k\\nbbox reg set n/a n/a n/a n/a n/a val 1 n/a val\\nCNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7\\nmAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4\\nmedian AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3\\nTable 4: ILSVRC2013 ablation study of data usage choices, ﬁne-tuning, and bounding-box regression.\\nproposals with a multi-scale pyramid of regular square re-\\ngions and change the per-class bounding-box regressors to\\na single bounding-box regressor, then the systems would\\nbe very similar (modulo some potentially signiﬁcant differ-\\nences in how they are trained: CNN detection ﬁne-tuning,\\nusing SVMs, etc.). It is worth noting that OverFeat has\\na signiﬁcant speed advantage over R-CNN: it is about 9x'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='faster, based on a ﬁgure of 2 seconds per image quoted from\\n[34]. This speed comes from the fact that OverFeat’s slid-\\ning windows (i.e., region proposals) are not warped at the\\nimage level and therefore computation can be easily shared\\nbetween overlapping windows. Sharing is implemented by\\nrunning the entire network in a convolutional fashion over\\narbitrary-sized inputs. Speeding up R-CNN should be pos-\\nsible in a variety of ways and remains as future work.\\n5. Semantic segmentation\\nRegion classiﬁcation is a standard technique for seman-\\ntic segmentation, allowing us to easily apply R-CNN to the\\nPASCAL VOC segmentation challenge. To facilitate a di-\\nrect comparison with the current leading semantic segmen-\\ntation system (called O 2P for “second-order pooling”) [4],\\nwe work within their open source framework. O 2P uses\\nCPMC to generate 150 region proposals per image and then\\npredicts the quality of each region, for each class, using'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='support vector regression (SVR). The high performance of\\ntheir approach is due to the quality of the CPMC regions\\nand the powerful second-order pooling of multiple feature\\ntypes (enriched variants of SIFT and LBP). We also note\\nthat Farabet et al. [16] recently demonstrated good results\\non several dense scene labeling datasets (not including PAS-\\nCAL) using a CNN as a multi-scale per-pixel classiﬁer.\\nWe follow [2, 4] and extend the PASCAL segmentation\\ntraining set to include the extra annotations made available\\nby Hariharan et al. [22]. Design decisions and hyperparam-\\neters were cross-validated on the VOC 2011 validation set.\\nFinal test results were evaluated only once.\\nCNN features for segmentation. We evaluate three strate-\\ngies for computing features on CPMC regions, all of which\\nbegin by warping the rectangular window around the re-\\ngion to 227 ×227. The ﬁrst strategy ( full) ignores the re-\\ngion’s shape and computes CNN features directly on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='warped window, exactly as we did for detection. However,\\nthese features ignore the non-rectangular shape of the re-\\ngion. Two regions might have very similar bounding boxes\\nwhile having very little overlap. Therefore, the second strat-\\negy (fg) computes CNN features only on a region’s fore-\\nground mask. We replace the background with the mean\\ninput so that background regions are zero after mean sub-\\ntraction. The third strategy ( full+fg) simply concatenates\\nthe full and fg features; our experiments validate their com-\\nplementarity.\\nfull R-CNN fg R-CNN full+fg R-CNN\\nO2P [4] fc6 fc7 fc6 fc7 fc6 fc7\\n46.4 43.0 42.5 43.7 42.1 47.9 45.8\\nTable 5: Segmentation mean accuracy (%) on VOC 2011 vali-\\ndation. Column 1 presents O2P; 2-7 use our CNN pre-trained on\\nILSVRC 2012.\\nResults on VOC 2011. Table 5 shows a summary of our\\nresults on the VOC 2011 validation set compared with O2P.\\n(See Appendix E for complete per-category results.) Within'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='each feature computation strategy, layer fc6 always outper-\\nforms fc7 and the following discussion refers to the fc6 fea-\\ntures. The fg strategy slightly outperforms full, indicating\\nthat the masked region shape provides a stronger signal,\\nmatching our intuition. However, full+fg achieves an aver-\\nage accuracy of 47.9%, our best result by a margin of 4.2%\\n(also modestly outperforming O2P), indicating that the con-\\ntext provided by the full features is highly informative even\\ngiven the fg features. Notably, training the 20 SVRs on our\\nfull+fg features takes an hour on a single core, compared to\\n10+ hours for training on O2P features.\\nIn Table 6 we present results on the VOC 2011 test\\nset, comparing our best-performing method, fc 6 (full+fg),\\nagainst two strong baselines. Our method achieves the high-\\nest segmentation accuracy for 11 out of 21 categories, and\\nthe highest overall segmentation accuracy of 47.9%, aver-\\naged across categories (but likely ties with the O 2P result'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='under any reasonable margin of error). Still better perfor-\\nmance could likely be achieved by ﬁne-tuning.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='VOC 2011 test bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nR&P [2] 83.446.8 18.9 36.6 31.2 42.7 57.3 47.4 44.1 8.1 39.436.136.3 49.5 48.3 50.7 26.3 47.2 22.1 42.0 43.2 40.8\\nO2P [4] 85.469.722.3 45.244.4 46.9 66.7 57.8 56.213.5 46.132.3 41.259.1 55.3 51.0 36.2 50.4 27.846.944.6 47.6\\nours(full+fgR-CNN fc6) 84.266.923.7 58.337.4 55.4 73.3 58.7 56.59.7 45.5 29.549.3 40.1 57.8 53.9 33.8 60.7 22.747.141.3 47.9\\nTable 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the “Regions and Parts” (R&P)\\nmethod of [2] and the second-order pooling (O 2P) method of [4]. Without any ﬁne-tuning, our CNN achieves top segmentation perfor-\\nmance, outperforming R&P and roughly matching O2P.\\n6. Conclusion\\nIn recent years, object detection performance had stag-\\nnated. The best performing systems were complex en-\\nsembles combining multiple low-level image features with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='high-level context from object detectors and scene classi-\\nﬁers. This paper presents a simple and scalable object de-\\ntection algorithm that gives a 30% relative improvement\\nover the best previous results on PASCAL VOC 2012.\\nWe achieved this performance through two insights. The\\nﬁrst is to apply high-capacity convolutional neural net-\\nworks to bottom-up region proposals in order to localize\\nand segment objects. The second is a paradigm for train-\\ning large CNNs when labeled training data is scarce. We\\nshow that it is highly effective to pre-train the network—\\nwith supervision—for a auxiliary task with abundant data\\n(image classiﬁcation) and then to ﬁne-tune the network for\\nthe target task where data is scarce (detection). We conjec-\\nture that the “supervised pre-training/domain-speciﬁc ﬁne-\\ntuning” paradigm will be highly effective for a variety of\\ndata-scarce vision problems.\\nWe conclude by noting that it is signiﬁcant that we\\nachieved these results by using a combination of classi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='cal tools from computer vision and deep learning (bottom-\\nup region proposals and convolutional neural networks).\\nRather than opposing lines of scientiﬁc inquiry, the two are\\nnatural and inevitable partners.\\nAcknowledgments. This research was supported in part\\nby DARPA Mind’s Eye and MSEE programs, by NSF\\nawards IIS-0905647, IIS-1134072, and IIS-1212798,\\nMURI N000014-10-1-0933, and by support from Toyota.\\nThe GPUs used in this research were generously donated\\nby the NVIDIA Corporation.\\nAppendix\\nA. Object proposal transformations\\nThe convolutional neural network used in this work re-\\nquires a ﬁxed-size input of 227 ×227 pixels. For detec-\\ntion, we consider object proposals that are arbitrary image\\nrectangles. We evaluated two approaches for transforming\\nobject proposals into valid CNN inputs.\\nThe ﬁrst method (“tightest square with context”) en-\\ncloses each object proposal inside the tightest square and\\n(A) (B) (C) (D)\\n (A) (B) (C) (D)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='Figure 7: Different object proposal transformations. (A) the\\noriginal object proposal at its actual scale relative to the trans-\\nformed CNN inputs; (B) tightest square with context; (C) tight-\\nest square without context; (D) warp. Within each column and\\nexample proposal, the top row corresponds top = 0pixels of con-\\ntext padding while the bottom row has p = 16 pixels of context\\npadding.\\nthen scales (isotropically) the image contained in that\\nsquare to the CNN input size. Figure 7 column (B) shows\\nthis transformation. A variant on this method (“tightest\\nsquare without context”) excludes the image content that\\nsurrounds the original object proposal. Figure 7 column\\n(C) shows this transformation. The second method (“warp”)\\nanisotropically scales each object proposal to the CNN in-\\nput size. Figure 7 column (D) shows the warp transforma-\\ntion.\\nFor each of these transformations, we also consider in-\\ncluding additional image context around the original object'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='proposal. The amount of context padding (p) is deﬁned as a\\nborder size around the original object proposal in the trans-\\nformed input coordinate frame. Figure 7 shows p = 0pix-\\nels in the top row of each example and p = 16 pixels in\\nthe bottom row. In all methods, if the source rectangle ex-\\ntends beyond the image, the missing data is replaced with\\nthe image mean (which is then subtracted before inputing\\nthe image into the CNN). A pilot set of experiments showed\\nthat warping with context padding ( p = 16pixels) outper-\\nformed the alternatives by a large margin (3-5 mAP points).\\nObviously more alternatives are possible, including using\\nreplication instead of mean padding. Exhaustive evaluation\\nof these alternatives is left as future work.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='B. Positive vs. negative examples and softmax\\nTwo design choices warrant further discussion. The ﬁrst\\nis: Why are positive and negative examples deﬁned differ-\\nently for ﬁne-tuning the CNN versus training the object de-\\ntection SVMs? To review the deﬁnitions brieﬂy, for ﬁne-\\ntuning we map each object proposal to the ground-truth in-\\nstance with which it has maximum IoU overlap (if any) and\\nlabel it as a positive for the matched ground-truth class if the\\nIoU is at least 0.5. All other proposals are labeled “back-\\nground” (i.e., negative examples for all classes). For train-\\ning SVMs, in contrast, we take only the ground-truth boxes\\nas positive examples for their respective classes and label\\nproposals with less than 0.3 IoU overlap with all instances\\nof a class as a negative for that class. Proposals that fall\\ninto the grey zone (more than 0.3 IoU overlap, but are not\\nground truth) are ignored.\\nHistorically speaking, we arrived at these deﬁnitions be-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='cause we started by training SVMs on features computed\\nby the ImageNet pre-trained CNN, and so ﬁne-tuning was\\nnot a consideration at that point in time. In that setup, we\\nfound that our particular label deﬁnition for training SVMs\\nwas optimal within the set of options we evaluated (which\\nincluded the setting we now use for ﬁne-tuning). When we\\nstarted using ﬁne-tuning, we initially used the same positive\\nand negative example deﬁnition as we were using for SVM\\ntraining. However, we found that results were much worse\\nthan those obtained using our current deﬁnition of positives\\nand negatives.\\nOur hypothesis is that this difference in how positives\\nand negatives are deﬁned is not fundamentally important\\nand arises from the fact that ﬁne-tuning data is limited.\\nOur current scheme introduces many “jittered” examples\\n(those proposals with overlap between 0.5 and 1, but not\\nground truth), which expands the number of positive exam-\\nples by approximately 30x. We conjecture that this large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='set is needed when ﬁne-tuning the entire network to avoid\\noverﬁtting. However, we also note that using these jittered\\nexamples is likely suboptimal because the network is not\\nbeing ﬁne-tuned for precise localization.\\nThis leads to the second issue: Why, after ﬁne-tuning,\\ntrain SVMs at all? It would be cleaner to simply apply the\\nlast layer of the ﬁne-tuned network, which is a 21-way soft-\\nmax regression classiﬁer, as the object detector. We tried\\nthis and found that performance on VOC 2007 dropped\\nfrom 54.2% to 50.9% mAP. This performance drop likely\\narises from a combination of several factors including that\\nthe deﬁnition of positive examples used in ﬁne-tuning does\\nnot emphasize precise localization and the softmax classi-\\nﬁer was trained on randomly sampled negative examples\\nrather than on the subset of “hard negatives” used for SVM\\ntraining.\\nThis result shows that it’s possible to obtain close to\\nthe same level of performance without training SVMs af-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='ter ﬁne-tuning. We conjecture that with some additional\\ntweaks to ﬁne-tuning the remaining performance gap may\\nbe closed. If true, this would simplify and speed up R-CNN\\ntraining with no loss in detection performance.\\nC. Bounding-box regression\\nWe use a simple bounding-box regression stage to im-\\nprove localization performance. After scoring each selec-\\ntive search proposal with a class-speciﬁc detection SVM,\\nwe predict a new bounding box for the detection using a\\nclass-speciﬁc bounding-box regressor. This is similar in\\nspirit to the bounding-box regression used in deformable\\npart models [17]. The primary difference between the two\\napproaches is that here we regress from features computed\\nby the CNN, rather than from geometric features computed\\non the inferred DPM part locations.\\nThe input to our training algorithm is a set of N train-\\ning pairs {(Pi,Gi)}i=1,...,N, where Pi = (Pi\\nx,Pi\\ny,Pi\\nw,Pi\\nh)\\nspeciﬁes the pixel coordinates of the center of proposalPi’s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='bounding box together with Pi’s width and height in pixels.\\nHence forth, we drop the superscript iunless it is needed.\\nEach ground-truth bounding box Gis speciﬁed in the same\\nway: G = (Gx,Gy,Gw,Gh). Our goal is to learn a trans-\\nformation that maps a proposed boxP to a ground-truth box\\nG.\\nWe parameterize the transformation in terms of four\\nfunctions dx(P), dy(P), dw(P), and dh(P). The ﬁrst\\ntwo specify a scale-invariant translation of the center of\\nP’s bounding box, while the second two specify log-space\\ntranslations of the width and height of P’s bounding box.\\nAfter learning these functions, we can transform an input\\nproposal P into a predicted ground-truth box ˆGby apply-\\ning the transformation\\nˆGx = Pwdx(P) +Px (1)\\nˆGy = Phdy(P) +Py (2)\\nˆGw = Pwexp(dw(P)) (3)\\nˆGh = Phexp(dh(P)). (4)\\nEach function d⋆(P) (where ⋆ is one of x,y,h,w ) is\\nmodeled as a linear function of the pool 5 features of pro-\\nposal P, denoted by φ5(P). (The dependence of φ5(P)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='on the image data is implicitly assumed.) Thus we have\\nd⋆(P) = wT\\n⋆φ5(P), where w⋆ is a vector of learnable\\nmodel parameters. We learn w⋆ by optimizing the regu-\\nlarized least squares objective (ridge regression):\\nw⋆ = argmin\\nˆw⋆\\nN∑\\ni\\n(ti\\n⋆ −ˆwT\\n⋆φ5(Pi))2 + λ∥ˆw⋆∥2 . (5)\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='The regression targets t⋆ for the training pair (P,G) are de-\\nﬁned as\\ntx = (Gx −Px)/Pw (6)\\nty = (Gy −Py)/Ph (7)\\ntw = log(Gw/Pw) (8)\\nth = log(Gh/Ph). (9)\\nAs a standard regularized least squares problem, this can be\\nsolved efﬁciently in closed form.\\nWe found two subtle issues while implementing\\nbounding-box regression. The ﬁrst is that regularization\\nis important: we set λ = 1000 based on a validation set.\\nThe second issue is that care must be taken when selecting\\nwhich training pairs (P,G) to use. Intuitively, if P is far\\nfrom all ground-truth boxes, then the task of transforming\\nP to a ground-truth box Gdoes not make sense. Using ex-\\namples like P would lead to a hopeless learning problem.\\nTherefore, we only learn from a proposal P if it is nearby\\nat least one ground-truth box. We implement “nearness” by\\nassigning P to the ground-truth box G with which it has\\nmaximum IoU overlap (in case it overlaps more than one) if\\nand only if the overlap is greater than a threshold (which we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='set to 0.6 using a validation set). All unassigned proposals\\nare discarded. We do this once for each object class in order\\nto learn a set of class-speciﬁc bounding-box regressors.\\nAt test time, we score each proposal and predict its new\\ndetection window only once. In principle, we could iterate\\nthis procedure (i.e., re-score the newly predicted bounding\\nbox, and then predict a new bounding box from it, and so\\non). However, we found that iterating does not improve\\nresults.\\nD. Additional feature visualizations\\nFigure 12 shows additional visualizations for 20 pool 5\\nunits. For each unit, we show the 24 region proposals that\\nmaximally activate that unit out of the full set of approxi-\\nmately 10 million regions in all of VOC 2007 test.\\nWe label each unit by its (y, x, channel) position in the\\n6 ×6 ×256 dimensional pool5 feature map. Within each\\nchannel, the CNN computes exactly the same function of\\nthe input region, with the (y, x) position changing only the\\nreceptive ﬁeld.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='E. Per-category segmentation results\\nIn Table 7 we show the per-category segmentation ac-\\ncuracy on VOC 2011 val for each of our six segmentation\\nmethods in addition to the O 2P method [4]. These results\\nshow which methods are strongest across each of the 20\\nPASCAL classes, plus the background class.\\nF. Analysis of cross-dataset redundancy\\nOne concern when training on an auxiliary dataset is that\\nthere might be redundancy between it and the test set. Even\\nthough the tasks of object detection and whole-image clas-\\nsiﬁcation are substantially different, making such cross-set\\nredundancy much less worrisome, we still conducted a thor-\\nough investigation that quantiﬁes the extent to which PAS-\\nCAL test images are contained within the ILSVRC 2012\\ntraining and validation sets. Our ﬁndings may be useful to\\nresearchers who are interested in using ILSVRC 2012 as\\ntraining data for the PASCAL image classiﬁcation task.\\nWe performed two checks for duplicate (and near-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='duplicate) images. The ﬁrst test is based on exact matches\\nof ﬂickr image IDs, which are included in the VOC 2007\\ntest annotations (these IDs are intentionally kept secret for\\nsubsequent PASCAL test sets). All PASCAL images, and\\nabout half of ILSVRC, were collected from ﬂickr.com. This\\ncheck turned up 31 matches out of 4952 (0.63%).\\nThe second check uses GIST [30] descriptor matching,\\nwhich was shown in [13] to have excellent performance at\\nnear-duplicate image detection in large (>1 million) image\\ncollections. Following [13], we computed GIST descrip-\\ntors on warped 32 ×32 pixel versions of all ILSVRC 2012\\ntrainval and PASCAL 2007 test images.\\nEuclidean distance nearest-neighbor matching of GIST\\ndescriptors revealed 38 near-duplicate images (including all\\n31 found by ﬂickr ID matching). The matches tend to vary\\nslightly in JPEG compression level and resolution, and to a\\nlesser extent cropping. These ﬁndings show that the overlap\\nis small, less than 1%. For VOC 2012, because ﬂickr IDs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='are not available, we used the GIST matching method only.\\nBased on GIST matches, 1.5% of VOC 2012 test images\\nare in ILSVRC 2012 trainval. The slightly higher rate for\\nVOC 2012 is likely due to the fact that the two datasets\\nwere collected closer together in time than VOC 2007 and\\nILSVRC 2012 were.\\nG. Document changelog\\nThis document tracks the progress of R-CNN. To help\\nreaders understand how it has changed over time, here’s a\\nbrief changelog describing the revisions.\\nv1 Initial version.\\nv2 CVPR 2014 camera-ready revision. Includes substan-\\ntial improvements in detection performance brought about\\nby (1) starting ﬁne-tuning from a higher learning rate (0.001\\ninstead of 0.0001), (2) using context padding when prepar-\\ning CNN inputs, and (3) bounding-box regression to ﬁx lo-\\ncalization errors.\\nv3 Results on the ILSVRC2013 detection dataset and com-\\nparison with OverFeat were integrated into several sections\\n(primarily Section 2 and Section 4).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='VOC 2011 val bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nO2P [4] 84.069.021.7 47.7 42.2 42.464.7 65.857.4 12.9 37.4 20.5 43.7 35.7 52.7 51.0 35.8 51.0 28.4 59.8 49.746.4\\nfullR-CNN fc6 81.356.2 23.9 42.9 40.7 38.8 59.2 56.5 53.2 11.4 34.6 16.7 48.1 37.0 51.4 46.0 31.5 44.0 24.3 53.7 51.143.0\\nfullR-CNN fc7 81.052.825.143.8 40.5 42.7 55.4 57.7 51.3 8.7 32.5 11.5 48.1 37.0 50.5 46.4 30.2 42.1 21.2 57.7 56.0 42.5\\nfgR-CNN fc6 81.454.1 21.1 40.6 38.753.6 59.9 57.2 52.5 9.1 36.5 23.6 46.4 38.1 53.2 51.3 32.2 38.7 29.053.0 47.543.7\\nfgR-CNN fc7 80.950.1 20.0 40.2 34.1 40.9 59.7 59.8 52.7 7.3 32.1 14.3 48.8 42.9 54.0 48.6 28.9 42.6 24.9 52.2 48.8 42.1\\nfull+fgR-CNN fc6 83.160.4 23.2 48.447.3 52.6 61.6 60.659.1 10.8 45.8 20.9 57.7 43.3 57.4 52.9 34.7 48.7 28.1 60.0 48.647.9\\nfull+fgR-CNN fc7 82.356.7 20.649.944.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.155.2 45.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='Table 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.\\nv4 The softmax vs. SVM results in Appendix B contained\\nan error, which has been ﬁxed. We thank Sergio Guadar-\\nrama for helping to identify this issue.\\nv5 Added results using the new 16-layer network architec-\\nture from Simonyan and Zisserman [43] to Section 3.3 and\\nTable 3.\\nReferences\\n[1] B. Alexe, T. Deselaers, and V . Ferrari. Measuring the object-\\nness of image windows. TPAMI, 2012. 2\\n[2] P. Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\\nJ. Malik. Semantic segmentation using regions and parts. In\\nCVPR, 2012. 10, 11\\n[3] P. Arbel ´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-\\nlik. Multiscale combinatorial grouping. In CVPR, 2014. 3\\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\\nmantic segmentation with second-order pooling. In ECCV,\\n2012. 4, 10, 11, 13, 14\\n[5] J. Carreira and C. Sminchisescu. CPMC: Automatic ob-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='ject segmentation using constrained parametric min-cuts.\\nTPAMI, 2012. 2, 3\\n[6] D. Cires ¸an, A. Giusti, L. Gambardella, and J. Schmidhu-\\nber. Mitosis detection in breast cancer histology images with\\ndeep neural networks. In MICCAI, 2013. 3\\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In CVPR, 2005. 1\\n[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, and J. Yagnik. Fast, accurate detection of\\n100,000 object classes on a single machine. In CVPR, 2013.\\n3\\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\\nFei. ImageNet Large Scale Visual Recognition Competition\\n2012 (ILSVRC2012). http://www.image-net.org/\\nchallenges/LSVRC/2012/. 1\\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. ImageNet: A large-scale hierarchical image database.\\nIn CVPR, 2009. 1\\n[11] J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. C.\\nBerg, and L. Fei-Fei. Scalable multi-label annotation. In\\nCHI, 2014. 8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='[12] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. InICML,\\n2014. 2\\n[13] M. Douze, H. J ´egou, H. Sandhawalia, L. Amsaleg, and\\nC. Schmid. Evaluation of gist descriptors for web-scale im-\\nage search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\\nChallenge. IJCV, 2010. 1, 4\\n[16] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. TPAMI, 2013. 10\\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. TPAMI, 2010. 2, 4, 7, 12\\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='segmentation for top-down detection. In CVPR, 2013. 4, 5\\n[19] K. Fukushima. Neocognitron: A self-organizing neu-\\nral network model for a mechanism of pattern recogni-\\ntion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-\\nnatively trained deformable part models, release 5. http:\\n//www.cs.berkeley.edu/˜rbg/latent-v5/. 2,\\n5, 6, 7\\n[21] C. Gu, J. J. Lim, P. Arbel ´aez, and J. Malik. Recognition\\nusing regions. In CVPR, 2009. 2\\n[22] B. Hariharan, P. Arbel ´aez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In ICCV, 2011.\\n10\\n[23] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In ECCV. 2012. 2, 7, 8\\n[24] Y . Jia. Caffe: An open source convolutional archi-\\ntecture for fast feature embedding. http://caffe.\\nberkeleyvision.org/, 2013. 3\\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='siﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 3, 4, 7\\n[26] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\\nW. Hubbard, and L. Jackel. Backpropagation applied to\\nhandwritten zip code recognition. Neural Comp., 1989. 1\\n[27] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. of the\\nIEEE, 1998. 1\\n[28] J. J. Lim, C. L. Zitnick, and P. Doll ´ar. Sketch tokens: A\\nlearned mid-level representation for contour and object de-\\ntection. In CVPR, 2013. 6, 7\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='class AP class AP class AP class AP class AP\\naccordion 50.8 centipede 30.4 hair spray 13.8 pencil box 11.4 snowplow 69.2\\nairplane 50.0 chain saw 14.1 hamburger 34.2 pencil sharpener 9.0 soap dispenser 16.8\\nant 31.8 chair 19.5 hammer 9.9 perfume 32.8 soccer ball 43.7\\nantelope 53.8 chime 24.6 hamster 46.0 person 41.7 sofa 16.3\\napple 30.9 cocktail shaker 46.2 harmonica 12.6 piano 20.5 spatula 6.8\\narmadillo 54.0 coffee maker 21.5 harp 50.4 pineapple 22.6 squirrel 31.3\\nartichoke 45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball 21.0 starﬁsh 45.1\\naxe 11.8 computer mouse 21.2 head cabbage 17.4 pitcher 19.2 stethoscope 18.3\\nbaby bed 42.0 corkscrew 24.2 helmet 33.4 pizza 43.7 stove 8.1\\nbackpack 2.8 cream 29.9 hippopotamus 38.0 plastic bag 6.4 strainer 9.9\\nbagel 37.5 croquet ball 30.0 horizontal bar 7.0 plate rack 15.2 strawberry 26.8\\nbalance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2\\nbanana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='band aid 17.4 cup or mug 34.0 iPod 59.2 porcupine 37.2 swimming trunks 9.1\\nbanjo 55.3 diaper 10.1 isopod 19.5 power drill 7.9 swine 45.3\\nbaseball 41.8 digital clock 18.5 jellyﬁsh 23.7 pretzel 24.8 syringe 5.7\\nbasketball 65.3 dishwasher 19.9 koala bear 44.3 printer 21.3 table 21.7\\nbathing cap 37.2 dog 76.8 ladle 3.0 puck 14.1 tape player 21.4\\nbeaker 11.3 domestic cat 44.1 ladybug 58.4 punching bag 29.4 tennis ball 59.1\\nbear 62.7 dragonﬂy 27.8 lamp 9.1 purse 8.0 tick 42.6\\nbee 52.9 drum 19.9 laptop 35.4 rabbit 71.0 tie 24.6\\nbell pepper 38.8 dumbbell 14.1 lemon 33.3 racket 16.2 tiger 61.8\\nbench 12.7 electric fan 35.0 lion 51.3 ray 41.1 toaster 29.2\\nbicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 trafﬁc light 24.7\\nbinder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8\\nbird 70.9 ﬁg 44.5 lobster 32.4 remote control 41.6 trombone 13.8\\nbookshelf 19.3 ﬁling cabinet 20.6 maillot 31.0 rubber eraser 2.5 trumpet 14.4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='bow tie 38.8 ﬂower pot 20.2 maraca 30.1 rugby ball 34.5 turtle 59.1\\nbow 9.0 ﬂute 4.9 microphone 4.0 ruler 11.5 tv or monitor 41.7\\nbowl 26.7 fox 59.3 microwave 40.1 salt or pepper shaker 24.6 unicycle 27.2\\nbrassiere 31.2 french horn 24.2 milk can 33.3 saxophone 40.8 vacuum 19.5\\nburrito 25.7 frog 64.1 miniskirt 14.9 scorpion 57.3 violin 13.7\\nbus 57.5 frying pan 21.5 monkey 49.6 screwdriver 10.6 volleyball 59.7\\nbutterﬂy 88.5 giant panda 42.5 motorcycle 42.2 seal 20.9 wafﬂe iron 24.0\\ncamel 37.6 goldﬁsh 28.6 mushroom 31.8 sheep 48.9 washer 39.8\\ncan opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1\\ncar 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9\\ncart 48.0 guacamole 32.3 oboe 27.5 snail 36.2 whale 48.6\\ncattle 32.3 guitar 33.1 orange 38.8 snake 33.8 wine bottle 31.2\\ncello 28.9 hair dryer 13.0 otter 22.2 snowmobile 58.8 zebra 49.6\\nTable 8: Per-class average precision (%) on the ILSVRC2013 detection test set.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='[29] D. Lowe. Distinctive image features from scale-invariant\\nkeypoints. IJCV, 2004. 1\\n[30] A. Oliva and A. Torralba. Modeling the shape of the scene:\\nA holistic representation of the spatial envelope.IJCV, 2001.\\n13\\n[31] X. Ren and D. Ramanan. Histograms of sparse codes for\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='lemon 0.79\\nlemon 0.70\\nlemon 0.56lemon 0.50\\nperson 0.88\\nperson 0.72\\ncocktail shaker 0.56\\ndog 0.97dog 0.85 dog 0.57\\nbird 0.63\\ndog 0.97dog 0.95\\ndog 0.64\\nhelmet 0.65\\nhelmet 0.52\\nmotorcycle 0.65\\nperson 0.75\\nperson 0.58\\nsnowmobile 0.83\\nsnowmobile 0.83\\nbow tie 0.86\\nperson 0.82\\nbird 0.61\\ndog 0.66\\ndog 0.61\\ndomestic cat 0.57\\nbird 0.96\\ndog 0.91\\ndog 0.77\\nsofa 0.71\\ndog 0.95\\ndog 0.55\\nladybug 1.00\\nperson 0.87\\ncar 0.96 car 0.66car 0.63\\nbird 0.98\\nperson 0.65\\nwatercraft 1.00\\nwatercraft 0.69\\npretzel 0.78\\ncar 0.96\\nperson 0.65person 0.58person 0.52\\nperson 0.52\\nbird 0.99 bird 0.91\\nbird 0.75\\ndog 0.98\\nflower pot 0.62\\ndog 0.97dog 0.56\\ntrain 1.00\\ntrain 0.53\\narmadillo 1.00\\narmadillo 0.56\\nbird 0.93\\ndog 0.92\\nswine 0.88\\nbird 1.00\\nbutterfly 0.96\\nperson 0.90\\nflower pot 0.62\\nsnake 0.70\\nturtle 0.54\\nbell pepper 0.81\\nbell pepper 0.62\\nbell pepper 0.54\\nruler 1.00\\nantelope 0.53\\nmushroom 0.93\\ntv or monitor 0.82\\ntv or monitor 0.76tv or monitor 0.54\\nbird 0.89\\nlipstick 0.80\\nlipstick 0.61\\nperson 0.58\\ndog 0.97\\nsoccer ball 0.90'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='Figure 8: Example detections on the val2 set from the conﬁguration that achieved 31.0% mAP on val2. Each image was sampled randomly\\n(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the\\nprecision value of that detection from the detector’s precision-recall curve. Viewing digitally with zoom is recommended.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17'}, page_content='baby bed 0.55helmet 0.51\\npitcher 0.57\\ndog 0.98\\nhat with a wide brim 0.78\\nperson 0.86\\nbird 0.52table 0.60\\nmonkey 0.97\\ntable 0.68\\nwatercraft 0.55\\nperson 0.88\\ncar 0.61\\nperson 0.87\\nperson 0.51\\nsunglasses 0.51\\ndog 0.94dog 0.55\\nbird 0.52\\nmonkey 0.87\\nmonkey 0.81\\nswine 0.50\\ndog 0.97\\nhat with a wide brim 0.96\\nsnake 0.74\\ndog 0.93\\nperson 0.77\\ndog 0.97\\nguacamole 0.64\\npretzel 0.69\\ntable 0.54\\ndog 0.71\\nperson 0.85\\nladybug 0.90\\nperson 0.52\\nzebra 0.83 zebra 0.80\\nzebra 0.55\\nzebra 0.52\\ndog 0.98\\nhat with a wide brim 0.60person 0.85\\nperson 0.81 person 0.73\\nelephant 1.00\\nbird 0.99\\nperson 0.58\\ndog 0.98\\ncart 1.00\\nchair 0.79chair 0.64\\nperson 0.91person 0.87 person 0.57\\nperson 0.52\\ncomputer keyboard 0.52\\ndog 0.97 dog 0.92\\nperson 0.77\\nbird 0.94\\nbutterfly 0.98\\nperson 0.73\\nperson 0.61\\nbird 1.00\\nbird 0.78\\nperson 0.91 person 0.75\\nstethoscope 0.83\\nbird 0.83\\nFigure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='person 0.81\\nperson 0.57\\nperson 0.53\\nmotorcycle 0.64\\nperson 0.73\\nperson 0.51\\nbagel 0.57\\npineapple 1.00\\nbowl 0.63\\nguacamole 1.00tennis ball 0.60\\nlemon 0.88\\nlemon 0.86lemon 0.80\\nlemon 0.78\\norange 0.78\\norange 0.73\\norange 0.71\\ngolf ball 1.00\\ngolf ball 1.00\\ngolf ball 0.89\\ngolf ball 0.81\\ngolf ball 0.79\\ngolf ball 0.76golf ball 0.60\\ngolf ball 0.60\\ngolf ball 0.51\\nlemon 0.53\\nsoccer ball 0.67\\nlamp 0.61\\ntable 0.59\\nbee 0.85\\njellyfish 0.71\\nbowl 0.54\\nhamburger 0.78\\ndumbbell 1.00person 0.52\\nmicrophone 1.00\\nperson 0.85\\nhead cabbage 0.83\\nhead cabbage 0.75\\ndog 0.74\\ngoldfish 0.76\\nperson 0.57\\nguitar 1.00\\nguitar 1.00\\nguitar 0.88\\ntable 0.63\\ncomputer keyboard 0.78\\nmicrowave 0.60\\ntable 0.53\\ntick 0.64\\nlemon 0.80\\ntennis ball 0.67\\nrabbit 1.00\\ndog 0.98\\nperson 0.81\\nperson 0.92\\nsunglasses 0.52\\nwatercraft 0.86\\nmilk can 1.00\\nmilk can 1.00\\nbookshelf 0.50\\nchair 0.86\\ngiant panda 0.61\\nperson 0.87\\nantelope 0.74\\ncattle 0.81\\ndog 0.87\\nhorse 0.78\\npomegranate 1.00\\nchair 0.86\\ntv or monitor 0.52\\nantelope 0.68\\nbird 0.94\\nsnake 0.60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='dog 0.98\\ndog 0.88\\nperson 0.79\\nsnake 0.76\\ntable 0.62\\ntv or monitor 0.80\\ntv or monitor 0.58\\ntv or monitor 0.54\\nlamp 0.86lamp 0.65\\ntable 0.83\\nmonkey 1.00monkey 1.00\\nmonkey 0.90\\nmonkey 0.88\\nmonkey 0.52\\ndog 0.88fox 1.00\\nfox 0.81\\nperson 0.88\\nwatercraft 0.91\\nwatercraft 0.56\\nbird 0.95\\nbird 0.78\\nisopod 0.56\\nbird 0.69\\nstarfish 0.67\\ndragonfly 0.70\\ndragonfly 0.60\\nhamburger 0.72\\nhamburger 0.60\\ncup or mug 0.72\\nelectric fan 1.00\\nelectric fan 0.83\\nelectric fan 0.78helmet 0.64\\nsoccer ball 0.63\\nFigure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing\\ndigitally with zoom is recommended.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='object detection. In CVPR, 2013. 6, 7\\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\\nbased face detection. TPAMI, 1998. 2\\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\\ning internal representations by error propagation. Parallel\\nDistributed Processing, 1:318–362, 1986. 1\\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\\ntion and Detection using Convolutional Networks. In ICLR,\\n2014. 1, 2, 4, 10\\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. In CVPR, 2013. 2\\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\\nfor visual object detection. In AAAI Technical Report, 4th\\nHuman Computation Workshop, 2012. 8\\n[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\\nfor object detection. In NIPS, 2013. 2\\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\\nSelective search for object recognition. IJCV, 2013. 1, 2, 3,\\n4, 5, 9\\n[40] R. Vaillant, C. Monrocq, and Y . LeCun. Original approach\\nfor the localisation of objects in images. IEE Proc on Vision,\\nImage, and Signal Processing, 1994. 2\\n[41] X. Wang, M. Yang, S. Zhu, and Y . Lin. Regionlets for generic\\nobject detection. In ICCV, 2013. 3, 5\\n[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-\\ntional networks for mid and high level feature learning. In\\nCVPR, 2011. 4\\n[43] K. Simonyan and A. Zisserman. Very Deep Convolu-\\ntional Networks for Large-Scale Image Recognition. arXiv\\npreprint, arXiv:1409.1556, 2014. 6, 7, 14\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='person 0.82\\nsnake 0.76\\nfrog 0.78\\nbird 0.79\\ngoldfish 0.76\\ngoldfish 0.76\\ngoldfish 0.58\\nperson 0.94\\nstethoscope 0.56\\nperson 0.95person 0.92person 0.67\\nperson 0.60\\ntable 0.81\\njellyfish 0.67\\nlemon 0.52\\nperson 0.78\\nperson 0.65\\nwatercraft 0.55\\nbaseball 1.00\\nperson 0.94\\nperson 0.82\\nperson 0.80\\nperson 0.61\\nperson 0.55\\nperson 0.52\\ncomputer keyboard 0.81\\ndog 0.60 person 0.88\\nperson 0.79\\nperson 0.68\\nperson 0.59\\ntv or monitor 0.82\\nlizard 0.58\\nchair 0.50\\nperson 0.74\\ntable 0.82\\nperson 0.94\\nperson 0.94\\nperson 0.95\\nperson 0.81person 0.69\\nrugby ball 0.91\\nperson 0.84 person 0.59\\nvolleyball 0.70\\npineapple 1.00\\nbrassiere 0.71\\nperson 0.95 person 0.94person 0.94\\nperson 0.81 person 0.80person 0.80\\nperson 0.79\\nperson 0.79\\nperson 0.69\\nperson 0.66\\nperson 0.58\\nperson 0.56person 0.54\\nswimming trunks 0.56\\nbaseball 0.86\\nhelmet 0.74\\nperson 0.75\\nminiskirt 0.64\\nperson 0.92\\nvacuum 1.00\\ndog 0.98\\ndog 0.93\\nperson 0.94 person 0.75\\nperson 0.65\\nperson 0.53\\nski 0.80 ski 0.80\\nbird 0.55\\ntiger 1.00\\ntiger 0.67\\ntiger 0.59\\nbird 0.56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='whale 1.00\\nchair 0.53\\nperson 0.92\\nperson 0.92\\nperson 0.82person 0.78\\nbowl 0.52\\nstrawberry 0.79strawberry 0.70\\nburrito 0.54\\ncroquet ball 0.91croquet ball 0.91croquet ball 0.91 croquet ball 0.91\\nmushroom 0.57\\nwatercraft 0.91\\nwatercraft 0.87\\nwatercraft 0.58\\nplastic bag 0.62\\nplastic bag 0.62\\nwhale 0.88\\ncar 0.70\\ndog 0.94\\ntv or monitor 0.57\\ncart 0.80\\nperson 0.79\\nperson 0.53\\nhat with a wide brim 0.89person 0.88\\nperson 0.82\\nperson 0.79\\nperson 0.56\\nperson 0.54\\ntraffic light 0.79\\nbird 0.59\\ncucumber 0.53\\ncucumber 0.52\\nantelope 1.00\\nantelope 1.00\\nantelope 0.94\\nantelope 0.73\\nantelope 0.63\\nantelope 0.63\\nfox 0.57\\nbalance beam 0.50horizontal bar 1.00\\nperson 0.80\\nperson 0.90\\nsnake 0.64\\ndog 0.98\\ndog 0.97\\nhelmet 0.69\\nhorse 0.92\\nhorse 0.69\\nperson 0.82\\nperson 0.72\\norange 0.79\\norange 0.71\\norange 0.66\\norange 0.66\\norange 0.59\\norange 0.56\\nbird 0.97\\nbird 0.96\\nbird 0.96\\nbird 0.94\\nbird 0.89\\nbird 0.64\\nbird 0.56\\nbird 0.53bird 0.52\\nguitar 1.00\\nperson 0.82\\nbicycle 0.92\\nperson 0.90\\nperson 0.83\\ncar 1.00 car 0.97'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='dog 0.98dog 0.86\\ndog 0.85\\ndog 0.65dog 0.50\\nperson 0.83\\nperson 0.80\\nperson 0.74person 0.54\\nelephant 0.60\\nFigure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,1) (top 1 − 24)\\n1.0 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,2) (top 1 − 24)\\n1.0 0.9 0.9 0.9 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,3) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,4) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,5) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,6) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,7) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,8) (top 1 − 24)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,9) (top 1 − 24)\\n0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,10) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.5 0.5\\npool5 feature: (3,3,11) (top 1 − 24)\\n0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,12) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,13) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\npool5 feature: (3,3,14) (top 1 − 24)\\n0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,15) (top 1 − 24)\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,16) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,17) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,18) (top 1 − 24)\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,19) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,20) (top 1 − 24)\\n1.0 0.9 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\nFigure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly\\nactivate each of 20 units. Each montage is labeled by the unit’s (y, x, channel) position in the6 ×6 ×256 dimensional pool5 feature map.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='Each image region is drawn with an overlay of the unit’s receptive ﬁeld in white. The activation value (which we normalize by dividing by\\nthe max activation value over all units in a channel) is shown in the receptive ﬁeld’s upper-left corner. Best viewed digitally with zoom.\\n21')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a748c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan\\AppData\\Local\\Temp\\ipykernel_14064\\50022705.py:5: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "# Vector Embading and vector store chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "db = Chroma.from_documents(document[:20],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189be442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant\\nperformance boost. Since we combine region proposals\\nwith CNNs, we call our method R-CNN: Regions with CNN\\nfeatures. We also compare R-CNN to OverFeat, a recently\\nproposed sliding-window detector based on a similar CNN\\narchitecture. We ﬁnd that R-CNN outperforms OverFeat\\nby a large margin on the 200-class ILSVRC2013 detection\\ndataset. Source code for the complete system is available at\\nhttp://www.cs.berkeley.edu/˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [39] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\nOn the 200-class ILSVRC2013 detection dataset, R-CNN’s\\nmAP is 31.4% , a large improvement over OverFeat [34], which\\nhad the previous best result at 24.3%.\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training\\nalgorithm. Building on Rumelhart et al. [33], LeCun et\\nal. [26] showed that stochastic gradient descent via back-\\npropagation was effective for training convolutional neural\\nnetworks (CNNs), a class of models that extend the neocog-\\nnitron.\\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\\nfell out of fashion with the rise of support vector machines.\\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\\nby showing substantially higher image classiﬁcation accu-\\nracy on the ImageNet Large Scale Visual Recognition Chal-\\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\\ning a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\n1\\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector Database \n",
    "query = \"What is The paper talking about\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd77bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# db1 = FAISS.from_documents(document[:10],embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9355175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7409676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is The paper talking about\"\n",
    "# result = db1.similarity_search(query)\n",
    "# result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fecb8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama3.2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa8d8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following questions based only on the provided context.\n",
    "Think before providing any answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question:{input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89533d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain Integration \n",
    "# Create Stuff Document Chain\n",
    "\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedf35d",
   "metadata": {},
   "source": [
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Document objects as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create retriever (NEW - this is the modern way)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c45cdeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001DE90710EC0>, search_kwargs={})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b17eae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRetriever chain: This chain takes in a user inquary, which is then passed to the retriever to fetch relevent documents. Those documents(and the original input) are then passed to the llm to generate a response\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retriever chain: This chain takes in a user inquary, which is then passed to the retriever to fetch relevent documents. Those documents(and the original input) are then passed to the llm to generate a response\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b2e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the chain (MODERN LCEL APPROACH)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no mention of a galaxy in the provided text. The text appears to be related to object detection with a deep learning-based approach called R-CNN, and does not mention galaxies or astronomy.\n"
     ]
    }
   ],
   "source": [
    "# 7. Use the chain\n",
    "query = \"What is the name of our galaxy\"\n",
    "answer = rag_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c132a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
