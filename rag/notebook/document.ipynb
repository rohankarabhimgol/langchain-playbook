{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd83130a",
   "metadata": {},
   "source": [
    "### DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a15956",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document datastructure \n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21263de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Rohan', 'date_created': '2025-01-01'}, page_content='This is some thing to learn about RAG ')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content=\"This is some thing to learn about RAG \",\n",
    "    metadata = {\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\":1,\n",
    "        \"author\":\"Rohan\",\n",
    "        \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206309cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51d9da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text file created\n"
     ]
    }
   ],
   "source": [
    "sample_texts = {\n",
    "    \"../data/text_files/python_intro.txt\":'''Thinking about learning to code? Discover why Python is the most recommended language for beginners and experts alike, from web development to AI.\n",
    "\n",
    "Have you ever felt like the digital world is moving forward, and you're just watching? Or maybe you have a brilliant idea for an app, a tool to automate a tedious task, or a desire to break into tech? There’s one skill that unlocks all these doors and more: learning to code.\n",
    "\n",
    "But with hundreds of programming languages out there, which one should you choose? For an overwhelming majority of beginners, professionals, and even giants like Google and Netflix, the answer is Python.\n",
    "\n",
    "Here’s why Python isn’t just a language; it’s a superpower.\n",
    "\n",
    "1. It’s Beginner-Friendly & Readable\n",
    "Python was designed with clarity in mind. Its syntax is clean, intuitive, and almost reads like plain English. Where other languages use confusing curly braces and semicolons, Python uses indentation. This lowers the barrier to entry dramatically. You spend less time wrestling with complex syntax and more time understanding core programming concepts. It’s the perfect \"gateway language.\"\n",
    "\n",
    "2. It’s Incredibly Versatile (The \"Swiss Army Knife\")\n",
    "Python’s \"batteries-included\" philosophy means it has powerful libraries for almost everything:\n",
    "\n",
    "Web Development: Use frameworks like Django (for robust, scalable sites) and Flask (for lightweight applications) to build the next Instagram or Pinterest.\n",
    "\n",
    "Data Science & Analytics: Libraries like Pandas, NumPy, and Matplotlib make it the go-to language for analyzing data, creating visualizations, and uncovering insights.\n",
    "\n",
    "Artificial Intelligence & Machine Learning: This is Python’s superstar domain. TensorFlow, PyTorch, and scikit-learn are industry standards for building intelligent systems.\n",
    "\n",
    "Automation & Scripting: Automate boring tasks—renaming files, scraping web data, sending emails, filling forms. Python is the ultimate productivity booster.\n",
    "\n",
    "Software Development, Game Dev, Internet of Things (IoT) – you name it.\n",
    "\n",
    "3. It Has a Massive Community & Ocean of Libraries\n",
    "Stuck on a problem? Chances are, millions have been there before. Python’s vast, friendly community ensures you’ll find help on forums like Stack Overflow. The Python Package Index (PyPI) hosts over 400,000 libraries, meaning you don’t have to build everything from scratch. Need to add a feature? There’s probably a library for that.\n",
    "\n",
    "4. It’s a Career Rocket Fuel\n",
    "Python skills are in explosive demand. Roles like Python Developer, Data Scientist, Machine Learning Engineer, DevOps Engineer, and Backend Developer consistently rank among the top-paying and most future-proof jobs. Learning Python is a strategic investment in your career, opening doors in virtually every industry—from finance and healthcare to entertainment and science.\n",
    "\n",
    "5. It’s the Language of the Future (AI/ML)\n",
    "We are living in the era of artificial intelligence. Python is the undisputed lingua franca of AI, machine learning, and data science. If you have any interest in shaping the future—whether it’s creating smart recommendations, self-driving car algorithms, or advanced medical diagnostics—Python is your foundational tool.\n",
    "\n",
    "Conclusion: Start Your Journey Today\n",
    "Whether you’re a student, a professional looking to upskill, or a curious mind with a side-project idea, Python is the most rewarding and practical first step. It’s powerful yet gentle, professional yet accessible. It’s not just about writing code; it’s about building solutions, automating the mundane, and creating value.\n",
    "\n",
    "The best time to learn Python was five years ago. The second-best time is now. The resources are free, the community is waiting, and the opportunities are endless. What will you build?\n",
    "\n",
    "''',\n",
    "\"../data/text_files/machine_learning.txt\":'''Machine Learning sounds complex, but it's transforming our world. Discover why understanding ML is becoming an essential skill for the future, no matter your field.\n",
    "\n",
    "We interact with machine learning (ML) dozens of times a day, often without realizing it. That Netflix recommendation? ML. Your spam filter? ML. The voice assistant in your phone, the fraud alert from your bank, even the social media feed you’re scrolling through—all powered by ML.\n",
    "\n",
    "Once a niche field for PhDs, machine learning is now a fundamental literacy for the 21st century. Here’s why you should consider learning it, regardless of your background.\n",
    "\n",
    "1. It’s the Engine of the Modern World (And Your Career)\n",
    "ML is no longer a \"nice-to-have\"; it's the core competitive advantage for businesses. From optimizing supply chains and predicting market trends to personalizing customer experiences and developing new drugs, ML is driving innovation. Understanding it makes you an invaluable asset, opening doors to roles like Machine Learning Engineer, Data Scientist, AI Researcher, or ML Product Manager—some of the most lucrative and in-demand jobs globally.\n",
    "\n",
    "2. It Supercharges Your Problem-Solving Toolkit\n",
    "Learning ML teaches you a new way to think about problems. Instead of programming explicit rules (\"if this, then that\"), you learn to create systems that learn from data and find patterns you might never see. This data-driven, probabilistic mindset is incredibly powerful, applicable to business strategy, scientific research, and even creative pursuits. It's about teaching computers to learn, not just to follow instructions.\n",
    "\n",
    "3. Demystify the Technology That Shapes Your Life\n",
    "Algorithms influence what we see, buy, and believe. By learning ML, you move from being a passive consumer of technology to an informed citizen and critic. You’ll understand the ethical implications—like bias in AI systems, data privacy, and the future of work. This knowledge empowers you to ask better questions, make informed decisions, and participate in crucial societal conversations about our technological future.\n",
    "\n",
    "4. The Tools Are Now Accessible to Everyone\n",
    "The barrier to entry has never been lower. Thanks to:\n",
    "\n",
    "Powerful, Open-Source Libraries: Frameworks like scikit-learn, TensorFlow, and PyTorch make building ML models as simple as writing a few lines of Python.\n",
    "\n",
    "Massive, Available Datasets: Platforms like Kaggle offer real-world data to practice on.\n",
    "\n",
    "Cloud Computing: You don’t need a supercomputer; you can rent one by the minute from services like Google Colab, AWS, or Azure.\n",
    "You can start learning and building meaningful projects with just a laptop and an internet connection.\n",
    "\n",
    "5. It Unlocks Creativity and Innovation Across Fields\n",
    "ML isn't just for techies. It’s a creative tool:\n",
    "\n",
    "In Art & Music: Generating novel images, compositions, and styles.\n",
    "\n",
    "In Healthcare: Analyzing medical images for early disease detection.\n",
    "\n",
    "In Environmental Science: Modeling climate change or tracking wildlife.\n",
    "\n",
    "In Your Own Domain: What unique problem in your field could be solved with pattern recognition or prediction? Learning ML gives you the tools to find out.\n",
    "\n",
    "Conclusion: Don't Just Watch the Future—Build It\n",
    "Machine learning is not a passing trend; it’s a foundational shift in how we use computers. Learning it isn't about becoming an expert overnight. It's about gaining a critical new lens on the world.\n",
    "\n",
    "Start with the basics: a bit of Python, some statistics, and the curiosity to explore. The journey will challenge you, but it will also equip you with one of the most relevant and transformative skills of our time.\n",
    "\n",
    "The age of artificial intelligence is here. Will you simply use it, or will you understand, shape, and create with it? The choice, and the tools, are in your hands.\n",
    "\n",
    "'''\n",
    "}\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "print(\"Sample Text file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4d07fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Studies\\Programming\\LangChain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Thinking about learning to code? Discover why Python is the most recommended language for beginners and experts alike, from web development to AI.\\n\\nHave you ever felt like the digital world is moving forward, and you\\'re just watching? Or maybe you have a brilliant idea for an app, a tool to automate a tedious task, or a desire to break into tech? There’s one skill that unlocks all these doors and more: learning to code.\\n\\nBut with hundreds of programming languages out there, which one should you choose? For an overwhelming majority of beginners, professionals, and even giants like Google and Netflix, the answer is Python.\\n\\nHere’s why Python isn’t just a language; it’s a superpower.\\n\\n1. It’s Beginner-Friendly & Readable\\nPython was designed with clarity in mind. Its syntax is clean, intuitive, and almost reads like plain English. Where other languages use confusing curly braces and semicolons, Python uses indentation. This lowers the barrier to entry dramatically. You spend less time wrestling with complex syntax and more time understanding core programming concepts. It’s the perfect \"gateway language.\"\\n\\n2. It’s Incredibly Versatile (The \"Swiss Army Knife\")\\nPython’s \"batteries-included\" philosophy means it has powerful libraries for almost everything:\\n\\nWeb Development: Use frameworks like Django (for robust, scalable sites) and Flask (for lightweight applications) to build the next Instagram or Pinterest.\\n\\nData Science & Analytics: Libraries like Pandas, NumPy, and Matplotlib make it the go-to language for analyzing data, creating visualizations, and uncovering insights.\\n\\nArtificial Intelligence & Machine Learning: This is Python’s superstar domain. TensorFlow, PyTorch, and scikit-learn are industry standards for building intelligent systems.\\n\\nAutomation & Scripting: Automate boring tasks—renaming files, scraping web data, sending emails, filling forms. Python is the ultimate productivity booster.\\n\\nSoftware Development, Game Dev, Internet of Things (IoT) – you name it.\\n\\n3. It Has a Massive Community & Ocean of Libraries\\nStuck on a problem? Chances are, millions have been there before. Python’s vast, friendly community ensures you’ll find help on forums like Stack Overflow. The Python Package Index (PyPI) hosts over 400,000 libraries, meaning you don’t have to build everything from scratch. Need to add a feature? There’s probably a library for that.\\n\\n4. It’s a Career Rocket Fuel\\nPython skills are in explosive demand. Roles like Python Developer, Data Scientist, Machine Learning Engineer, DevOps Engineer, and Backend Developer consistently rank among the top-paying and most future-proof jobs. Learning Python is a strategic investment in your career, opening doors in virtually every industry—from finance and healthcare to entertainment and science.\\n\\n5. It’s the Language of the Future (AI/ML)\\nWe are living in the era of artificial intelligence. Python is the undisputed lingua franca of AI, machine learning, and data science. If you have any interest in shaping the future—whether it’s creating smart recommendations, self-driving car algorithms, or advanced medical diagnostics—Python is your foundational tool.\\n\\nConclusion: Start Your Journey Today\\nWhether you’re a student, a professional looking to upskill, or a curious mind with a side-project idea, Python is the most rewarding and practical first step. It’s powerful yet gentle, professional yet accessible. It’s not just about writing code; it’s about building solutions, automating the mundane, and creating value.\\n\\nThe best time to learn Python was five years ago. The second-best time is now. The resources are free, the community is waiting, and the opportunities are endless. What will you build?\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cacefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 107.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning sounds complex, but it\\'s transforming our world. Discover why understanding ML is becoming an essential skill for the future, no matter your field.\\n\\nWe interact with machine learning (ML) dozens of times a day, often without realizing it. That Netflix recommendation? ML. Your spam filter? ML. The voice assistant in your phone, the fraud alert from your bank, even the social media feed you’re scrolling through—all powered by ML.\\n\\nOnce a niche field for PhDs, machine learning is now a fundamental literacy for the 21st century. Here’s why you should consider learning it, regardless of your background.\\n\\n1. It’s the Engine of the Modern World (And Your Career)\\nML is no longer a \"nice-to-have\"; it\\'s the core competitive advantage for businesses. From optimizing supply chains and predicting market trends to personalizing customer experiences and developing new drugs, ML is driving innovation. Understanding it makes you an invaluable asset, opening doors to roles like Machine Learning Engineer, Data Scientist, AI Researcher, or ML Product Manager—some of the most lucrative and in-demand jobs globally.\\n\\n2. It Supercharges Your Problem-Solving Toolkit\\nLearning ML teaches you a new way to think about problems. Instead of programming explicit rules (\"if this, then that\"), you learn to create systems that learn from data and find patterns you might never see. This data-driven, probabilistic mindset is incredibly powerful, applicable to business strategy, scientific research, and even creative pursuits. It\\'s about teaching computers to learn, not just to follow instructions.\\n\\n3. Demystify the Technology That Shapes Your Life\\nAlgorithms influence what we see, buy, and believe. By learning ML, you move from being a passive consumer of technology to an informed citizen and critic. You’ll understand the ethical implications—like bias in AI systems, data privacy, and the future of work. This knowledge empowers you to ask better questions, make informed decisions, and participate in crucial societal conversations about our technological future.\\n\\n4. The Tools Are Now Accessible to Everyone\\nThe barrier to entry has never been lower. Thanks to:\\n\\nPowerful, Open-Source Libraries: Frameworks like scikit-learn, TensorFlow, and PyTorch make building ML models as simple as writing a few lines of Python.\\n\\nMassive, Available Datasets: Platforms like Kaggle offer real-world data to practice on.\\n\\nCloud Computing: You don’t need a supercomputer; you can rent one by the minute from services like Google Colab, AWS, or Azure.\\nYou can start learning and building meaningful projects with just a laptop and an internet connection.\\n\\n5. It Unlocks Creativity and Innovation Across Fields\\nML isn\\'t just for techies. It’s a creative tool:\\n\\nIn Art & Music: Generating novel images, compositions, and styles.\\n\\nIn Healthcare: Analyzing medical images for early disease detection.\\n\\nIn Environmental Science: Modeling climate change or tracking wildlife.\\n\\nIn Your Own Domain: What unique problem in your field could be solved with pattern recognition or prediction? Learning ML gives you the tools to find out.\\n\\nConclusion: Don\\'t Just Watch the Future—Build It\\nMachine learning is not a passing trend; it’s a foundational shift in how we use computers. Learning it isn\\'t about becoming an expert overnight. It\\'s about gaining a critical new lens on the world.\\n\\nStart with the basics: a bit of Python, some statistics, and the curiosity to explore. The journey will challenge you, but it will also equip you with one of the most relevant and transformative skills of our time.\\n\\nThe age of artificial intelligence is here. Will you simply use it, or will you understand, shape, and create with it? The choice, and the tools, are in your hands.\\n\\n'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Thinking about learning to code? Discover why Python is the most recommended language for beginners and experts alike, from web development to AI.\\n\\nHave you ever felt like the digital world is moving forward, and you\\'re just watching? Or maybe you have a brilliant idea for an app, a tool to automate a tedious task, or a desire to break into tech? There’s one skill that unlocks all these doors and more: learning to code.\\n\\nBut with hundreds of programming languages out there, which one should you choose? For an overwhelming majority of beginners, professionals, and even giants like Google and Netflix, the answer is Python.\\n\\nHere’s why Python isn’t just a language; it’s a superpower.\\n\\n1. It’s Beginner-Friendly & Readable\\nPython was designed with clarity in mind. Its syntax is clean, intuitive, and almost reads like plain English. Where other languages use confusing curly braces and semicolons, Python uses indentation. This lowers the barrier to entry dramatically. You spend less time wrestling with complex syntax and more time understanding core programming concepts. It’s the perfect \"gateway language.\"\\n\\n2. It’s Incredibly Versatile (The \"Swiss Army Knife\")\\nPython’s \"batteries-included\" philosophy means it has powerful libraries for almost everything:\\n\\nWeb Development: Use frameworks like Django (for robust, scalable sites) and Flask (for lightweight applications) to build the next Instagram or Pinterest.\\n\\nData Science & Analytics: Libraries like Pandas, NumPy, and Matplotlib make it the go-to language for analyzing data, creating visualizations, and uncovering insights.\\n\\nArtificial Intelligence & Machine Learning: This is Python’s superstar domain. TensorFlow, PyTorch, and scikit-learn are industry standards for building intelligent systems.\\n\\nAutomation & Scripting: Automate boring tasks—renaming files, scraping web data, sending emails, filling forms. Python is the ultimate productivity booster.\\n\\nSoftware Development, Game Dev, Internet of Things (IoT) – you name it.\\n\\n3. It Has a Massive Community & Ocean of Libraries\\nStuck on a problem? Chances are, millions have been there before. Python’s vast, friendly community ensures you’ll find help on forums like Stack Overflow. The Python Package Index (PyPI) hosts over 400,000 libraries, meaning you don’t have to build everything from scratch. Need to add a feature? There’s probably a library for that.\\n\\n4. It’s a Career Rocket Fuel\\nPython skills are in explosive demand. Roles like Python Developer, Data Scientist, Machine Learning Engineer, DevOps Engineer, and Backend Developer consistently rank among the top-paying and most future-proof jobs. Learning Python is a strategic investment in your career, opening doors in virtually every industry—from finance and healthcare to entertainment and science.\\n\\n5. It’s the Language of the Future (AI/ML)\\nWe are living in the era of artificial intelligence. Python is the undisputed lingua franca of AI, machine learning, and data science. If you have any interest in shaping the future—whether it’s creating smart recommendations, self-driving car algorithms, or advanced medical diagnostics—Python is your foundational tool.\\n\\nConclusion: Start Your Journey Today\\nWhether you’re a student, a professional looking to upskill, or a curious mind with a side-project idea, Python is the most rewarding and practical first step. It’s powerful yet gentle, professional yet accessible. It’s not just about writing code; it’s about building solutions, automating the mundane, and creating value.\\n\\nThe best time to learn Python was five years ago. The second-best time is now. The resources are free, the community is waiting, and the opportunities are endless. What will you build?\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## Load all the files form a directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", # Pattern to match files\n",
    "    loader_cls= TextLoader, # loader class to use can provide multiple cls by specifying it in list eg:[TextLoader,PdfLoader]\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87009ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant\\nperformance boost. Since we combine region proposals\\nwith CNNs, we call our method R-CNN: Regions with CNN\\nfeatures. We also compare R-CNN to OverFeat, a recently\\nproposed sliding-window detector based on a similar CNN\\narchitecture. We ﬁnd that R-CNN outperforms OverFeat\\nby a large margin on the 200-class ILSVRC2013 detection\\ndataset. Source code for the complete system is available at\\nhttp://www.cs.berkeley.edu/˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [39] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\nOn the 200-class ILSVRC2013 detection dataset, R-CNN’s\\nmAP is 31.4% , a large improvement over OverFeat [34], which\\nhad the previous best result at 24.3%.\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training\\nalgorithm. Building on Rumelhart et al. [33], LeCun et\\nal. [26] showed that stochastic gradient descent via back-\\npropagation was effective for training convolutional neural\\nnetworks (CNNs), a class of models that extend the neocog-\\nnitron.\\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\\nfell out of fashion with the rise of support vector machines.\\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\\nby showing substantially higher image classiﬁcation accu-\\nracy on the ImageNet Large Scale Visual Recognition Chal-\\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\\ning a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\n1\\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='debated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question by bridging the gap between\\nimage classiﬁcation and object detection. This paper is the\\nﬁrst to show that a CNN can lead to dramatically higher ob-\\nject detection performance on PASCAL VOC as compared\\nto systems based on simpler HOG-like features. To achieve\\nthis result, we focused on two problems: localizing objects\\nwith a deep network and training a high-capacity model\\nwith only a small quantity of annotated detection data.\\nUnlike image classiﬁcation, detection requires localiz-\\ning (likely many) objects within an image. One approach\\nframes localization as a regression problem. However, work\\nfrom Szegedy et al. [38], concurrent with our own, indi-\\ncates that this strategy may not fare well in practice (they\\nreport a mAP of 30.5% on VOC 2007 compared to the\\n58.5% achieved by our method). An alternative is to build a\\nsliding-window detector. CNNs have been used in this way\\nfor at least two decades, typically on constrained object cat-\\negories, such as faces [32, 40] and pedestrians [35]. In order\\nto maintain high spatial resolution, these CNNs typically\\nonly have two convolutional and pooling layers. We also\\nconsidered adopting a sliding-window approach. However,\\nunits high up in our network, which has ﬁve convolutional\\nlayers, have very large receptive ﬁelds ( 195 ×195 pixels)\\nand strides (32×32 pixels) in the input image, which makes\\nprecise localization within the sliding-window paradigm an\\nopen technical challenge.\\nInstead, we solve the CNN localization problem by oper-\\nating within the “recognition using regions” paradigm [21],\\nwhich has been successful for both object detection [39] and\\nsemantic segmentation [5]. At test time, our method gener-\\nates around 2000 category-independent region proposals for\\nthe input image, extracts a ﬁxed-length feature vector from\\neach proposal using a CNN, and then classiﬁes each region\\nwith category-speciﬁc linear SVMs. We use a simple tech-\\nnique (afﬁne image warping) to compute a ﬁxed-size CNN\\ninput from each region proposal, regardless of the region’s\\nshape. Figure 1 presents an overview of our method and\\nhighlights some of our results. Since our system combines\\nregion proposals with CNNs, we dub the method R-CNN:\\nRegions with CNN features.\\nIn this updated version of this paper, we provide a head-\\nto-head comparison of R-CNN and the recently proposed\\nOverFeat [34] detection system by running R-CNN on the\\n200-class ILSVRC2013 detection dataset. OverFeat uses a\\nsliding-window CNN for detection and until now was the\\nbest performing method on ILSVRC2013 detection. We\\nshow that R-CNN signiﬁcantly outperforms OverFeat, with\\na mAP of 31.4% versus 24.3%.\\nA second challenge faced in detection is that labeled data\\nis scarce and the amount currently available is insufﬁcient\\nfor training a large CNN. The conventional solution to this\\nproblem is to useunsupervised pre-training, followed by su-\\npervised ﬁne-tuning (e.g., [35]). The second principle con-\\ntribution of this paper is to show thatsupervised pre-training\\non a large auxiliary dataset (ILSVRC), followed by domain-\\nspeciﬁc ﬁne-tuning on a small dataset (PASCAL), is an\\neffective paradigm for learning high-capacity CNNs when\\ndata is scarce. In our experiments, ﬁne-tuning for detection\\nimproves mAP performance by 8 percentage points. After\\nﬁne-tuning, our system achieves a mAP of 54% on VOC\\n2010 compared to 33% for the highly-tuned, HOG-based\\ndeformable part model (DPM) [17, 20]. We also point read-\\ners to contemporaneous work by Donahue et al. [12], who\\nshow that Krizhevsky’s CNN can be used (without ﬁne-\\ntuning) as a blackbox feature extractor, yielding excellent\\nperformance on several recognition tasks including scene\\nclassiﬁcation, ﬁne-grained sub-categorization, and domain\\nadaptation.\\nOur system is also quite efﬁcient. The only class-speciﬁc\\ncomputations are a reasonably small matrix-vector product\\nand greedy non-maximum suppression. This computational\\nproperty follows from features that are shared across all cat-\\negories and that are also two orders of magnitude lower-\\ndimensional than previously used region features (cf. [39]).\\nUnderstanding the failure modes of our approach is also\\ncritical for improving it, and so we report results from the\\ndetection analysis tool of Hoiem et al. [23]. As an im-\\nmediate consequence of this analysis, we demonstrate that\\na simple bounding-box regression method signiﬁcantly re-\\nduces mislocalizations, which are the dominant error mode.\\nBefore developing technical details, we note that because\\nR-CNN operates on regions it is natural to extend it to the\\ntask of semantic segmentation. With minor modiﬁcations,\\nwe also achieve competitive results on the PASCAL VOC\\nsegmentation task, with an average segmentation accuracy\\nof 47.9% on the VOC 2011 test set.\\n2. Object detection with R-CNN\\nOur object detection system consists of three modules.\\nThe ﬁrst generates category-independent region proposals.\\nThese proposals deﬁne the set of candidate detections avail-\\nable to our detector. The second module is a large convo-\\nlutional neural network that extracts a ﬁxed-length feature\\nvector from each region. The third module is a set of class-\\nspeciﬁc linear SVMs. In this section, we present our design\\ndecisions for each module, describe their test-time usage,\\ndetail how their parameters are learned, and show detection\\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\\n2.1. Module design\\nRegion proposals. A variety of recent papers offer meth-\\nods for generating category-independent region proposals.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='aeroplane bicycle bird car\\nFigure 2: Warped training samples from VOC 2007 train.\\nExamples include: objectness [1], selective search [39],\\ncategory-independent object proposals [14], constrained\\nparametric min-cuts (CPMC) [5], multi-scale combinatorial\\ngrouping [3], and Cires ¸an et al. [6], who detect mitotic cells\\nby applying a CNN to regularly-spaced square crops, which\\nare a special case of region proposals. While R-CNN is ag-\\nnostic to the particular region proposal method, we use se-\\nlective search to enable a controlled comparison with prior\\ndetection work (e.g., [39, 41]).\\nFeature extraction. We extract a 4096-dimensional fea-\\nture vector from each region proposal using the Caffe [24]\\nimplementation of the CNN described by Krizhevsky et\\nal. [25]. Features are computed by forward propagating\\na mean-subtracted 227 ×227 RGB image through ﬁve con-\\nvolutional layers and two fully connected layers. We refer\\nreaders to [24, 25] for more network architecture details.\\nIn order to compute features for a region proposal, we\\nmust ﬁrst convert the image data in that region into a form\\nthat is compatible with the CNN (its architecture requires\\ninputs of a ﬁxed 227 ×227 pixel size). Of the many possi-\\nble transformations of our arbitrary-shaped regions, we opt\\nfor the simplest. Regardless of the size or aspect ratio of the\\ncandidate region, we warp all pixels in a tight bounding box\\naround it to the required size. Prior to warping, we dilate the\\ntight bounding box so that at the warped size there are ex-\\nactly ppixels of warped image context around the original\\nbox (we use p = 16). Figure 2 shows a random sampling\\nof warped training regions. Alternatives to warping are dis-\\ncussed in Appendix A.\\n2.2. Test-time detection\\nAt test time, we run selective search on the test image\\nto extract around 2000 region proposals (we use selective\\nsearch’s “fast mode” in all experiments). We warp each\\nproposal and forward propagate it through the CNN in or-\\nder to compute features. Then, for each class, we score\\neach extracted feature vector using the SVM trained for that\\nclass. Given all scored regions in an image, we apply a\\ngreedy non-maximum suppression (for each class indepen-\\ndently) that rejects a region if it has an intersection-over-\\nunion (IoU) overlap with a higher scoring selected region\\nlarger than a learned threshold.\\nRun-time analysis. Two properties make detection efﬁ-\\ncient. First, all CNN parameters are shared across all cate-\\ngories. Second, the feature vectors computed by the CNN\\nare low-dimensional when compared to other common ap-\\nproaches, such as spatial pyramids with bag-of-visual-word\\nencodings. The features used in the UV A detection system\\n[39], for example, are two orders of magnitude larger than\\nours (360k vs. 4k-dimensional).\\nThe result of such sharing is that the time spent com-\\nputing region proposals and features (13s/image on a GPU\\nor 53s/image on a CPU) is amortized over all classes. The\\nonly class-speciﬁc computations are dot products between\\nfeatures and SVM weights and non-maximum suppression.\\nIn practice, all dot products for an image are batched into\\na single matrix-matrix product. The feature matrix is typi-\\ncally 2000×4096 and the SVM weight matrix is4096×N,\\nwhere N is the number of classes.\\nThis analysis shows that R-CNN can scale to thousands\\nof object classes without resorting to approximate tech-\\nniques, such as hashing. Even if there were 100k classes,\\nthe resulting matrix multiplication takes only 10 seconds on\\na modern multi-core CPU. This efﬁciency is not merely the\\nresult of using region proposals and shared features. The\\nUV A system, due to its high-dimensional features, would\\nbe two orders of magnitude slower while requiring 134GB\\nof memory just to store 100k linear predictors, compared to\\njust 1.5GB for our lower-dimensional features.\\nIt is also interesting to contrast R-CNN with the recent\\nwork from Dean et al. on scalable detection using DPMs\\nand hashing [8]. They report a mAP of around 16% on VOC\\n2007 at a run-time of 5 minutes per image when introducing\\n10k distractor classes. With our approach, 10k detectors can\\nrun in about a minute on a CPU, and because no approxi-\\nmations are made mAP would remain at 59% (Section 3.2).\\n2.3. Training\\nSupervised pre-training. We discriminatively pre-trained\\nthe CNN on a large auxiliary dataset (ILSVRC2012 clas-\\nsiﬁcation) using image-level annotations only (bounding-\\nbox labels are not available for this data). Pre-training\\nwas performed using the open source Caffe CNN library\\n[24]. In brief, our CNN nearly matches the performance\\nof Krizhevsky et al. [25], obtaining a top-1 error rate 2.2\\npercentage points higher on the ILSVRC2012 classiﬁcation\\nvalidation set. This discrepancy is due to simpliﬁcations in\\nthe training process.\\nDomain-speciﬁc ﬁne-tuning. To adapt our CNN to the\\nnew task (detection) and the new domain (warped proposal\\nwindows), we continue stochastic gradient descent (SGD)\\ntraining of the CNN parameters using only warped region\\nproposals. Aside from replacing the CNN’s ImageNet-\\nspeciﬁc 1000-way classiﬁcation layer with a randomly ini-\\ntialized (N + 1)-way classiﬁcation layer (where N is the\\nnumber of object classes, plus 1 for background), the CNN\\narchitecture is unchanged. For VOC, N = 20 and for\\nILSVRC2013, N = 200. We treat all region proposals with\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='≥0.5 IoU overlap with a ground-truth box as positives for\\nthat box’s class and the rest as negatives. We start SGD at\\na learning rate of 0.001 (1/10th of the initial pre-training\\nrate), which allows ﬁne-tuning to make progress while not\\nclobbering the initialization. In each SGD iteration, we uni-\\nformly sample 32 positive windows (over all classes) and\\n96 background windows to construct a mini-batch of size\\n128. We bias the sampling towards positive windows be-\\ncause they are extremely rare compared to background.\\nObject category classiﬁers. Consider training a binary\\nclassiﬁer to detect cars. It’s clear that an image region\\ntightly enclosing a car should be a positive example. Simi-\\nlarly, it’s clear that a background region, which has nothing\\nto do with cars, should be a negative example. Less clear\\nis how to label a region that partially overlaps a car. We re-\\nsolve this issue with an IoU overlap threshold, below which\\nregions are deﬁned as negatives. The overlap threshold,0.3,\\nwas selected by a grid search over {0,0.1,..., 0.5}on a\\nvalidation set. We found that selecting this threshold care-\\nfully is important. Setting it to 0.5, as in [39], decreased\\nmAP by 5 points. Similarly, setting it to 0 decreased mAP\\nby 4 points. Positive examples are deﬁned simply to be the\\nground-truth bounding boxes for each class.\\nOnce features are extracted and training labels are ap-\\nplied, we optimize one linear SVM per class. Since the\\ntraining data is too large to ﬁt in memory, we adopt the\\nstandard hard negative mining method [17, 37]. Hard neg-\\native mining converges quickly and in practice mAP stops\\nincreasing after only a single pass over all images.\\nIn Appendix B we discuss why the positive and negative\\nexamples are deﬁned differently in ﬁne-tuning versus SVM\\ntraining. We also discuss the trade-offs involved in training\\ndetection SVMs rather than simply using the outputs from\\nthe ﬁnal softmax layer of the ﬁne-tuned CNN.\\n2.4. Results on PASCAL VOC 2010-12\\nFollowing the PASCAL VOC best practices [15], we\\nvalidated all design decisions and hyperparameters on the\\nVOC 2007 dataset (Section 3.2). For ﬁnal results on the\\nVOC 2010-12 datasets, we ﬁne-tuned the CNN on VOC\\n2012 train and optimized our detection SVMs on VOC 2012\\ntrainval. We submitted test results to the evaluation server\\nonly once for each of the two major algorithm variants (with\\nand without bounding-box regression).\\nTable 1 shows complete results on VOC 2010. We com-\\npare our method against four strong baselines, including\\nSegDPM [18], which combines DPM detectors with the\\noutput of a semantic segmentation system [4] and uses ad-\\nditional inter-detector context and image-classiﬁer rescor-\\ning. The most germane comparison is to the UV A system\\nfrom Uijlings et al. [39], since our systems use the same re-\\ngion proposal algorithm. To classify regions, their method\\nbuilds a four-level spatial pyramid and populates it with\\ndensely sampled SIFT, Extended OpponentSIFT, and RGB-\\nSIFT descriptors, each vector quantized with 4000-word\\ncodebooks. Classiﬁcation is performed with a histogram\\nintersection kernel SVM. Compared to their multi-feature,\\nnon-linear kernel SVM approach, we achieve a large im-\\nprovement in mAP, from 35.1% to 53.7% mAP, while also\\nbeing much faster (Section 2.2). Our method achieves sim-\\nilar performance (53.3% mAP) on VOC 2011/12 test.\\n2.5. Results on ILSVRC2013 detection\\nWe ran R-CNN on the 200-class ILSVRC2013 detection\\ndataset using the same system hyperparameters that we used\\nfor PASCAL VOC. We followed the same protocol of sub-\\nmitting test results to the ILSVRC2013 evaluation server\\nonly twice, once with and once without bounding-box re-\\ngression.\\nFigure 3 compares R-CNN to the entries in the ILSVRC\\n2013 competition and to the post-competition OverFeat re-\\nsult [34]. R-CNN achieves a mAP of 31.4%, which is sig-\\nniﬁcantly ahead of the second-best result of 24.3% from\\nOverFeat. To give a sense of the AP distribution over\\nclasses, box plots are also presented and a table of per-\\nclass APs follows at the end of the paper in Table 8. Most\\nof the competing submissions (OverFeat, NEC-MU, UvA-\\nEuvision, Toronto A, and UIUC-IFP) used convolutional\\nneural networks, indicating that there is signiﬁcant nuance\\nin how CNNs can be applied to object detection, leading to\\ngreatly varying outcomes.\\nIn Section 4, we give an overview of the ILSVRC2013\\ndetection dataset and provide details about choices that we\\nmade when running R-CNN on it.\\n3. Visualization, ablation, and modes of error\\n3.1. Visualizing learned features\\nFirst-layer ﬁlters can be visualized directly and are easy\\nto understand [25]. They capture oriented edges and oppo-\\nnent colors. Understanding the subsequent layers is more\\nchallenging. Zeiler and Fergus present a visually attrac-\\ntive deconvolutional approach in [42]. We propose a simple\\n(and complementary) non-parametric method that directly\\nshows what the network learned.\\nThe idea is to single out a particular unit (feature) in the\\nnetwork and use it as if it were an object detector in its own\\nright. That is, we compute the unit’s activations on a large\\nset of held-out region proposals (about 10 million), sort the\\nproposals from highest to lowest activation, perform non-\\nmaximum suppression, and then display the top-scoring re-\\ngions. Our method lets the selected unit “speak for itself”\\nby showing exactly which inputs it ﬁres on. We avoid aver-\\naging in order to see different visual modes and gain insight\\ninto the invariances computed by the unit.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='VOC 2010 testaero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nDPM v5 [20]† 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4\\nUV A [39] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1\\nRegionlets [41]65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7\\nSegDPM [18]† 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4\\nR-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2\\nR-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7\\nTable 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UV A and Regionlets since all\\nmethods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM\\nwas the top-performer on the PASCAL VOC leaderboard. †DPM and SegDPM use context rescoring not used by the other methods.\\n0 20 40 60 80 100\\nUIUC−IFP \\nDelta \\nGPU_UCLA \\nSYSU_Vision \\nToronto A \\n*OverFeat (1) \\n*NEC−MU \\nUvA−Euvision \\n*OverFeat (2) \\n*R−CNN BB \\nmean average precision (mAP) in %\\nILSVRC2013 detection test set mAP\\n \\n \\n1.0%\\n6.1%\\n9.8%\\n10.5%\\n11.5%\\n19.4%\\n20.9%\\n22.6%\\n24.3%\\n31.4%\\ncompetition result\\npost competition result\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n*R−CNN BB\\nUvA−Euvision\\n*NEC−MU\\n*OverFeat (1)\\nToronto A\\nSYSU_Vision\\nGPU_UCLA\\nDelta\\nUIUC−IFP\\naverage precision (AP) in %\\nILSVRC2013 detection test set class AP box plots\\nFigure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data\\n(images and labels from the ILSVRC classiﬁcation dataset in all cases). (Right) Box plots for the 200 average precision values per\\nmethod. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for\\nR-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; seeR-CNN-ILSVRC2013-APs.txt). The red\\nline marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each\\nmethod. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).\\n1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9\\n1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\nFigure 4: Top regions for six pool5 units. Receptive ﬁelds and activation values are drawn in white. Some units are aligned to concepts,\\nsuch as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reﬂections (6).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='VOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN pool5 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2\\nR-CNN fc6 59.3 61.8 43.1 34.0 25.1 53.1 60.6 52.8 21.7 47.8 42.7 47.8 52.5 58.5 44.6 25.6 48.3 34.0 53.1 58.0 46.2\\nR-CNN fc7 57.6 57.9 38.5 31.8 23.7 51.2 58.9 51.4 20.0 50.5 40.9 46.0 51.6 55.9 43.3 23.3 48.1 35.3 51.0 57.4 44.7\\nR-CNN FT pool5 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3\\nR-CNN FT fc6 63.5 66.0 47.9 37.7 29.9 62.5 70.2 60.2 32.0 57.9 47.0 53.5 60.1 64.2 52.2 31.3 55.0 50.0 57.7 63.0 53.1\\nR-CNN FT fc7 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nDPM v5 [20] 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7\\nDPM ST [28] 23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1\\nDPM HSC [31] 32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3\\nTable 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without ﬁne-tuning. Rows 4-6 show\\nresults for the CNN pre-trained on ILSVRC 2012 and then ﬁne-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box\\nregression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The ﬁrst uses\\nonly HOG, while the next two use different feature learning approaches to augment or replace HOG.\\nVOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN T-Net 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN T-Net BB68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nR-CNN O-Net 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2\\nR-CNN O-Net BB73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nTable 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The ﬁrst two rows are results from\\nTable 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\\nand Zisserman (O-Net) [43].\\nWe visualize units from layer pool 5, which is the max-\\npooled output of the network’s ﬁfth and ﬁnal convolutional\\nlayer. The pool 5 feature map is 6 ×6 ×256 = 9216-\\ndimensional. Ignoring boundary effects, each pool5 unit has\\na receptive ﬁeld of195×195 pixels in the original227×227\\npixel input. A central pool 5 unit has a nearly global view,\\nwhile one near the edge has a smaller, clipped support.\\nEach row in Figure 4 displays the top 16 activations for\\na pool5 unit from a CNN that we ﬁne-tuned on VOC 2007\\ntrainval. Six of the 256 functionally unique units are visu-\\nalized (Appendix D includes more). These units were se-\\nlected to show a representative sample of what the network\\nlearns. In the second row, we see a unit that ﬁres on dog\\nfaces and dot arrays. The unit corresponding to the third row\\nis a red blob detector. There are also detectors for human\\nfaces and more abstract patterns such as text and triangular\\nstructures with windows. The network appears to learn a\\nrepresentation that combines a small number of class-tuned\\nfeatures together with a distributed representation of shape,\\ntexture, color, and material properties. The subsequent fully\\nconnected layer fc 6 has the ability to model a large set of\\ncompositions of these rich features.\\n3.2. Ablation studies\\nPerformance layer-by-layer, without ﬁne-tuning. To un-\\nderstand which layers are critical for detection performance,\\nwe analyzed results on the VOC 2007 dataset for each of the\\nCNN’s last three layers. Layer pool5 was brieﬂy described\\nin Section 3.1. The ﬁnal two layers are summarized below.\\nLayer fc6 is fully connected to pool 5. To compute fea-\\ntures, it multiplies a4096×9216 weight matrix by the pool5\\nfeature map (reshaped as a 9216-dimensional vector) and\\nthen adds a vector of biases. This intermediate vector is\\ncomponent-wise half-wave rectiﬁed (x←max(0,x)).\\nLayer fc7 is the ﬁnal layer of the network. It is imple-\\nmented by multiplying the features computed by fc 6 by a\\n4096 ×4096 weight matrix, and similarly adding a vector\\nof biases and applying half-wave rectiﬁcation.\\nWe start by looking at results from the CNN without\\nﬁne-tuning on PASCAL, i.e. all CNN parameters were\\npre-trained on ILSVRC 2012 only. Analyzing performance\\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\\nfc7 generalize worse than features from fc 6. This means\\nthat 29%, or about 16.8 million, of the CNN’s parameters\\ncan be removed without degrading mAP. More surprising is\\nthat removing both fc7 and fc6 produces quite good results\\neven though pool5 features are computed using only 6% of\\nthe CNN’s parameters. Much of the CNN’s representational\\npower comes from its convolutional layers, rather than from\\nthe much larger densely connected layers. This ﬁnding sug-\\ngests potential utility in computing a dense feature map, in\\nthe sense of HOG, of an arbitrary-sized image by using only\\nthe convolutional layers of the CNN. This representation\\nwould enable experimentation with sliding-window detec-\\ntors, including DPM, on top of pool5 features.\\nPerformance layer-by-layer, with ﬁne-tuning. We now\\nlook at results from our CNN after having ﬁne-tuned its pa-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='rameters on VOC 2007 trainval. The improvement is strik-\\ning (Table 2 rows 4-6): ﬁne-tuning increases mAP by 8.0\\npercentage points to 54.2%. The boost from ﬁne-tuning is\\nmuch larger for fc6 and fc7 than for pool5, which suggests\\nthat the pool 5 features learned from ImageNet are general\\nand that most of the improvement is gained from learning\\ndomain-speciﬁc non-linear classiﬁers on top of them.\\nComparison to recent feature learning methods. Rela-\\ntively few feature learning methods have been tried on PAS-\\nCAL VOC detection. We look at two recent approaches that\\nbuild on deformable part models. For reference, we also in-\\nclude results for the standard HOG-based DPM [20].\\nThe ﬁrst DPM feature learning method, DPM ST [28],\\naugments HOG features with histograms of “sketch token”\\nprobabilities. Intuitively, a sketch token is a tight distri-\\nbution of contours passing through the center of an image\\npatch. Sketch token probabilities are computed at each pixel\\nby a random forest that was trained to classify35×35 pixel\\npatches into one of 150 sketch tokens or background.\\nThe second method, DPM HSC [31], replaces HOG with\\nhistograms of sparse codes (HSC). To compute an HSC,\\nsparse code activations are solved for at each pixel using\\na learned dictionary of 100 7 ×7 pixel (grayscale) atoms.\\nThe resulting activations are rectiﬁed in three ways (full and\\nboth half-waves), spatially pooled, unit ℓ2 normalized, and\\nthen power transformed (x←sign(x)|x|α).\\nAll R-CNN variants strongly outperform the three DPM\\nbaselines (Table 2 rows 8-10), including the two that use\\nfeature learning. Compared to the latest version of DPM,\\nwhich uses only HOG features, our mAP is more than 20\\npercentage points higher: 54.2% vs. 33.7%— a 61% rela-\\ntive improvement. The combination of HOG and sketch to-\\nkens yields 2.5 mAP points over HOG alone, while HSC\\nimproves over HOG by 4 mAP points (when compared\\ninternally to their private DPM baselines—both use non-\\npublic implementations of DPM that underperform the open\\nsource version [20]). These methods achieve mAPs of\\n29.1% and 34.3%, respectively.\\n3.3. Network architectures\\nMost results in this paper use the network architecture\\nfrom Krizhevsky et al. [25]. However, we have found that\\nthe choice of architecture has a large effect on R-CNN de-\\ntection performance. In Table 3 we show results on VOC\\n2007 test using the 16-layer deep network recently proposed\\nby Simonyan and Zisserman [43]. This network was one of\\nthe top performers in the recent ILSVRC 2014 classiﬁca-\\ntion challenge. The network has a homogeneous structure\\nconsisting of 13 layers of 3 ×3 convolution kernels, with\\nﬁve max pooling layers interspersed, and topped with three\\nfully-connected layers. We refer to this network as “O-Net”\\nfor OxfordNet and the baseline as “T-Net” for TorontoNet.\\nTo use O-Net in R-CNN, we downloaded the pub-\\nlicly available pre-trained network weights for the\\nVGG ILSVRC 16 layers model from the Caffe Model\\nZoo.1 We then ﬁne-tuned the network using the same pro-\\ntocol as we used for T-Net. The only difference was to use\\nsmaller minibatches (24 examples) as required in order to\\nﬁt within GPU memory. The results in Table 3 show that R-\\nCNN with O-Net substantially outperforms R-CNN with T-\\nNet, increasing mAP from 58.5% to 66.0%. However there\\nis a considerable drawback in terms of compute time, with\\nthe forward pass of O-Net taking roughly 7 times longer\\nthan T-Net.\\n3.4. Detection error analysis\\nWe applied the excellent detection analysis tool from\\nHoiem et al. [23] in order to reveal our method’s error\\nmodes, understand how ﬁne-tuning changes them, and to\\nsee how our error types compare with DPM. A full sum-\\nmary of the analysis tool is beyond the scope of this pa-\\nper and we encourage readers to consult [23] to understand\\nsome ﬁner details (such as “normalized AP”). Since the\\nanalysis is best absorbed in the context of the associated\\nplots, we present the discussion within the captions of Fig-\\nure 5 and Figure 6.\\n3.5. Bounding-box regression\\nBased on the error analysis, we implemented a sim-\\nple method to reduce localization errors. Inspired by the\\nbounding-box regression employed in DPM [17], we train a\\nlinear regression model to predict a new detection window\\ngiven the pool 5 features for a selective search region pro-\\nposal. Full details are given in Appendix C. Results in Ta-\\nble 1, Table 2, and Figure 5 show that this simple approach\\nﬁxes a large number of mislocalized detections, boosting\\nmAP by 3 to 4 points.\\n3.6. Qualitative results\\nQualitative detection results on ILSVRC2013 are pre-\\nsented in Figure 8 and Figure 9 at the end of the paper. Each\\nimage was sampled randomly from the val 2 set and all de-\\ntections from all detectors with a precision greater than 0.5\\nare shown. Note that these are not curated and give a re-\\nalistic impression of the detectors in action. More qualita-\\ntive results are presented in Figure 10 and Figure 11, but\\nthese have been curated. We selected each image because it\\ncontained interesting, surprising, or amusing results. Here,\\nalso, all detections at precision greater than 0.5 are shown.\\n4. The ILSVRC2013 detection dataset\\nIn Section 2 we presented results on the ILSVRC2013\\ndetection dataset. This dataset is less homogeneous than\\n1https://github.com/BVLC/caffe/wiki/Model-Zoo\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='occ trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.212\\n0.612\\n0.420\\n0.557\\n0.201\\n0.720\\n0.344\\n0.606\\n0.351\\n0.677\\n0.244\\n0.609\\n0.516\\nnormalized AP\\nR−CNN fc6: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.179\\n0.701\\n0.498\\n0.634\\n0.335\\n0.766\\n0.442\\n0.672\\n0.429\\n0.723\\n0.325\\n0.685\\n0.593\\nnormalized AP\\nR−CNN FT fc7: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.211\\n0.731\\n0.542\\n0.676\\n0.385\\n0.786\\n0.484\\n0.709\\n0.453\\n0.779\\n0.368\\n0.720\\n0.633\\nnormalized AP\\nR−CNN FT fc7 BB: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.132\\n0.339\\n0.216\\n0.347\\n0.056\\n0.487\\n0.126\\n0.453\\n0.137\\n0.391\\n0.094\\n0.388\\n0.297\\nnormalized AP\\nDPM voc−release5: sensitivity and impact\\nFigure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and\\nlowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part\\nvisibility). We show plots for our method (R-CNN) with and without ﬁne-tuning (FT) and bounding-box regression (BB) as well as for\\nDPM voc-release5. Overall, ﬁne-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve\\nboth the highest and lowest performing subsets for nearly all characteristics. This indicates that ﬁne-tuning does more than simply improve\\nthe lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.\\nInstead, ﬁne-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\nFigure 5: Distribution of top-ranked false positive (FP) types.\\nEach plot shows the evolving distribution of FP types as more FPs\\nare considered in order of decreasing score. Each FP is catego-\\nrized into 1 of 4 types: Loc—poor localization (a detection with\\nan IoU overlap with the correct class between 0.1 and 0.5, or a du-\\nplicate); Sim—confusion with a similar category; Oth—confusion\\nwith a dissimilar object category; BG—a FP that ﬁred on back-\\nground. Compared with DPM (see [23]), signiﬁcantly more of\\nour errors result from poor localization, rather than confusion with\\nbackground or other object classes, indicating that the CNN fea-\\ntures are much more discriminative than HOG. Loose localiza-\\ntion likely results from our use of bottom-up region proposals and\\nthe positional invariance learned from pre-training the CNN for\\nwhole-image classiﬁcation. Column three shows how our simple\\nbounding-box regression method ﬁxes many localization errors.\\nPASCAL VOC, requiring choices about how to use it. Since\\nthese decisions are non-trivial, we cover them in this sec-\\ntion.\\n4.1. Dataset overview\\nThe ILSVRC2013 detection dataset is split into three\\nsets: train (395,918), val (20,121), and test (40,152), where\\nthe number of images in each set is in parentheses. The\\nval and test splits are drawn from the same image distribu-\\ntion. These images are scene-like and similar in complexity\\n(number of objects, amount of clutter, pose variability, etc.)\\nto PASCAL VOC images. The val and test splits are exhaus-\\ntively annotated, meaning that in each image all instances\\nfrom all 200 classes are labeled with bounding boxes. The\\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\\nsiﬁcation image distribution. These images have more vari-\\nable complexity with a skew towards images of a single cen-\\ntered object. Unlike val and test, the train images (due to\\ntheir large number) are not exhaustively annotated. In any\\ngiven train image, instances from the 200 classes may or\\nmay not be labeled. In addition to these image sets, each\\nclass has an extra set of negative images. Negative images\\nare manually checked to validate that they do not contain\\nany instances of their associated class. The negative im-\\nage sets were not used in this work. More information on\\nhow ILSVRC was collected and annotated can be found in\\n[11, 36].\\nThe nature of these splits presents a number of choices\\nfor training R-CNN. The train images cannot be used for\\nhard negative mining, because annotations are not exhaus-\\ntive. Where should negative examples come from? Also,\\nthe train images have different statistics than val and test.\\nShould the train images be used at all, and if so, to what\\nextent? While we have not thoroughly evaluated a large\\nnumber of choices, we present what seemed like the most\\nobvious path based on previous experience.\\nOur general strategy is to rely heavily on the val set and\\nuse some of the train images as an auxiliary source of pos-\\nitive examples. To use val for both training and valida-\\ntion, we split it into roughly equally sized “val1” and “val2”\\nsets. Since some classes have very few examples in val (the\\nsmallest has only 31 and half have fewer than 110), it is\\nimportant to produce an approximately class-balanced par-\\ntition. To do this, a large number of candidate splits were\\ngenerated and the one with the smallest maximum relative\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='class imbalance was selected. 2 Each candidate split was\\ngenerated by clustering val images using their class counts\\nas features, followed by a randomized local search that may\\nimprove the split balance. The particular split used here has\\na maximum relative imbalance of about 11% and a median\\nrelative imbalance of 4%. The val1/val2 split and code used\\nto produce them will be publicly available to allow other re-\\nsearchers to compare their methods on the val splits used in\\nthis report.\\n4.2. Region proposals\\nWe followed the same region proposal approach that was\\nused for detection on PASCAL. Selective search [39] was\\nrun in “fast mode” on each image in val1, val2, and test (but\\nnot on images in train). One minor modiﬁcation was re-\\nquired to deal with the fact that selective search is not scale\\ninvariant and so the number of regions produced depends\\non the image resolution. ILSVRC image sizes range from\\nvery small to a few that are several mega-pixels, and so we\\nresized each image to a ﬁxed width (500 pixels) before run-\\nning selective search. On val, selective search resulted in an\\naverage of 2403 region proposals per image with a 91.6%\\nrecall of all ground-truth bounding boxes (at 0.5 IoU thresh-\\nold). This recall is notably lower than in PASCAL, where\\nit is approximately 98%, indicating signiﬁcant room for im-\\nprovement in the region proposal stage.\\n4.3. Training data\\nFor training data, we formed a set of images and boxes\\nthat includes all selective search and ground-truth boxes\\nfrom val 1 together with up to N ground-truth boxes per\\nclass from train (if a class has fewer than N ground-truth\\nboxes in train, then we take all of them). We’ll call this\\ndataset of images and boxes val 1+trainN. In an ablation\\nstudy, we show mAP on val2 for N ∈{0,500,1000}(Sec-\\ntion 4.5).\\nTraining data is required for three procedures in R-CNN:\\n(1) CNN ﬁne-tuning, (2) detector SVM training, and (3)\\nbounding-box regressor training. CNN ﬁne-tuning was run\\nfor 50k SGD iteration on val1+trainN using the exact same\\nsettings as were used for PASCAL. Fine-tuning on a sin-\\ngle NVIDIA Tesla K20 took 13 hours using Caffe. For\\nSVM training, all ground-truth boxes from val 1+trainN\\nwere used as positive examples for their respective classes.\\nHard negative mining was performed on a randomly se-\\nlected subset of 5000 images from val 1. An initial experi-\\nment indicated that mining negatives from all of val1, versus\\na 5000 image subset (roughly half of it), resulted in only a\\n0.5 percentage point drop in mAP, while cutting SVM train-\\ning time in half. No negative examples were taken from\\n2Relative imbalance is measured as |a −b|/(a + b) where a and b are\\nclass counts in each half of the split.\\ntrain because the annotations are not exhaustive. The ex-\\ntra sets of veriﬁed negative images were not used. The\\nbounding-box regressors were trained on val1.\\n4.4. Validation and evaluation\\nBefore submitting results to the evaluation server, we\\nvalidated data usage choices and the effect of ﬁne-tuning\\nand bounding-box regression on the val2 set using the train-\\ning data described above. All system hyperparameters (e.g.,\\nSVM C hyperparameters, padding used in region warp-\\ning, NMS thresholds, bounding-box regression hyperpa-\\nrameters) were ﬁxed at the same values used for PAS-\\nCAL. Undoubtedly some of these hyperparameter choices\\nare slightly suboptimal for ILSVRC, however the goal of\\nthis work was to produce a preliminary R-CNN result on\\nILSVRC without extensive dataset tuning. After selecting\\nthe best choices on val 2, we submitted exactly two result\\nﬁles to the ILSVRC2013 evaluation server. The ﬁrst sub-\\nmission was without bounding-box regression and the sec-\\nond submission was with bounding-box regression. For\\nthese submissions, we expanded the SVM and bounding-\\nbox regressor training sets to use val +train1k and val, re-\\nspectively. We used the CNN that was ﬁne-tuned on\\nval1+train1k to avoid re-running ﬁne-tuning and feature\\ncomputation.\\n4.5. Ablation study\\nTable 4 shows an ablation study of the effects of differ-\\nent amounts of training data, ﬁne-tuning, and bounding-\\nbox regression. A ﬁrst observation is that mAP on val 2\\nmatches mAP on test very closely. This gives us conﬁ-\\ndence that mAP on val 2 is a good indicator of test set per-\\nformance. The ﬁrst result, 20.9%, is what R-CNN achieves\\nusing a CNN pre-trained on the ILSVRC2012 classiﬁca-\\ntion dataset (no ﬁne-tuning) and given access to the small\\namount of training data in val1 (recall that half of the classes\\nin val 1 have between 15 and 55 examples). Expanding\\nthe training set to val 1+trainN improves performance to\\n24.1%, with essentially no difference between N = 500\\nand N = 1000. Fine-tuning the CNN using examples from\\njust val1 gives a modest improvement to 26.5%, however\\nthere is likely signiﬁcant overﬁtting due to the small number\\nof positive training examples. Expanding the ﬁne-tuning\\nset to val1+train1k, which adds up to 1000 positive exam-\\nples per class from the train set, helps signiﬁcantly, boosting\\nmAP to 29.7%. Bounding-box regression improves results\\nto 31.0%, which is a smaller relative gain that what was ob-\\nserved in PASCAL.\\n4.6. Relationship to OverFeat\\nThere is an interesting relationship between R-CNN and\\nOverFeat: OverFeat can be seen (roughly) as a special case\\nof R-CNN. If one were to replace selective search region\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='test set val2 val2 val2 val2 val2 val2 test test\\nSVM training set val1 val1+train.5k val1+train1k val1+train1k val1+train1k val1+train1k val+train1k val+train1k\\nCNN ﬁne-tuning set n/a n/a n/a val 1 val1+train1k val1+train1k val1+train1k val1+train1k\\nbbox reg set n/a n/a n/a n/a n/a val 1 n/a val\\nCNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7\\nmAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4\\nmedian AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3\\nTable 4: ILSVRC2013 ablation study of data usage choices, ﬁne-tuning, and bounding-box regression.\\nproposals with a multi-scale pyramid of regular square re-\\ngions and change the per-class bounding-box regressors to\\na single bounding-box regressor, then the systems would\\nbe very similar (modulo some potentially signiﬁcant differ-\\nences in how they are trained: CNN detection ﬁne-tuning,\\nusing SVMs, etc.). It is worth noting that OverFeat has\\na signiﬁcant speed advantage over R-CNN: it is about 9x\\nfaster, based on a ﬁgure of 2 seconds per image quoted from\\n[34]. This speed comes from the fact that OverFeat’s slid-\\ning windows (i.e., region proposals) are not warped at the\\nimage level and therefore computation can be easily shared\\nbetween overlapping windows. Sharing is implemented by\\nrunning the entire network in a convolutional fashion over\\narbitrary-sized inputs. Speeding up R-CNN should be pos-\\nsible in a variety of ways and remains as future work.\\n5. Semantic segmentation\\nRegion classiﬁcation is a standard technique for seman-\\ntic segmentation, allowing us to easily apply R-CNN to the\\nPASCAL VOC segmentation challenge. To facilitate a di-\\nrect comparison with the current leading semantic segmen-\\ntation system (called O 2P for “second-order pooling”) [4],\\nwe work within their open source framework. O 2P uses\\nCPMC to generate 150 region proposals per image and then\\npredicts the quality of each region, for each class, using\\nsupport vector regression (SVR). The high performance of\\ntheir approach is due to the quality of the CPMC regions\\nand the powerful second-order pooling of multiple feature\\ntypes (enriched variants of SIFT and LBP). We also note\\nthat Farabet et al. [16] recently demonstrated good results\\non several dense scene labeling datasets (not including PAS-\\nCAL) using a CNN as a multi-scale per-pixel classiﬁer.\\nWe follow [2, 4] and extend the PASCAL segmentation\\ntraining set to include the extra annotations made available\\nby Hariharan et al. [22]. Design decisions and hyperparam-\\neters were cross-validated on the VOC 2011 validation set.\\nFinal test results were evaluated only once.\\nCNN features for segmentation. We evaluate three strate-\\ngies for computing features on CPMC regions, all of which\\nbegin by warping the rectangular window around the re-\\ngion to 227 ×227. The ﬁrst strategy ( full) ignores the re-\\ngion’s shape and computes CNN features directly on the\\nwarped window, exactly as we did for detection. However,\\nthese features ignore the non-rectangular shape of the re-\\ngion. Two regions might have very similar bounding boxes\\nwhile having very little overlap. Therefore, the second strat-\\negy (fg) computes CNN features only on a region’s fore-\\nground mask. We replace the background with the mean\\ninput so that background regions are zero after mean sub-\\ntraction. The third strategy ( full+fg) simply concatenates\\nthe full and fg features; our experiments validate their com-\\nplementarity.\\nfull R-CNN fg R-CNN full+fg R-CNN\\nO2P [4] fc6 fc7 fc6 fc7 fc6 fc7\\n46.4 43.0 42.5 43.7 42.1 47.9 45.8\\nTable 5: Segmentation mean accuracy (%) on VOC 2011 vali-\\ndation. Column 1 presents O2P; 2-7 use our CNN pre-trained on\\nILSVRC 2012.\\nResults on VOC 2011. Table 5 shows a summary of our\\nresults on the VOC 2011 validation set compared with O2P.\\n(See Appendix E for complete per-category results.) Within\\neach feature computation strategy, layer fc6 always outper-\\nforms fc7 and the following discussion refers to the fc6 fea-\\ntures. The fg strategy slightly outperforms full, indicating\\nthat the masked region shape provides a stronger signal,\\nmatching our intuition. However, full+fg achieves an aver-\\nage accuracy of 47.9%, our best result by a margin of 4.2%\\n(also modestly outperforming O2P), indicating that the con-\\ntext provided by the full features is highly informative even\\ngiven the fg features. Notably, training the 20 SVRs on our\\nfull+fg features takes an hour on a single core, compared to\\n10+ hours for training on O2P features.\\nIn Table 6 we present results on the VOC 2011 test\\nset, comparing our best-performing method, fc 6 (full+fg),\\nagainst two strong baselines. Our method achieves the high-\\nest segmentation accuracy for 11 out of 21 categories, and\\nthe highest overall segmentation accuracy of 47.9%, aver-\\naged across categories (but likely ties with the O 2P result\\nunder any reasonable margin of error). Still better perfor-\\nmance could likely be achieved by ﬁne-tuning.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='VOC 2011 test bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nR&P [2] 83.446.8 18.9 36.6 31.2 42.7 57.3 47.4 44.1 8.1 39.436.136.3 49.5 48.3 50.7 26.3 47.2 22.1 42.0 43.2 40.8\\nO2P [4] 85.469.722.3 45.244.4 46.9 66.7 57.8 56.213.5 46.132.3 41.259.1 55.3 51.0 36.2 50.4 27.846.944.6 47.6\\nours(full+fgR-CNN fc6) 84.266.923.7 58.337.4 55.4 73.3 58.7 56.59.7 45.5 29.549.3 40.1 57.8 53.9 33.8 60.7 22.747.141.3 47.9\\nTable 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the “Regions and Parts” (R&P)\\nmethod of [2] and the second-order pooling (O 2P) method of [4]. Without any ﬁne-tuning, our CNN achieves top segmentation perfor-\\nmance, outperforming R&P and roughly matching O2P.\\n6. Conclusion\\nIn recent years, object detection performance had stag-\\nnated. The best performing systems were complex en-\\nsembles combining multiple low-level image features with\\nhigh-level context from object detectors and scene classi-\\nﬁers. This paper presents a simple and scalable object de-\\ntection algorithm that gives a 30% relative improvement\\nover the best previous results on PASCAL VOC 2012.\\nWe achieved this performance through two insights. The\\nﬁrst is to apply high-capacity convolutional neural net-\\nworks to bottom-up region proposals in order to localize\\nand segment objects. The second is a paradigm for train-\\ning large CNNs when labeled training data is scarce. We\\nshow that it is highly effective to pre-train the network—\\nwith supervision—for a auxiliary task with abundant data\\n(image classiﬁcation) and then to ﬁne-tune the network for\\nthe target task where data is scarce (detection). We conjec-\\nture that the “supervised pre-training/domain-speciﬁc ﬁne-\\ntuning” paradigm will be highly effective for a variety of\\ndata-scarce vision problems.\\nWe conclude by noting that it is signiﬁcant that we\\nachieved these results by using a combination of classi-\\ncal tools from computer vision and deep learning (bottom-\\nup region proposals and convolutional neural networks).\\nRather than opposing lines of scientiﬁc inquiry, the two are\\nnatural and inevitable partners.\\nAcknowledgments. This research was supported in part\\nby DARPA Mind’s Eye and MSEE programs, by NSF\\nawards IIS-0905647, IIS-1134072, and IIS-1212798,\\nMURI N000014-10-1-0933, and by support from Toyota.\\nThe GPUs used in this research were generously donated\\nby the NVIDIA Corporation.\\nAppendix\\nA. Object proposal transformations\\nThe convolutional neural network used in this work re-\\nquires a ﬁxed-size input of 227 ×227 pixels. For detec-\\ntion, we consider object proposals that are arbitrary image\\nrectangles. We evaluated two approaches for transforming\\nobject proposals into valid CNN inputs.\\nThe ﬁrst method (“tightest square with context”) en-\\ncloses each object proposal inside the tightest square and\\n(A) (B) (C) (D)\\n (A) (B) (C) (D)\\nFigure 7: Different object proposal transformations. (A) the\\noriginal object proposal at its actual scale relative to the trans-\\nformed CNN inputs; (B) tightest square with context; (C) tight-\\nest square without context; (D) warp. Within each column and\\nexample proposal, the top row corresponds top = 0pixels of con-\\ntext padding while the bottom row has p = 16 pixels of context\\npadding.\\nthen scales (isotropically) the image contained in that\\nsquare to the CNN input size. Figure 7 column (B) shows\\nthis transformation. A variant on this method (“tightest\\nsquare without context”) excludes the image content that\\nsurrounds the original object proposal. Figure 7 column\\n(C) shows this transformation. The second method (“warp”)\\nanisotropically scales each object proposal to the CNN in-\\nput size. Figure 7 column (D) shows the warp transforma-\\ntion.\\nFor each of these transformations, we also consider in-\\ncluding additional image context around the original object\\nproposal. The amount of context padding (p) is deﬁned as a\\nborder size around the original object proposal in the trans-\\nformed input coordinate frame. Figure 7 shows p = 0pix-\\nels in the top row of each example and p = 16 pixels in\\nthe bottom row. In all methods, if the source rectangle ex-\\ntends beyond the image, the missing data is replaced with\\nthe image mean (which is then subtracted before inputing\\nthe image into the CNN). A pilot set of experiments showed\\nthat warping with context padding ( p = 16pixels) outper-\\nformed the alternatives by a large margin (3-5 mAP points).\\nObviously more alternatives are possible, including using\\nreplication instead of mean padding. Exhaustive evaluation\\nof these alternatives is left as future work.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='B. Positive vs. negative examples and softmax\\nTwo design choices warrant further discussion. The ﬁrst\\nis: Why are positive and negative examples deﬁned differ-\\nently for ﬁne-tuning the CNN versus training the object de-\\ntection SVMs? To review the deﬁnitions brieﬂy, for ﬁne-\\ntuning we map each object proposal to the ground-truth in-\\nstance with which it has maximum IoU overlap (if any) and\\nlabel it as a positive for the matched ground-truth class if the\\nIoU is at least 0.5. All other proposals are labeled “back-\\nground” (i.e., negative examples for all classes). For train-\\ning SVMs, in contrast, we take only the ground-truth boxes\\nas positive examples for their respective classes and label\\nproposals with less than 0.3 IoU overlap with all instances\\nof a class as a negative for that class. Proposals that fall\\ninto the grey zone (more than 0.3 IoU overlap, but are not\\nground truth) are ignored.\\nHistorically speaking, we arrived at these deﬁnitions be-\\ncause we started by training SVMs on features computed\\nby the ImageNet pre-trained CNN, and so ﬁne-tuning was\\nnot a consideration at that point in time. In that setup, we\\nfound that our particular label deﬁnition for training SVMs\\nwas optimal within the set of options we evaluated (which\\nincluded the setting we now use for ﬁne-tuning). When we\\nstarted using ﬁne-tuning, we initially used the same positive\\nand negative example deﬁnition as we were using for SVM\\ntraining. However, we found that results were much worse\\nthan those obtained using our current deﬁnition of positives\\nand negatives.\\nOur hypothesis is that this difference in how positives\\nand negatives are deﬁned is not fundamentally important\\nand arises from the fact that ﬁne-tuning data is limited.\\nOur current scheme introduces many “jittered” examples\\n(those proposals with overlap between 0.5 and 1, but not\\nground truth), which expands the number of positive exam-\\nples by approximately 30x. We conjecture that this large\\nset is needed when ﬁne-tuning the entire network to avoid\\noverﬁtting. However, we also note that using these jittered\\nexamples is likely suboptimal because the network is not\\nbeing ﬁne-tuned for precise localization.\\nThis leads to the second issue: Why, after ﬁne-tuning,\\ntrain SVMs at all? It would be cleaner to simply apply the\\nlast layer of the ﬁne-tuned network, which is a 21-way soft-\\nmax regression classiﬁer, as the object detector. We tried\\nthis and found that performance on VOC 2007 dropped\\nfrom 54.2% to 50.9% mAP. This performance drop likely\\narises from a combination of several factors including that\\nthe deﬁnition of positive examples used in ﬁne-tuning does\\nnot emphasize precise localization and the softmax classi-\\nﬁer was trained on randomly sampled negative examples\\nrather than on the subset of “hard negatives” used for SVM\\ntraining.\\nThis result shows that it’s possible to obtain close to\\nthe same level of performance without training SVMs af-\\nter ﬁne-tuning. We conjecture that with some additional\\ntweaks to ﬁne-tuning the remaining performance gap may\\nbe closed. If true, this would simplify and speed up R-CNN\\ntraining with no loss in detection performance.\\nC. Bounding-box regression\\nWe use a simple bounding-box regression stage to im-\\nprove localization performance. After scoring each selec-\\ntive search proposal with a class-speciﬁc detection SVM,\\nwe predict a new bounding box for the detection using a\\nclass-speciﬁc bounding-box regressor. This is similar in\\nspirit to the bounding-box regression used in deformable\\npart models [17]. The primary difference between the two\\napproaches is that here we regress from features computed\\nby the CNN, rather than from geometric features computed\\non the inferred DPM part locations.\\nThe input to our training algorithm is a set of N train-\\ning pairs {(Pi,Gi)}i=1,...,N, where Pi = (Pi\\nx,Pi\\ny,Pi\\nw,Pi\\nh)\\nspeciﬁes the pixel coordinates of the center of proposalPi’s\\nbounding box together with Pi’s width and height in pixels.\\nHence forth, we drop the superscript iunless it is needed.\\nEach ground-truth bounding box Gis speciﬁed in the same\\nway: G = (Gx,Gy,Gw,Gh). Our goal is to learn a trans-\\nformation that maps a proposed boxP to a ground-truth box\\nG.\\nWe parameterize the transformation in terms of four\\nfunctions dx(P), dy(P), dw(P), and dh(P). The ﬁrst\\ntwo specify a scale-invariant translation of the center of\\nP’s bounding box, while the second two specify log-space\\ntranslations of the width and height of P’s bounding box.\\nAfter learning these functions, we can transform an input\\nproposal P into a predicted ground-truth box ˆGby apply-\\ning the transformation\\nˆGx = Pwdx(P) +Px (1)\\nˆGy = Phdy(P) +Py (2)\\nˆGw = Pwexp(dw(P)) (3)\\nˆGh = Phexp(dh(P)). (4)\\nEach function d⋆(P) (where ⋆ is one of x,y,h,w ) is\\nmodeled as a linear function of the pool 5 features of pro-\\nposal P, denoted by φ5(P). (The dependence of φ5(P)\\non the image data is implicitly assumed.) Thus we have\\nd⋆(P) = wT\\n⋆φ5(P), where w⋆ is a vector of learnable\\nmodel parameters. We learn w⋆ by optimizing the regu-\\nlarized least squares objective (ridge regression):\\nw⋆ = argmin\\nˆw⋆\\nN∑\\ni\\n(ti\\n⋆ −ˆwT\\n⋆φ5(Pi))2 + λ∥ˆw⋆∥2 . (5)\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='The regression targets t⋆ for the training pair (P,G) are de-\\nﬁned as\\ntx = (Gx −Px)/Pw (6)\\nty = (Gy −Py)/Ph (7)\\ntw = log(Gw/Pw) (8)\\nth = log(Gh/Ph). (9)\\nAs a standard regularized least squares problem, this can be\\nsolved efﬁciently in closed form.\\nWe found two subtle issues while implementing\\nbounding-box regression. The ﬁrst is that regularization\\nis important: we set λ = 1000 based on a validation set.\\nThe second issue is that care must be taken when selecting\\nwhich training pairs (P,G) to use. Intuitively, if P is far\\nfrom all ground-truth boxes, then the task of transforming\\nP to a ground-truth box Gdoes not make sense. Using ex-\\namples like P would lead to a hopeless learning problem.\\nTherefore, we only learn from a proposal P if it is nearby\\nat least one ground-truth box. We implement “nearness” by\\nassigning P to the ground-truth box G with which it has\\nmaximum IoU overlap (in case it overlaps more than one) if\\nand only if the overlap is greater than a threshold (which we\\nset to 0.6 using a validation set). All unassigned proposals\\nare discarded. We do this once for each object class in order\\nto learn a set of class-speciﬁc bounding-box regressors.\\nAt test time, we score each proposal and predict its new\\ndetection window only once. In principle, we could iterate\\nthis procedure (i.e., re-score the newly predicted bounding\\nbox, and then predict a new bounding box from it, and so\\non). However, we found that iterating does not improve\\nresults.\\nD. Additional feature visualizations\\nFigure 12 shows additional visualizations for 20 pool 5\\nunits. For each unit, we show the 24 region proposals that\\nmaximally activate that unit out of the full set of approxi-\\nmately 10 million regions in all of VOC 2007 test.\\nWe label each unit by its (y, x, channel) position in the\\n6 ×6 ×256 dimensional pool5 feature map. Within each\\nchannel, the CNN computes exactly the same function of\\nthe input region, with the (y, x) position changing only the\\nreceptive ﬁeld.\\nE. Per-category segmentation results\\nIn Table 7 we show the per-category segmentation ac-\\ncuracy on VOC 2011 val for each of our six segmentation\\nmethods in addition to the O 2P method [4]. These results\\nshow which methods are strongest across each of the 20\\nPASCAL classes, plus the background class.\\nF. Analysis of cross-dataset redundancy\\nOne concern when training on an auxiliary dataset is that\\nthere might be redundancy between it and the test set. Even\\nthough the tasks of object detection and whole-image clas-\\nsiﬁcation are substantially different, making such cross-set\\nredundancy much less worrisome, we still conducted a thor-\\nough investigation that quantiﬁes the extent to which PAS-\\nCAL test images are contained within the ILSVRC 2012\\ntraining and validation sets. Our ﬁndings may be useful to\\nresearchers who are interested in using ILSVRC 2012 as\\ntraining data for the PASCAL image classiﬁcation task.\\nWe performed two checks for duplicate (and near-\\nduplicate) images. The ﬁrst test is based on exact matches\\nof ﬂickr image IDs, which are included in the VOC 2007\\ntest annotations (these IDs are intentionally kept secret for\\nsubsequent PASCAL test sets). All PASCAL images, and\\nabout half of ILSVRC, were collected from ﬂickr.com. This\\ncheck turned up 31 matches out of 4952 (0.63%).\\nThe second check uses GIST [30] descriptor matching,\\nwhich was shown in [13] to have excellent performance at\\nnear-duplicate image detection in large (>1 million) image\\ncollections. Following [13], we computed GIST descrip-\\ntors on warped 32 ×32 pixel versions of all ILSVRC 2012\\ntrainval and PASCAL 2007 test images.\\nEuclidean distance nearest-neighbor matching of GIST\\ndescriptors revealed 38 near-duplicate images (including all\\n31 found by ﬂickr ID matching). The matches tend to vary\\nslightly in JPEG compression level and resolution, and to a\\nlesser extent cropping. These ﬁndings show that the overlap\\nis small, less than 1%. For VOC 2012, because ﬂickr IDs\\nare not available, we used the GIST matching method only.\\nBased on GIST matches, 1.5% of VOC 2012 test images\\nare in ILSVRC 2012 trainval. The slightly higher rate for\\nVOC 2012 is likely due to the fact that the two datasets\\nwere collected closer together in time than VOC 2007 and\\nILSVRC 2012 were.\\nG. Document changelog\\nThis document tracks the progress of R-CNN. To help\\nreaders understand how it has changed over time, here’s a\\nbrief changelog describing the revisions.\\nv1 Initial version.\\nv2 CVPR 2014 camera-ready revision. Includes substan-\\ntial improvements in detection performance brought about\\nby (1) starting ﬁne-tuning from a higher learning rate (0.001\\ninstead of 0.0001), (2) using context padding when prepar-\\ning CNN inputs, and (3) bounding-box regression to ﬁx lo-\\ncalization errors.\\nv3 Results on the ILSVRC2013 detection dataset and com-\\nparison with OverFeat were integrated into several sections\\n(primarily Section 2 and Section 4).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='VOC 2011 val bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nO2P [4] 84.069.021.7 47.7 42.2 42.464.7 65.857.4 12.9 37.4 20.5 43.7 35.7 52.7 51.0 35.8 51.0 28.4 59.8 49.746.4\\nfullR-CNN fc6 81.356.2 23.9 42.9 40.7 38.8 59.2 56.5 53.2 11.4 34.6 16.7 48.1 37.0 51.4 46.0 31.5 44.0 24.3 53.7 51.143.0\\nfullR-CNN fc7 81.052.825.143.8 40.5 42.7 55.4 57.7 51.3 8.7 32.5 11.5 48.1 37.0 50.5 46.4 30.2 42.1 21.2 57.7 56.0 42.5\\nfgR-CNN fc6 81.454.1 21.1 40.6 38.753.6 59.9 57.2 52.5 9.1 36.5 23.6 46.4 38.1 53.2 51.3 32.2 38.7 29.053.0 47.543.7\\nfgR-CNN fc7 80.950.1 20.0 40.2 34.1 40.9 59.7 59.8 52.7 7.3 32.1 14.3 48.8 42.9 54.0 48.6 28.9 42.6 24.9 52.2 48.8 42.1\\nfull+fgR-CNN fc6 83.160.4 23.2 48.447.3 52.6 61.6 60.659.1 10.8 45.8 20.9 57.7 43.3 57.4 52.9 34.7 48.7 28.1 60.0 48.647.9\\nfull+fgR-CNN fc7 82.356.7 20.649.944.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.155.2 45.7\\nTable 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.\\nv4 The softmax vs. SVM results in Appendix B contained\\nan error, which has been ﬁxed. We thank Sergio Guadar-\\nrama for helping to identify this issue.\\nv5 Added results using the new 16-layer network architec-\\nture from Simonyan and Zisserman [43] to Section 3.3 and\\nTable 3.\\nReferences\\n[1] B. Alexe, T. Deselaers, and V . Ferrari. Measuring the object-\\nness of image windows. TPAMI, 2012. 2\\n[2] P. Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\\nJ. Malik. Semantic segmentation using regions and parts. In\\nCVPR, 2012. 10, 11\\n[3] P. Arbel ´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-\\nlik. Multiscale combinatorial grouping. In CVPR, 2014. 3\\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\\nmantic segmentation with second-order pooling. In ECCV,\\n2012. 4, 10, 11, 13, 14\\n[5] J. Carreira and C. Sminchisescu. CPMC: Automatic ob-\\nject segmentation using constrained parametric min-cuts.\\nTPAMI, 2012. 2, 3\\n[6] D. Cires ¸an, A. Giusti, L. Gambardella, and J. Schmidhu-\\nber. Mitosis detection in breast cancer histology images with\\ndeep neural networks. In MICCAI, 2013. 3\\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In CVPR, 2005. 1\\n[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, and J. Yagnik. Fast, accurate detection of\\n100,000 object classes on a single machine. In CVPR, 2013.\\n3\\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\\nFei. ImageNet Large Scale Visual Recognition Competition\\n2012 (ILSVRC2012). http://www.image-net.org/\\nchallenges/LSVRC/2012/. 1\\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. ImageNet: A large-scale hierarchical image database.\\nIn CVPR, 2009. 1\\n[11] J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. C.\\nBerg, and L. Fei-Fei. Scalable multi-label annotation. In\\nCHI, 2014. 8\\n[12] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. InICML,\\n2014. 2\\n[13] M. Douze, H. J ´egou, H. Sandhawalia, L. Amsaleg, and\\nC. Schmid. Evaluation of gist descriptors for web-scale im-\\nage search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\\nChallenge. IJCV, 2010. 1, 4\\n[16] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. TPAMI, 2013. 10\\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. TPAMI, 2010. 2, 4, 7, 12\\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up\\nsegmentation for top-down detection. In CVPR, 2013. 4, 5\\n[19] K. Fukushima. Neocognitron: A self-organizing neu-\\nral network model for a mechanism of pattern recogni-\\ntion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-\\nnatively trained deformable part models, release 5. http:\\n//www.cs.berkeley.edu/˜rbg/latent-v5/. 2,\\n5, 6, 7\\n[21] C. Gu, J. J. Lim, P. Arbel ´aez, and J. Malik. Recognition\\nusing regions. In CVPR, 2009. 2\\n[22] B. Hariharan, P. Arbel ´aez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In ICCV, 2011.\\n10\\n[23] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In ECCV. 2012. 2, 7, 8\\n[24] Y . Jia. Caffe: An open source convolutional archi-\\ntecture for fast feature embedding. http://caffe.\\nberkeleyvision.org/, 2013. 3\\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\\nsiﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 3, 4, 7\\n[26] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\\nW. Hubbard, and L. Jackel. Backpropagation applied to\\nhandwritten zip code recognition. Neural Comp., 1989. 1\\n[27] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. of the\\nIEEE, 1998. 1\\n[28] J. J. Lim, C. L. Zitnick, and P. Doll ´ar. Sketch tokens: A\\nlearned mid-level representation for contour and object de-\\ntection. In CVPR, 2013. 6, 7\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='class AP class AP class AP class AP class AP\\naccordion 50.8 centipede 30.4 hair spray 13.8 pencil box 11.4 snowplow 69.2\\nairplane 50.0 chain saw 14.1 hamburger 34.2 pencil sharpener 9.0 soap dispenser 16.8\\nant 31.8 chair 19.5 hammer 9.9 perfume 32.8 soccer ball 43.7\\nantelope 53.8 chime 24.6 hamster 46.0 person 41.7 sofa 16.3\\napple 30.9 cocktail shaker 46.2 harmonica 12.6 piano 20.5 spatula 6.8\\narmadillo 54.0 coffee maker 21.5 harp 50.4 pineapple 22.6 squirrel 31.3\\nartichoke 45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball 21.0 starﬁsh 45.1\\naxe 11.8 computer mouse 21.2 head cabbage 17.4 pitcher 19.2 stethoscope 18.3\\nbaby bed 42.0 corkscrew 24.2 helmet 33.4 pizza 43.7 stove 8.1\\nbackpack 2.8 cream 29.9 hippopotamus 38.0 plastic bag 6.4 strainer 9.9\\nbagel 37.5 croquet ball 30.0 horizontal bar 7.0 plate rack 15.2 strawberry 26.8\\nbalance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2\\nbanana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8\\nband aid 17.4 cup or mug 34.0 iPod 59.2 porcupine 37.2 swimming trunks 9.1\\nbanjo 55.3 diaper 10.1 isopod 19.5 power drill 7.9 swine 45.3\\nbaseball 41.8 digital clock 18.5 jellyﬁsh 23.7 pretzel 24.8 syringe 5.7\\nbasketball 65.3 dishwasher 19.9 koala bear 44.3 printer 21.3 table 21.7\\nbathing cap 37.2 dog 76.8 ladle 3.0 puck 14.1 tape player 21.4\\nbeaker 11.3 domestic cat 44.1 ladybug 58.4 punching bag 29.4 tennis ball 59.1\\nbear 62.7 dragonﬂy 27.8 lamp 9.1 purse 8.0 tick 42.6\\nbee 52.9 drum 19.9 laptop 35.4 rabbit 71.0 tie 24.6\\nbell pepper 38.8 dumbbell 14.1 lemon 33.3 racket 16.2 tiger 61.8\\nbench 12.7 electric fan 35.0 lion 51.3 ray 41.1 toaster 29.2\\nbicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 trafﬁc light 24.7\\nbinder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8\\nbird 70.9 ﬁg 44.5 lobster 32.4 remote control 41.6 trombone 13.8\\nbookshelf 19.3 ﬁling cabinet 20.6 maillot 31.0 rubber eraser 2.5 trumpet 14.4\\nbow tie 38.8 ﬂower pot 20.2 maraca 30.1 rugby ball 34.5 turtle 59.1\\nbow 9.0 ﬂute 4.9 microphone 4.0 ruler 11.5 tv or monitor 41.7\\nbowl 26.7 fox 59.3 microwave 40.1 salt or pepper shaker 24.6 unicycle 27.2\\nbrassiere 31.2 french horn 24.2 milk can 33.3 saxophone 40.8 vacuum 19.5\\nburrito 25.7 frog 64.1 miniskirt 14.9 scorpion 57.3 violin 13.7\\nbus 57.5 frying pan 21.5 monkey 49.6 screwdriver 10.6 volleyball 59.7\\nbutterﬂy 88.5 giant panda 42.5 motorcycle 42.2 seal 20.9 wafﬂe iron 24.0\\ncamel 37.6 goldﬁsh 28.6 mushroom 31.8 sheep 48.9 washer 39.8\\ncan opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1\\ncar 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9\\ncart 48.0 guacamole 32.3 oboe 27.5 snail 36.2 whale 48.6\\ncattle 32.3 guitar 33.1 orange 38.8 snake 33.8 wine bottle 31.2\\ncello 28.9 hair dryer 13.0 otter 22.2 snowmobile 58.8 zebra 49.6\\nTable 8: Per-class average precision (%) on the ILSVRC2013 detection test set.\\n[29] D. Lowe. Distinctive image features from scale-invariant\\nkeypoints. IJCV, 2004. 1\\n[30] A. Oliva and A. Torralba. Modeling the shape of the scene:\\nA holistic representation of the spatial envelope.IJCV, 2001.\\n13\\n[31] X. Ren and D. Ramanan. Histograms of sparse codes for\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='lemon 0.79\\nlemon 0.70\\nlemon 0.56lemon 0.50\\nperson 0.88\\nperson 0.72\\ncocktail shaker 0.56\\ndog 0.97dog 0.85 dog 0.57\\nbird 0.63\\ndog 0.97dog 0.95\\ndog 0.64\\nhelmet 0.65\\nhelmet 0.52\\nmotorcycle 0.65\\nperson 0.75\\nperson 0.58\\nsnowmobile 0.83\\nsnowmobile 0.83\\nbow tie 0.86\\nperson 0.82\\nbird 0.61\\ndog 0.66\\ndog 0.61\\ndomestic cat 0.57\\nbird 0.96\\ndog 0.91\\ndog 0.77\\nsofa 0.71\\ndog 0.95\\ndog 0.55\\nladybug 1.00\\nperson 0.87\\ncar 0.96 car 0.66car 0.63\\nbird 0.98\\nperson 0.65\\nwatercraft 1.00\\nwatercraft 0.69\\npretzel 0.78\\ncar 0.96\\nperson 0.65person 0.58person 0.52\\nperson 0.52\\nbird 0.99 bird 0.91\\nbird 0.75\\ndog 0.98\\nflower pot 0.62\\ndog 0.97dog 0.56\\ntrain 1.00\\ntrain 0.53\\narmadillo 1.00\\narmadillo 0.56\\nbird 0.93\\ndog 0.92\\nswine 0.88\\nbird 1.00\\nbutterfly 0.96\\nperson 0.90\\nflower pot 0.62\\nsnake 0.70\\nturtle 0.54\\nbell pepper 0.81\\nbell pepper 0.62\\nbell pepper 0.54\\nruler 1.00\\nantelope 0.53\\nmushroom 0.93\\ntv or monitor 0.82\\ntv or monitor 0.76tv or monitor 0.54\\nbird 0.89\\nlipstick 0.80\\nlipstick 0.61\\nperson 0.58\\ndog 0.97\\nsoccer ball 0.90\\nFigure 8: Example detections on the val2 set from the conﬁguration that achieved 31.0% mAP on val2. Each image was sampled randomly\\n(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the\\nprecision value of that detection from the detector’s precision-recall curve. Viewing digitally with zoom is recommended.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17'}, page_content='baby bed 0.55helmet 0.51\\npitcher 0.57\\ndog 0.98\\nhat with a wide brim 0.78\\nperson 0.86\\nbird 0.52table 0.60\\nmonkey 0.97\\ntable 0.68\\nwatercraft 0.55\\nperson 0.88\\ncar 0.61\\nperson 0.87\\nperson 0.51\\nsunglasses 0.51\\ndog 0.94dog 0.55\\nbird 0.52\\nmonkey 0.87\\nmonkey 0.81\\nswine 0.50\\ndog 0.97\\nhat with a wide brim 0.96\\nsnake 0.74\\ndog 0.93\\nperson 0.77\\ndog 0.97\\nguacamole 0.64\\npretzel 0.69\\ntable 0.54\\ndog 0.71\\nperson 0.85\\nladybug 0.90\\nperson 0.52\\nzebra 0.83 zebra 0.80\\nzebra 0.55\\nzebra 0.52\\ndog 0.98\\nhat with a wide brim 0.60person 0.85\\nperson 0.81 person 0.73\\nelephant 1.00\\nbird 0.99\\nperson 0.58\\ndog 0.98\\ncart 1.00\\nchair 0.79chair 0.64\\nperson 0.91person 0.87 person 0.57\\nperson 0.52\\ncomputer keyboard 0.52\\ndog 0.97 dog 0.92\\nperson 0.77\\nbird 0.94\\nbutterfly 0.98\\nperson 0.73\\nperson 0.61\\nbird 1.00\\nbird 0.78\\nperson 0.91 person 0.75\\nstethoscope 0.83\\nbird 0.83\\nFigure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='person 0.81\\nperson 0.57\\nperson 0.53\\nmotorcycle 0.64\\nperson 0.73\\nperson 0.51\\nbagel 0.57\\npineapple 1.00\\nbowl 0.63\\nguacamole 1.00tennis ball 0.60\\nlemon 0.88\\nlemon 0.86lemon 0.80\\nlemon 0.78\\norange 0.78\\norange 0.73\\norange 0.71\\ngolf ball 1.00\\ngolf ball 1.00\\ngolf ball 0.89\\ngolf ball 0.81\\ngolf ball 0.79\\ngolf ball 0.76golf ball 0.60\\ngolf ball 0.60\\ngolf ball 0.51\\nlemon 0.53\\nsoccer ball 0.67\\nlamp 0.61\\ntable 0.59\\nbee 0.85\\njellyfish 0.71\\nbowl 0.54\\nhamburger 0.78\\ndumbbell 1.00person 0.52\\nmicrophone 1.00\\nperson 0.85\\nhead cabbage 0.83\\nhead cabbage 0.75\\ndog 0.74\\ngoldfish 0.76\\nperson 0.57\\nguitar 1.00\\nguitar 1.00\\nguitar 0.88\\ntable 0.63\\ncomputer keyboard 0.78\\nmicrowave 0.60\\ntable 0.53\\ntick 0.64\\nlemon 0.80\\ntennis ball 0.67\\nrabbit 1.00\\ndog 0.98\\nperson 0.81\\nperson 0.92\\nsunglasses 0.52\\nwatercraft 0.86\\nmilk can 1.00\\nmilk can 1.00\\nbookshelf 0.50\\nchair 0.86\\ngiant panda 0.61\\nperson 0.87\\nantelope 0.74\\ncattle 0.81\\ndog 0.87\\nhorse 0.78\\npomegranate 1.00\\nchair 0.86\\ntv or monitor 0.52\\nantelope 0.68\\nbird 0.94\\nsnake 0.60\\ndog 0.98\\ndog 0.88\\nperson 0.79\\nsnake 0.76\\ntable 0.62\\ntv or monitor 0.80\\ntv or monitor 0.58\\ntv or monitor 0.54\\nlamp 0.86lamp 0.65\\ntable 0.83\\nmonkey 1.00monkey 1.00\\nmonkey 0.90\\nmonkey 0.88\\nmonkey 0.52\\ndog 0.88fox 1.00\\nfox 0.81\\nperson 0.88\\nwatercraft 0.91\\nwatercraft 0.56\\nbird 0.95\\nbird 0.78\\nisopod 0.56\\nbird 0.69\\nstarfish 0.67\\ndragonfly 0.70\\ndragonfly 0.60\\nhamburger 0.72\\nhamburger 0.60\\ncup or mug 0.72\\nelectric fan 1.00\\nelectric fan 0.83\\nelectric fan 0.78helmet 0.64\\nsoccer ball 0.63\\nFigure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing\\ndigitally with zoom is recommended.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='object detection. In CVPR, 2013. 6, 7\\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\\nbased face detection. TPAMI, 1998. 2\\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\\ning internal representations by error propagation. Parallel\\nDistributed Processing, 1:318–362, 1986. 1\\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\\ntion and Detection using Convolutional Networks. In ICLR,\\n2014. 1, 2, 4, 10\\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. In CVPR, 2013. 2\\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\\nfor visual object detection. In AAAI Technical Report, 4th\\nHuman Computation Workshop, 2012. 8\\n[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4\\n[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\\nfor object detection. In NIPS, 2013. 2\\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\\nSelective search for object recognition. IJCV, 2013. 1, 2, 3,\\n4, 5, 9\\n[40] R. Vaillant, C. Monrocq, and Y . LeCun. Original approach\\nfor the localisation of objects in images. IEE Proc on Vision,\\nImage, and Signal Processing, 1994. 2\\n[41] X. Wang, M. Yang, S. Zhu, and Y . Lin. Regionlets for generic\\nobject detection. In ICCV, 2013. 3, 5\\n[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-\\ntional networks for mid and high level feature learning. In\\nCVPR, 2011. 4\\n[43] K. Simonyan and A. Zisserman. Very Deep Convolu-\\ntional Networks for Large-Scale Image Recognition. arXiv\\npreprint, arXiv:1409.1556, 2014. 6, 7, 14\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='person 0.82\\nsnake 0.76\\nfrog 0.78\\nbird 0.79\\ngoldfish 0.76\\ngoldfish 0.76\\ngoldfish 0.58\\nperson 0.94\\nstethoscope 0.56\\nperson 0.95person 0.92person 0.67\\nperson 0.60\\ntable 0.81\\njellyfish 0.67\\nlemon 0.52\\nperson 0.78\\nperson 0.65\\nwatercraft 0.55\\nbaseball 1.00\\nperson 0.94\\nperson 0.82\\nperson 0.80\\nperson 0.61\\nperson 0.55\\nperson 0.52\\ncomputer keyboard 0.81\\ndog 0.60 person 0.88\\nperson 0.79\\nperson 0.68\\nperson 0.59\\ntv or monitor 0.82\\nlizard 0.58\\nchair 0.50\\nperson 0.74\\ntable 0.82\\nperson 0.94\\nperson 0.94\\nperson 0.95\\nperson 0.81person 0.69\\nrugby ball 0.91\\nperson 0.84 person 0.59\\nvolleyball 0.70\\npineapple 1.00\\nbrassiere 0.71\\nperson 0.95 person 0.94person 0.94\\nperson 0.81 person 0.80person 0.80\\nperson 0.79\\nperson 0.79\\nperson 0.69\\nperson 0.66\\nperson 0.58\\nperson 0.56person 0.54\\nswimming trunks 0.56\\nbaseball 0.86\\nhelmet 0.74\\nperson 0.75\\nminiskirt 0.64\\nperson 0.92\\nvacuum 1.00\\ndog 0.98\\ndog 0.93\\nperson 0.94 person 0.75\\nperson 0.65\\nperson 0.53\\nski 0.80 ski 0.80\\nbird 0.55\\ntiger 1.00\\ntiger 0.67\\ntiger 0.59\\nbird 0.56\\nwhale 1.00\\nchair 0.53\\nperson 0.92\\nperson 0.92\\nperson 0.82person 0.78\\nbowl 0.52\\nstrawberry 0.79strawberry 0.70\\nburrito 0.54\\ncroquet ball 0.91croquet ball 0.91croquet ball 0.91 croquet ball 0.91\\nmushroom 0.57\\nwatercraft 0.91\\nwatercraft 0.87\\nwatercraft 0.58\\nplastic bag 0.62\\nplastic bag 0.62\\nwhale 0.88\\ncar 0.70\\ndog 0.94\\ntv or monitor 0.57\\ncart 0.80\\nperson 0.79\\nperson 0.53\\nhat with a wide brim 0.89person 0.88\\nperson 0.82\\nperson 0.79\\nperson 0.56\\nperson 0.54\\ntraffic light 0.79\\nbird 0.59\\ncucumber 0.53\\ncucumber 0.52\\nantelope 1.00\\nantelope 1.00\\nantelope 0.94\\nantelope 0.73\\nantelope 0.63\\nantelope 0.63\\nfox 0.57\\nbalance beam 0.50horizontal bar 1.00\\nperson 0.80\\nperson 0.90\\nsnake 0.64\\ndog 0.98\\ndog 0.97\\nhelmet 0.69\\nhorse 0.92\\nhorse 0.69\\nperson 0.82\\nperson 0.72\\norange 0.79\\norange 0.71\\norange 0.66\\norange 0.66\\norange 0.59\\norange 0.56\\nbird 0.97\\nbird 0.96\\nbird 0.96\\nbird 0.94\\nbird 0.89\\nbird 0.64\\nbird 0.56\\nbird 0.53bird 0.52\\nguitar 1.00\\nperson 0.82\\nbicycle 0.92\\nperson 0.90\\nperson 0.83\\ncar 1.00 car 0.97\\ndog 0.98dog 0.86\\ndog 0.85\\ndog 0.65dog 0.50\\nperson 0.83\\nperson 0.80\\nperson 0.74person 0.54\\nelephant 0.60\\nFigure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,1) (top 1 − 24)\\n1.0 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,2) (top 1 − 24)\\n1.0 0.9 0.9 0.9 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,3) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,4) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,5) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,6) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,7) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,8) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,9) (top 1 − 24)\\n0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,10) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.5 0.5\\npool5 feature: (3,3,11) (top 1 − 24)\\n0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,12) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,13) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\npool5 feature: (3,3,14) (top 1 − 24)\\n0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,15) (top 1 − 24)\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,16) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,17) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,18) (top 1 − 24)\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,19) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,20) (top 1 − 24)\\n1.0 0.9 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\nFigure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly\\nactivate each of 20 units. Each montage is labeled by the unit’s (y, x, channel) position in the6 ×6 ×256 dimensional pool5 feature map.\\nEach image region is drawn with an overlay of the unit’s receptive ﬁeld in white. The activation value (which we normalize by dividing by\\nthe max activation value over all units in a channel) is shown in the receptive ﬁeld’s upper-left corner. Best viewed digitally with zoom.\\n21'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}, page_content='Full Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=ttie20\\nTheoretical Issues in Ergonomics Science\\nISSN: 1463-922X (Print) 1464-536X (Online) Journal homepage: https://www.tandfonline.com/loi/ttie20\\nNeurophysiological measures of cognitive\\nworkload during human-computer interaction\\nAlan Gevins & Michael E. Smith\\nTo cite this article: Alan Gevins & Michael E. Smith (2003) Neurophysiological measures of\\ncognitive workload during human-computer interaction, Theoretical Issues in Ergonomics Science,\\n4:1-2, 113-131, DOI: 10.1080/14639220210159717\\nTo link to this article:  https://doi.org/10.1080/14639220210159717\\nPublished online: 26 Nov 2010.\\nSubmit your article to this journal \\nArticle views: 1622\\nView related articles \\nCiting articles: 82 View citing articles'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 1, 'page_label': '0'}, page_content='Neurophysiological measures of cognitive workload during human–\\ncomputer interaction\\nAlan Gevins* and Michael E. Smith\\nSan Francisco Brain Research Institute and SAM Technology, 425 Bush Street, 5th Floor, San\\nFrancisco, CA 94108, USA\\nKeywords: Human–computer interaction; adaptive automation; mental workload; working\\nmemory; attention; neurophysiological signals; EEG.\\nPerhaps the most basic issue in the study of cognitive workload is the problem of\\nhow to actually measure it. The electroencephalogram (EEG) continues to be the\\nclinical method of choice for monitoring brain function in assessing sleep dis-\\norders, level of anaesthesia and epilepsy. This preference reﬂects the EEG’s high\\nsensitivity to variations in alertness and attention, the unimposing conditions\\nunder which it can be recorded, and the low cost of the technology it requires.\\nThese characteristics also suggest that EEG-based monitoring methods might\\nprovide a useful tool in ergonomics. This paper reviews a long-term programme\\nof research aimed at developing cognitive workload monitoring methods based\\non EEG measures. This research programme began with basic studies of the way\\nneuroelectric signals change in response to highly controlled variations in task\\ndemands. The results yielded from such studies provided a basis on which to\\ndevelop appropriate signal processing methodologies to automatically diﬀeren-\\ntiate mental eﬀort-related changes in brain activity from artifactual contaminants\\nand for gauging relative magnitudes of mental eﬀort in diﬀerent task conditions.\\nThese methods were then evaluated in the context of more naturalistic computer-\\nbased work. The results obtained from these studies provide initial evidence for\\nthe scientiﬁc andtechnical feasibility of using EEG-based methods for monitoring\\ncognitive load during human–computer interaction.\\n1. Introduction\\nAlthough the EEG has limitations with respect to its use as a method for three-\\ndimensional anatomical localization of neurofunctional systems, it has clear advan-\\ntages relative to other neuroimaging techniques as a method for continuous mon-\\nitoring of brain function. Indeed, it is often the method of choice for some clinical\\nmonitoring tasks. For example, continuous EEG monitoring is an essential tool in\\nthe diagnostic evaluation of epilepsy (Thompson and Ebersole 1999) and in the\\nevaluation and treatment of sleep disorders (Carskadon and Rechtschaﬀen 1989).\\nIt is also coming to play an increasingly important role in neuro-intensive care unit\\nmonitoring (Vespaetal. 1999) and in gauging level-of-awareness during anesthesia\\n(John etal. 2001, O’Connoretal. 2001).\\nFor many years, eﬀorts have also been under way to evaluate the extent to\\nwhich the EEG might be useful as a monitoring modality in applied work contexts.\\nTo be useful in such settings, a monitoring method should be robust enough to be\\nreliably measured under relatively unstructured task conditions, sensitive enough to\\nTheor. Issues in Ergon. Sci., 2003, vol. 4, nos. 1–2, 113–131\\nTheoreticalIssuesinErgonomicsScience ISSN 1463–922X print/ISSN 1464–536X online# 2003 Taylor & Francis Ltd\\nhttp://www.tandf.co.uk/journals\\nDOI 10.1080/14639220210159717\\n*Author for correspondence. e-mail: alan@eeg.com'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='consistently vary with some dimension of interest, unobtrusive enough to not inter-\\nfere with operator performance and inexpensive enough to eventually be deployable\\noutside of specialized laboratory environments. It should also have reasonably good\\ntime resolution to allow tracking of changes in mental status as complex behaviours\\nunfold. The EEG appears to meet such requirements. Furthermore, the compactness\\nof EEG technology also means that, unlike other functional neuroimaging modal-\\nities (which require massive machinery, large teams of technicians and complete\\nimmobilization of the subject), EEGs can even be collected from an ambulatory\\nsubject who is literally wearing the entire recording apparatus (Gilliametal. 1999).\\nIn recent years, we have been evaluating the potential of the EEG as a measure of\\ncognitive workload, primarily in individuals working at computers. Modern, com-\\nputer-based work environments demand sustained vigilance to multiple streams of\\ninformation. Such conditions have the potential to exceed a human’s limited\\ncapacity to attend to and analyse information; cognitive overload has, thus, long\\nbeen recognized (Card et al.1983, Kieras 1988, Olson and Olson 1990) to be an\\nimportant source of performance errors during human–computer interaction. The\\npotential for overload is particularly acute in unskilled users, where unfamiliar pro-\\ncedures are likely to require greater commitment of cognitive resources (Anderson\\nand Boyle 1987, Carlsonetal. 1989). The ability to continuously monitor cognitive\\nworkload might, thus, be valuable in task analysis research and in eﬀorts to improve\\nthe usability of human–computer interfaces (Raskin 2000). Indeed, a central prob-\\nlem in interface design is to develop means to provide information to a user with\\nminimum disruption and distraction (Cadizetal. 2001).\\nBecause the problem of cognitive overload is widely recognized, it has been the\\ntopic of extensive empirical attention. Ironically, perhaps the most basic issue in the\\nstudy of cognitive workload is the problem of how to actually measure it. One\\npossibility is to use the EEG to directly measure the brain’s response to a particular\\nset of task demands. An EEG-based measurement of cognitive workload could help\\ncharacterize the success of eﬀorts to design suitable interfaces and interaction pro-\\ntocols. Such a tool might also aid in the design of appropriate adaptive-automation\\nstrategies (Morrison and Gluckman 1994, Byrne and Parasuraman 1996,\\nParasuraman etal. 2000).\\nWe have been taking a systematic approach towards developing EEG-based\\nmethods for addressing this problem. Our ﬁrst eﬀorts revolved around identiﬁcation\\nand characterization of the properties of EEG signals sensitive to variations in the\\ndiﬃculty of highly controlled cognitive tasks. We also evaluated methods for analy-\\nsis of such signals that might be suitable for use in a continuous monitoring context.\\nMore recently, we have begun to generalize those methods to assess computer-based\\ntasks that are more naturalistic in character. In the following, we review the progress\\nof those eﬀorts.\\n2. Brain signals sensitive to variations in mental eﬀort\\nOur ﬁrst objective in this programme of research was to attempt to better character-\\nize the neurophysiological changes that accompany increases in cognitive workload\\nand the allocation of mental eﬀort. We have approached this issue in the context of\\nEEG and event-related potential (ERP) studies of working memory (WM). WM can\\nbe construed as an outcome of the ability to control attention and sustain its focus\\non a particular active mental representation (or set of representations) in the face\\nof distracting inﬂuences (Engle et al.1999). In many ways, this notion is nearly\\n114 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='synonymous with what we commonly understand as the ability to eﬀortfully ‘con-\\ncentrate’ on task performance. This ability plays an important role in comprehen-\\nsion, reasoning, planning and learning (Baddeley 1992). Indeed, the eﬀortful use of\\nactive mental representations to guide performance appears critical to behavioural\\nﬂexibility (Goldman-Rakic 1987, 1988) and measures of it tend to be positively\\ncorrelated with performance on psychometric tests of cognitive ability and other\\nindices of scholastic aptitude (Carpenteret al.1990, Kyllonen and Christal 1990,\\nGevins and Smith 2000).\\nMost of our investigations related to the neurophysiological concomitants of\\nWM have required subjects to perform controlled ‘n-back’ style tasks (Gevinsetal.\\n1990, 1996, Gevins and Cutillo 1993) that demand sustained attention to a train of\\nstimuli. In these tasks, the load imposed on WM varies, while perceptual and motor\\ndemands are kept relatively constant. For example, in a spatial variant of the n-back\\ntask we have often employed, stimuli are presented at diﬀerent spatial positions on a\\ncomputer monitor once every 4 or 5s while the subject maintains a central ﬁxation.\\nSubjects must compare the spatial location of each stimulus with that of a previous\\nstimulus, indicating whether a match criterion is met by making a key press response\\non a computer mouse or other device. In an easy, low load version of the task,\\nsubjects compare each stimulus to the ﬁrst stimulus presented in each block of\\ntrials (0-back task). In a more diﬃcult, higher load versions, subjects compare the\\nposition of the current stimulus with that presented one, two or even three trials\\npreviously (1-, 2-, or 3-back tasks). These require constant updating of the informa-\\ntion stored in working memory on each trial, as well constant attention to new\\nstimuli and maintenance of previously presented information. To be successful in\\nsuch tasks when WM demands are high, subjects typically must make a signiﬁcant\\nand continuous mental eﬀort. Similar n-back tasks have recently been adopted in\\nmany other laboratories as a means to activate WM networks in a controlled fashion\\nin the context of conventional behavioural studies (McElree 2001), other electrophy-\\nsiological studies (Ross and Segalowitz 2000, Wintinket al.2001), studies of the\\neﬀects of magnetic ﬁelds on cognitive function (Koivistoet al.2000, Oliveri et al.\\n2001) and functional neuroimaging studies employing PET or fMRI methods\\n(Jonides et al.1993, Cohen et al.1994, McCarthy et al.1994, Braver et al.1997,\\nJansma etal. 2000).\\nThe stimulus-locked ERPs recorded in such conditions in themselves provide an\\nintriguing picture of the transient, rapidly shifting, sub-second patterns of activation\\nthat characterize the neurofunctional networks that underlie task performance\\n(Gevins and Cutillo 1993, Gevins et al.1995, 1996, McEvoy et al.1998, 2001).\\nThere has also been a long and productive history of experimentation with such\\nmeasures as indices of task imposed cognitive workload (Isrealetal. 1980, Sirevaag\\netal. 1993, HumphreyandKramer1994, Wilson etal. 1994, Krameretal. 1995, Kok\\n2001, Ullsperger et al.2001). However, to compute such measures requires either\\nthat a primary task itself emit distinct and more-or-less regular stimuli that an ERP\\nresponse can be reliably time-locked to (something lacking from most real-world\\nactivities), or that a task-irrelevant probe stimulus be added to the operator’s work\\nenvironment. In either case, the low signal-to-noise inherent in most single trial\\nERPs can often necessitate averaging the response over many similar events.\\nThe spectral composition of the ongoing EEG also displays regular patterns of\\nload-related modulation during n-back task performance. Some components of the\\nEEG spectrum could have signiﬁcant utility for continuous monitoring applications\\nEEGmonitoringofcognitiveworkload 115'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 4, 'page_label': '3'}, page_content='and, in contrast to ERP indices, can be measured either independently of speciﬁc\\ntask events or in temporal conjunction to them. Because of these properties, we have\\nfocused our eﬀorts to develop workload-monitoring methods on such spectral EEG\\nmeasures. For example, ﬁgure 1 displays spectral power in the 4–14Hz range at a\\nfrontal midline (Fz) and a parietal midline (Pz) scalp location computed from the\\ncontinuous EEG during performance of low load (0-back) and moderately high load\\n(2-back) versions of a spatial n-back task. The data represent the average response\\nfrom a group of 80 subjects in a large study of individual diﬀerences in cognitive\\nability (Gevins and Smith 2000), and show signiﬁcantdiﬀerences in spectral power as\\na function of task load that vary between electrode locations and frequency bands.\\nMore speciﬁcally, at the midline frontal site a 5–7Hz or/C18-band spectral peak is\\nincreased in power during the high load task relative to the low load task. This type\\nof frontal midline/C28-signal has frequently been reported to be enhanced in diﬃcult,\\nattention demanding tasks, particularly those requiring a sustained focus of con-\\ncentration (Mizukietal. 1980, Miyataetal. 1990, Yamamoto and Matsuoka 1990,\\nGundel and Wilson 1992, Gevins et al. 1997, 1998, Gevins and Smith 1999).\\nTopographic analyses have indicated that this task loading-related/C18-signal tends\\nto have a sharply deﬁned potential ﬁeld with a focus in the anterior midline\\n116 A.Gevins andM.E.Smith\\nFigure 1. Eﬀect of varying the diﬃculty of an n-back working memory task on the spectral\\npower of EEG signals. The ﬁgure illustrates spectral power in dB of the EEG in 4–14Hz\\nrange at frontal (Fz) and parietal (Pz) midline electrodes, averaged over all trials of the tasks\\nand collapsed over 80 subjects. Data are from Gevins and Smith (2000).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='region of the scalp (Inouyeet al.1994, Gevinset al.1997); such a restricted topo-\\ngraphy is unlikely to result from distributed generators in dorsolateral cortical\\nregions. Instead, attempts to model the generating source of the frontal/C18-rhythm\\nfrom both EEG (Gevinsetal. 1997) and magnetoencephalographic (Ishiietal. 1999)\\ndata have implicated the anterior cingulate cortex as a likely region of origin. This\\ncortical region is thought to be part of an anterior brain network that is critical to\\nattention control mechanisms and that is activated by the performance of complex\\ncognitive tasks (Posner and Peterson 1990, Posner and Rothbart 1992). In a review\\nof over 100 positron emission tomography (PET) activation studies that examined\\nanterior cingulate cortex activity, Pausetal. (1998) found that the major source of\\nvariance that aﬀected activation in this region was associated with changes in task\\ndiﬃculty. The EEG results are, thus, consistent with these views, implying that\\nperformance of tasks that require signiﬁcant mental eﬀort places high demands on\\nfrontal brain circuits involved with attention control.\\nIn contrast, ﬁgure 1 also indicates that signals in the 8–12Hz or/C11-band tend to\\nbe attenuated in the high load task relative to the low load task. This inverse rela-\\ntionship between task diﬃculty and/C11-power has been observed in many studies in\\nwhich task diﬃculty has been systematically manipulated (Galinetal. 1978, Gundel\\nand Wilson 1992, Gevinsetal. 1997, 1998, Gevins and Smith 1999). Indeed, this task\\ncorrelate of the /C11-rhythm has been recognized for over 70 years (Berger 1929).\\nBecause of this load-related attenuation, the magnitude of/C11-activity during cogni-\\ntive tasks has been hypothesized to be inversely proportional to the fraction of\\ncortical neurons recruited into a transient functional network for purposes of task\\nperformance (Gevins and Schaﬀer 1980, Pfurtscheller and Klimesch 1992,\\nMulholland 1995). This hypothesis is consistent with current understanding of the\\nneural mechanisms underlying generation of the/C11-rhythm (reviewed in Smithetal.\\n2001). Convergent evidence for this view is also provided by observations of a\\nnegative correlation between /C11-power and regional brain activation as measured\\nwith PET (Larsonet al.1998, Sadato et al.1998), and the frequent ﬁnding from\\nneuroimaging studies of greater and more extensive brain activation during task\\nperformance when task diﬃculty increases (Bakeret al.1996, Bunge et al.2000,\\nCarpenter etal. 2000, Garavanetal. 2000).\\nIn addition to signals in the/C18-a n d/C11-bands, other spectral components of the\\nEEG have also been reported to be sensitive to changes in eﬀortful attention. These\\ninclude slow wave activity in the/C14- (<3Hz) band (McCallumetal. 1988, Rockstroh\\net al.1989), high frequency activity in the/C12- (15–30Hz) and /C13- (30–50Hz) bands\\n(Sheer 1989) and rarely studied phenomenon such as the /C20-rhythm that occurs\\naround 8Hz in a small percentage of subjects (Kennedy et al.1948, Chapman\\net al.1962). Since such phenomena were observed relatively infrequently in our\\nseries of studies on working memory, we will not discuss them further, but they\\nmay, nonetheless, ultimately prove useful in eﬀorts to monitor cognitive workload\\nusing EEG measures.\\n3. Automating detection of mental eﬀort-related changes in the EEG\\nAs the data reviewed above indicate, spectral components of the EEG do in fact vary\\nin a predictable fashion in response to variations in the cognitive demands of tasks.\\nWhile this is a necessary condition for the development of an EEG-based monitor of\\ncognitive workload, it is not suﬃcient. A number of other issues must also be\\naddressed if such laboratory observations are to be transitioned into practical\\nEEGmonitoringofcognitiveworkload 117'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='tools. Foremost among them is the problem of EEG artifact. That is, in addition to\\nbrain activity, signals recorded at the scalp include contaminating potentials from\\neye movements and blinks, muscle activity, head movements and other physiological\\nand instrumental sources of artifact. Such contaminants can easily mask cognition-\\nrelated EEG signals (Barlow 1986, Gevinset al.1979a, b, c, 1980). In laboratory\\nstudies, human experts can be used to actively identify artifacts in raw data and\\neliminate any contaminated EEG segments, to insure that data used in analyses\\nrepresent actual brain activity. For large amounts of data, this is an expensive,\\nlabour-intensive process which itself is both subjective and variable. To be practical\\nin more routine applied contexts such decisions must be made algorithmically.\\nA great deal of research has been directed towards the problem of automated\\nartifact detection. In previous work in our laboratory, we have developed and objec-\\ntively evaluated several generations of automatic artifact detection algorithms. These\\ninclude multi-criteria spectral detectors (Gevinset al.1975, 1977), sharp transient\\nwaveform detectors (Gevins et al. 1976), and detectors using neural networks\\n(Gevins and Morgan 1986, 1988). We have found that our most recent generations\\nof detection algorithms perform about as well as the consensus of expert human\\njudges. In a database of/C2440000 eye-movement, head/body movement and muscle\\nartifacts, the algorithms successfully detected 98.3% of the artifacts, with a false\\ndetection rate of 2.9%, whereas the average expert human judge found 96.5% of\\nthe artifacts, with a 1.7% false detection rate. Thus, while further work on the topic\\nis needed, it is reasonable to expect that the problem of automated artifact detection\\nwill not be an insurmountable barrier to the development of an EEG-based cognitive\\nworkload monitor.\\nA closely related problem is the fact that, in subjects actively performing tasks\\nwith signiﬁcant perceptuomotor demands in a normal fashion, the incidence of data\\nsegments contaminated by artifacts can be high. As a result, it can be diﬃcult to\\nobtain enough artifact-free data segments for analysis. To minimize data loss, eﬀec-\\ntive digital signal processing methods must also be developed to ﬁlter contaminants\\nout of the EEG when possible. Our main approach to this problem has been to\\nimplement adaptive ﬁltering methods to decontaminate artifacts from EEG signals\\n(Du etal. 1994). We have found such methods to be eﬀective at recovering most of\\nthe artifact contaminated data recorded in our typical laboratory studies of subjects\\nworking on computer-based tasks. A variety of other methods have been employed\\nby diﬀerent investigators in response to this problem, including such techniques\\nas autoregressive modelling (Van den Berg-Lensssenet al.1989), source modeling\\napproaches(BergandScherg1994)andindependentcomponentsanalysis(Jung etal.\\n2000). As with the problem of artifact detection, continued progress in this area\\nsuggests that, at least under some conditions and for some types of artifacts, decon-\\ntamination strategies will evolve that will enable the automation of EEG processing\\nfor continuous monitoring applications.\\nPresuming, then, that automated pre-processing of the EEG can yield suﬃcient\\ndata for subsequent analyses, questions still remain as to whether the type of load-\\nrelated changes in EEG signals can be measured in a reliable fashion in individual\\nsubjects and whether such measurements can be accomplished with a temporal\\ngranularity suitable for tracking complex behaviours. That is, in the experiments\\ndescribed above,changes in the/C18- and/C11-bands in response to variations in WM load\\nwere demonstrated by collapsing over many minutes of data recorded from a subject\\nat each load level, and then comparing the mean diﬀerences between load levels\\n118 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='across groups of subjects using conventional parametric statistical tests. Under\\nnormal waking conditions, such task-related EEG measures have high test–re-test\\nreliability when compared across a group of subjects measured during two sessions\\nwith 1 week between them (McEvoyetal. 2000). However, for the development of\\nautomated EEG analysis techniques suitable for monitoring applications, load-\\nrelated changes in the EEG would ideally also be replicable when computed over\\nshort segments of data, and would need to have high enough signal-to-noise ratios to\\nbe measurable within such segments.\\nPrior work has demonstrated that multivariate combinations of EEG variables\\ncan be used to accurately discriminate between speciﬁc cognitive states (Gevinsetal.\\n1979a, b, c, Wilson and Fisher 1995). Neural network-based pattern classiﬁcation\\nalgorithms trained on data from individual subjects could also be used to automa-\\ntically discriminate data recorded during diﬀerent load levels of versions of the type\\nof n-back WM task described above. For example, we performed an experiment\\n(Gevins et al.1998) in which eight subjects performed both spatial and verbal ver-\\nsions of3-,2-and 1-back WMtasks on testsessionsconducted ondiﬀerent days. For\\neach single trial of data in each subject, spectral power estimates were computed in\\nthe /C18- and/C11-bands for each electrode site. Pattern recognition was performed with\\nthe classic Joseph-Viglione neural network algorithm (Joseph 1961, Viglione 1970,\\nGevins 1980, Gevins and Morgan 1986, 1988). This algorithm iteratively generates\\nand evaluates two-layered feed-forward neural networks from the set of signal fea-\\ntures, automatically identifying small sub-sets of features that produce the best\\nclassiﬁcation of examples from the sample of data set aside for training. The result-\\ning classiﬁer networks were then cross-validated on the remaining data not included\\nin the training sample.\\nUtilizing these procedures, we found that test data segments from 3-back vs\\n1-back load levels were discriminated with over 95% (p< 0:001) accuracy. Over\\n80% (p< 0:05) of test data segments associated with a 2-back load could also be\\ndiscriminated from data segments in the 3-back or 1-back task loads. Such results\\nprovide initial evidence that, at least for these types of tasks, it is possible to develop\\nalgorithms capable of discriminating diﬀerent cognitive workload levels with a high\\ndegree of accuracy. Not surprisingly, they also indicated that relatively large diﬀer-\\nences in cognitive workload are easier to detect than smaller diﬀerences, and that\\nthere is an inherent trade-oﬀ between the accuracy of classiﬁer performance and the\\ntemporal length of the data segments being classiﬁed.\\nHigh levels of accurate classiﬁcation were also achieved when applying networks\\ntrained with data from one day to data from another day and when applying net-\\nworks trained with data from one task (e.g. spatial WM) to data from another task\\n(e.g. verbal WM). We also attempted to develop networks trained with data from a\\ngroup of subjects to data from new subjects. Such generic networks were found on\\naverage to yield statistically signiﬁcant classiﬁcation results when discriminating the\\n1-back from the 3-back task load conditions, but their accuracy was much reduced\\nfrom that achievable with subject speciﬁc networks. On the one hand, such results\\nindicate that there is a fair amount of commonality across days, tasks and subjects in\\nthe particular set of EEG frequency-band measures that are sensitive to increases in\\ncognitive workload. Such commonalities can be exploited in eﬀorts to design eﬃcient\\nsensor montages and signal processing methods. Nonetheless, they also indicate that,\\nto achieve optimal performance using EEG-based cognitive load monitoring\\nmethods, it will likely be necessary to calibrate algorithms to accommodate\\nEEGmonitoringofcognitiveworkload 119'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='individual diﬀerences. Such conclusions are also consistent with the observation that\\npatterns of task-related EEG changes vary in conjunction with individual diﬀerences\\nin cognitive ability and cognitive style (Gevins and Smith 2000).\\nFinally, it is also worthwhile to brieﬂy mention another potential direction for\\nsuch methods. In examining changes in neurophysiological correlates of n-back task\\nperformance over time, we found substantial changes in both the magnitude and the\\ntopography of task-related modulation of EEG activity in the /C11-a n d/C18-bands\\nbetween the time when naı¨ve subjects were ﬁrst learning to perform versions of\\nthe n-back WM and after they had developed some skill at it (Gevinset al.1997,\\nSmith etal. 1999). To further analyse these data, we used neural network methods\\nanalogous to those described above in an attempt to discriminate the task-related\\nEEG signals recorded in the 3-back task when subjects were ﬁrst learning the task\\nfrom signals recorded during skilled performance. Across subjects, we were able to\\nobtain very high levels of classiﬁcation accuracy (range: 96–100%,p< 0:001) for\\ndiscriminating naı¨ve from practiced states. Such results imply that the WM demands\\nof task performance changed with practice and that such changes could be auto-\\nmaticallydetectedwithEEG methods. Since overload of attentional orWM capacity\\nhas been found to be a limiting factor in the early stages of procedural skill acquisi-\\ntion (Woltz 1988, Kyllonen and Shute 1989), minimizing the potential of such over-\\nload is an important design guideline for the development of intelligent tutoring\\nsystems (Anderson and Boyle 1987, Carlson et al. 1989). The data described\\nabove, thus, suggest that it might be possible to utilize information provided by\\nsuch monitoring methods to adapt a computer-aided instruction protocol to the\\ncognitive constraints and skill levels of individual students.\\n4. Extension of neurophysiology-based workload monitoring methods to\\n‘naturalistic’ HCI\\nThe types of results described above provide evidence for the basic feasibility of\\nusing EEG-based methods for unobtrusively monitoring cognitive task load in\\nindividuals engaged in computer-based work. However, the n-back WM task\\nmade minimal demands on perceptual and motor systems and it only required\\nthat a subject’s eﬀort be focused on a single repetitive activity. The ability to reliably\\nmeasure cognitive load in individual subjects under such constrained circumstances\\nmight in itself be useful. For example, it has been applied to the problem of assessing\\nthe eﬀect of environmental stressors on cognitive functions (Gevins and Smith 1999).\\nEven so, in more naturalistic work environments, task demands are usually less\\nstructured and mental resources often must be divided between competing activities,\\nraising questions as to whether results obtained with the n-back task could generalize\\nto contexts that are more realistic.\\nRecent studies have demonstrated that more complicated forms of human–\\ncomputer interaction (such as videogame play) produce mental eﬀort-related mod-\\nulation of the EEG that is similar to that observed during n-back tasks (Pellouchoud\\netal. 1999, Smithetal. 1999). This implies that it might be possible to extend EEG-\\nbased multivariate methods for monitoring task load to such circumstances. To\\nevaluate this possibility we performed a subsequent study (Smithet al.2001) in\\nwhich the EEG was recorded while subjects performed the Multi-Attribute Task\\nBattery (MATB; Comstock and Arnegard 1992). The MATB is a personal com-\\nputer-based multi-tasking environment that simulates some of the activities a pilot\\nmight be required to perform. It has been used in several prior studies of mental\\n120 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='workload and adaptive automation (Parasuramanetal. 1993, 1996, Fournieretal.\\n1999). The data collected during performance of the MATB were used to test\\nwhether it is possible to derive combinations of EEG features that can be used for\\nindexing task loading during a relatively complex form of human–computer inter-\\naction.\\nThe MATB task included four concurrently performed sub-tasks in separate\\nwindows on a computer screen (for graphic depictions of the MATB visual display,\\nsee Fournier et al.(1999) and Molloy and Parasuraman (1996)). These included a\\n‘systems monitoring task’ that required the operator to monitor and respond to\\nsimulated warning lights and gauges, a ‘resource management task’ in which fuel\\nlevels in two tanks had to be maintained at a certain level, a ‘communications task’\\nthat involved receiving audio messages and making frequency adjustments on virtual\\nradios, and a compensatory tracking task that simulated manual control of aircraft\\nposition. Manipulating the diﬃculty of each sub-task served to vary load; such\\nmanipulations were made in a between blocks fashion. Subjects learned to perform\\nlow-, medium- and high-load (LL, ML and HL) versions of the tasks. For compar-\\nison purposes, they also performed a ‘passive watching’ (PW) condition in which\\nthey observed the tasks unfolding without actively performing them.\\nSubjects engaged in extensive training on the tasks on one day, and then returned\\nto the laboratory on a subsequent day for testing. On the test day, subjects per-\\nformed multiple 5min blocks of each task diﬃculty level. Behavioural and subjective\\nworkload ratings provided evidence that, on average, workload did indeed increase\\nin a monotonic fashion across the PW, LL, ML and HL task conditions. This\\nincrease in workload was associated with systematic changes in the EEG. In par-\\nticular, as in the prior study of workload changes in the n-back task paradigm,\\nfrontal /C18-band activity tended to increase with increasing task diﬃculty, whereas\\n/C11-band activity tended to decrease (ﬁgure 2). Such results indicated that the work-\\nload manipulations were successful, and that spectral features in the/C18- and/C11-range\\nmight be useful in attempting to automatically monitor changes in workload with\\nEEG measures.\\nSeparate blocks of data were, thus, used to derive and then independently vali-\\ndate subject-speciﬁc, EEG-based, multivariate cognitive workload functions. In con-\\ntrast to the two-class pattern detection functions that were employed to discriminate\\nbetween diﬀerent task load levels in the prior study, we evaluated a diﬀerent tech-\\nnique that results in a single subject-speciﬁc function that produces a continuous\\nindex of cognitive workload and, hence, could be applied to data collected at each\\ndiﬃculty level of the task. In this procedure, the EEG data was ﬁrst decomposed into\\nshort windows and a set of spectral power estimates of activity in the/C18- and /C11-\\nfrequency ranges was extracted from each window. A unique multivariate function\\nwas then deﬁned for each subject that maximized the statistical distance or diver-\\ngence (Tou and Gonzalez 1974) between a small sample of data from low and high\\ntask load conditions. To cross-validate the function it was tested on new data seg-\\nments from the same subject. Across subjects (ﬁgure 3), mean task load index values\\nwere found to increase systematically with increasing task diﬃculty, and diﬀered\\nsigniﬁcantly between the diﬀerent versions of the task (Smithet al.2001). These\\nresults provide encouraging initial evidence that EEG measures can indeed provide\\na modality for measuring cognitive workload during more complex forms of com-\\nputer interaction. Although complex, the signal processing and pattern classiﬁcation\\nalgorithms employed in this study were for real time implementation. A prototype\\nEEGmonitoringofcognitiveworkload 121'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 10, 'page_label': '9'}, page_content='online system running on a circa 1997 personal computer performed the requisite\\ncalculations on-line and provided an updated estimate of cognitive workload at 4s\\nintervals while subjects were engaged in task performance.\\nTo further evaluate the utility of the approach described above as a tool for\\nresearch on human–computer interaction, we also performed a small exploratory\\nstudy that involved more naturalistic computer tasks. In this experiment (Smith and\\n122 A.Gevins andM.E.Smith\\n \\n-1.5\\n-1\\n-0.5\\n0\\n0.5\\n1\\n1.5\\nPW                        LL                         ML                           HL\\nMean (s.e.m)  Power (z-scores)\\nFz Theta 6-7Hz\\nPz Alpha 8-10Hz\\nFigure 2. Mean (n¼ 16) EEG power for the frontal/C18- and parietal/C11-EEG signals during\\nperformance of the MATB ﬂight simulation task. Data have been normalized within each\\nsubject. Data are presented for each of four task versions (PW¼passive watch, L¼low\\nload, ML¼moderate load, HL¼high load). Normalized spectral power for frontal/C18and\\nparietal /C11are plotted./C28-power increases from the PW to the HL task version, whereas/C18-\\npower decreases from the PW to the HL task version. Data are from Smithetal. (2001).\\n         \\nCognitive Workload Scores During MATB\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nPassive\\nWatching\\n     Low     \\nLoad\\nMedium\\nLoad\\n     High     \\nLoad\\nMATB Task Condition\\nIndex Value \\n \\nFigure 3. Mean and SEM (n = 16) EEG-based cognitive workload index values during\\nperformance of the MATB ﬂight simulation task. Data are presented for each of four\\ntask versions (PW¼passive watch, LL¼low load, ML¼moderate load, HL¼high load).\\nAverage cognitive workload index scores increased monotonically with increasing task\\ndiﬃculty. Data are from Smithetal. (2001b).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='Gevins, unpublished observations), EEG data was ﬁrst recorded from subjects as\\nthey performed multiple series of trials of easy (0-back) and diﬃcult (3-back) n-back\\ntasks, as well as when they rested quietly, in order to establish a baseline view of the\\nresponse of their brain to increased cognitive load. Data was then recorded while\\nsubjects performed more common computer-based tasks that were performed under\\ntime pressure and that were more-or-less intellectually demanding. These more nat-\\nuralistic activities required subjects to perform word processing, take a computer-\\nbased aptitude test, and search for information on the web. The word processing\\ntask requiredsubjects tocorrect asmanymisspellings and grammatical errors asthey\\ncould in the time allotted, working on a lengthy text sample using a popular word\\nprocessing program. The aptitude test was a practice version of the Computer-\\nAdaptive GMAT\\n1 test. Subjects were asked to solve as many ‘data suﬃciency’\\nproblems as possible in the time allotted; such problems make a high demand on\\nlogical and quantitative reasoning skills and require signiﬁcant mental eﬀort to\\ncomplete in a timely fashion. The web-searching task required subjects to use a\\npopular web browser and search engine to ﬁnd as many answers as possible in the\\ntime allotted to a list of trivia questions provided by the experimenter. For example,\\nsubjects were required to use the browser and search engine to ‘convert 98.6 degrees\\nFahrenheit into degrees Kelvin’, ‘ﬁnd the population of the 94105 area code in the\\n1990 US Census’ and ‘ﬁnd the monthly mortgage payment on a $349000, 30 year\\nmortgage with a 7.5% interest rate’. Each type of task was structured such that\\nsubjects would be unlikely to be able to complete it in the time allotted.\\nThe same basic analysis procedure described above that was applied to the EEG\\ndata recorded during MATB performance was also employed in this study. More\\nspeciﬁcally, a personalized continuous index of cognitive workload was ﬁrst devel-\\noped for each subject from a calibration set of data. In this case, the calibration data\\nused to create the subject-speciﬁc index of cognitive workload included samples of\\nthe subject’s 0-back and 3-back WM task EEG data. The resulting function was then\\napplied to windowed samples of that subject’s data from the quiet resting condition,\\nfrom samples of 0-back and 3-back data not included in the calibration data set,\\nand from samples of data during performance of the various naturalistic types of\\ncomputer-based work.\\nA summary of the results from these analyses, averaged across data segments\\nwithin each task condition and compared between conditions, is presented in ﬁgure\\n4. These comparisons indicated that the cognitive load index performed in a pre-\\ndictable fashion. That is, the condition in which the subject was asked to sit quietly\\nand passively view a blank screen produced an average EEG-based cognitive work-\\nload around the zero point of the scale. Average index values during 0-back task\\nperformance were slightly higher than those during the resting condition, and aver-\\nage index values during the 3-back task were signiﬁcantly higher than those recorded\\neither during the 0-back WM task or during the resting state. All three naturalistic\\ntasks produced workload index values slightly higher than that obtained in the 3-\\nback task, which might be expected given that the n-back tasks had been practiced\\nand were repetitive in nature, whereas the other tasks were novel and required the\\nuse of strategies of information gathering, reasoning and responding that were less\\nstereotyped in form. Among the naturalistic tasks, the highest levels of cognitive\\nworkload were recorded during the computerized aptitude-testing task—the con-\\ndition that was also subjectively experienced as the most diﬃcult.\\nEEGmonitoringofcognitiveworkload 123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 12, 'page_label': '11'}, page_content='This pattern of results is interesting, not only because it conforms witha priori\\nexpectations about how workload would vary among the diﬀerent tasks, but also\\nbecause it provides data relevant to the issue of how the workload measure is\\naﬀected by diﬀerences in perceptuomotor demands across conditions. Since in the\\nn-back tasks, stimuli and motor demands are kept constant between the 0-back and\\n3-back load levels, the observed EEG diﬀerences in those conditions are clearly\\nclosely related to diﬀerences in the amounts of mental work demanded by the two\\ntask variants rather than other factors. However, in the study of MATB task per-\\nformancedescribed above, thesource ofvariation inthe index is somewhat less clear.\\nOn the one hand, performance and subjective measures unambiguously indicated\\nthat the mental eﬀort required to perform the high load version of the MATB was\\nsubstantially greater than that required by the low load (or passive watching) ver-\\nsions. On the other hand, the perceptuomotor requirements in the high load version\\nwere also substantially greater than those imposed by the other version. In this latter\\nexperiment, such confounds was less of a concern. Indeed, both the text editing task\\nand the web searching task required more eﬀortful visual search and more active\\nphysical responding than the aptitude test, whereas the aptitude test had little read-\\ning and less responding and instead required a great deal of thinking and mental\\nevaluation of possibilities. Thus, the fact that the average cognitive workload values\\nduring performance of the aptitude test were higher than those observed in the other\\ntasks provides convergent support for the notion that the subject-speciﬁc indices\\nwere more closely tracking variations in mental demands rather than variations in\\nperceptuomotor demands in these instances.\\n124 A.Gevins andM.E.Smith\\nCOGNITIVE LOA D DURING HCI\\n-0.3\\n0\\n0.3\\n0.6\\n0.9\\n1.2\\n1.5\\neyes open\\nresting\\n0-back 3-back speeded\\ntext\\nediting\\naptitude\\ntest taking\\nspeeded\\nweb\\nsearching\\nindex value\\n Figure 4. Mean and SEM (n¼ 7) EEG-based cognitive workload index values during rest-\\ning conditions, easy and diﬃcult versions of the n-back WM tasks, and a few naturalistic\\ntypesofcomputer-based work(seetextforfulldescription oftasksandprocedure).Thedata\\nrepresent average index values over the course of each type of task. The easy WM and\\nresting conditions produced signiﬁcantly lower values than the more diﬃcult WM condition\\nor during the naturalistic tasks.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 13, 'page_label': '12'}, page_content='5. Conclusions\\nIn summary, the results reviewed above indicate that the EEG changes in a highly\\npredictable way in response to sustained changes in task load and associated changes\\nin the mental eﬀort required for task performance. It appears that such changes can\\nbe automatically detected and measured using algorithms that combine parameters\\nof the EEG power spectra into multivariate functions. Such methods can be eﬀective\\nboth in gauging the variations in cognitive workload imposed by highly controlled\\nlaboratory tasks and in monitoring diﬀerences in the mental eﬀort required to\\nperform tasks that more closely resemble those that an individual might encounter\\nin a real-world work environment.\\nThe results presented would beneﬁt from further replication, and are in need of\\nsigniﬁcant reﬁnement. For example, the data presented herein collapsed cognitive\\nactivity over ‘whole-tasks’, that is, the data were collapsed over many minutes of\\nsustained performance. However, cognitive workload indices were calculated over\\ndata segments that were frequently updated and were of much shorter duration.\\nFuture work will need to try to identify how such momentary measures of cognitive\\nworkload vary with speciﬁc intra-task events. One possibility for using EEG spectral\\nmeasures to evaluate cognitive load in response to speciﬁc task events is to employ\\n‘event-related desynchonization’ (ERD) methods that compare post-stimulus power\\nto a pre-stimulus baseline measure and use degree of change in the spectra as a\\nload measure. However, past eﬀorts to evaluate such measurements in somewhat\\nnaturalistic tasks (in fact, during the MATB) have found that, although useful in\\nsingle tasks, the ERD is insensitive to workload variations in multi-tasking contexts\\n(Fournier et al.1999). This failure likely reﬂects the fact that, in such contexts,\\nworkload is likely to be relatively high even in the pre-stimulus baseline period,\\nand so any stimulus related change in the EEG spectra is likely to be fairly small.\\nIt is an open question whether other types of EEG-based methods might be more\\nfruitfully applied in such circumstances.\\nAnother area of future reﬁnement is related to the current unitary nature of the\\ncognitive workload measures. That is, some views of the structure of the mental\\nresources that can be allocated to task performance posit a relative independence\\nofthe resources involvedwithcognitive processesand those involved with perceptual\\nprocessing and motor expression. Future development of such methods should, thus,\\nexplore the possibility of developing somewhat orthogonal physiological indices that\\ncan diﬀerentiate between the loading of one or another type of neural resource\\nsystem. While the need for such future reﬁnements is clear, the current results, none-\\ntheless, provide compelling initial evidence for the feasibility of creating EEG-based\\ntechnologies for monitoring cognitive workload during human–computer inter-\\naction.\\nAcknowledgements\\nThis research was supported by several agencies of the US Government including\\nthe Air Force Oﬃce of Scientiﬁc Research, the National Science Foundation and\\nthe National Aeronautics and Space Administration. We thank Drs Harrison\\nLeong, Robert Du and Linda McEvoy for their contributions to the studies reviewed\\nherein.\\nEEGmonitoringofcognitiveworkload 125'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='References\\nAnderson,J.R. and Boyle,S.F. 1987, Cognitive principles in the design of computer tutors,\\nin P. Morris (ed.),ModellingCognition (London: John Wiley & Sons Ltd), 93–133.\\nBaddeley,A. 1992, Working Memory,Science, 255, 556–559.\\nBaker,S.C.,Rogers,R.D.,Owen,A.M.,Frith,C.D.,Dolan,R.J.,Frackowiak,R.S.\\nand R o b b i n s ,T .W .1996, Neural systems engaged by planning: a PET study of the\\nTower of London task,Neuropsychologia, 34, 515–526.\\nBarlow,J.S. 1986, Artifact processing rejection and reduction in EEG data processing, in\\nF. H. Lopes da Silva, W. Storm van Leeuwen and A. Remond (eds),Handbook of\\nElectroencephalography and Clinical Neurophysiology Vol 2(Amsterdam: Elsevier),\\n15–65.\\nBerg,P. and Scherg,M. 1994, A multiple source approach to the correction of eye artifacts,\\nElectroencephalographyandClinicalNeurophysiology , 90, 229–241.\\nBerger, H.1929, Uber das Elektroenzephalogramm des Menschen,Archives of Psychiatry,\\n87, 527–570.\\nB r a v e r ,T .S . ,C o h e n ,J .D . ,N y s t r o m ,L .E . ,J o n i d e s ,J . ,S m i t h ,E .E .and Noll, D. C.\\n1997, A parametric study of prefrontal cortex involvement in human working\\nmemory, Neuroimage, 5, 49–62.\\nBunge,S.A.,Klingberg,T.,Jacobsen,R.B. and Gabrieli,J.D. 2000, A resource model\\nof the neural basis of executive working memory,ProceedingsoftheNationalAcademy\\nofSciences(USA) , 97, 3573–3578.\\nByrne, E. A. and Parasuraman, R. 1996, Psychophysiology and adaptive automation,\\nBiologicalPsychology, 42, 249–268.\\nCadiz,J.J.,Venolia,G.D.,Jancke,G. and Gupta,A., 2001, Sideshow:Providingperiph-\\neralawarenessofimportantInformation (MSR-TR-2001-83, Redmond, WA: Microsoft\\nResearch, Microsoft Corporation).\\nC a r d ,S .K . ,M o r a n ,T .P .and Newell, A. 1983, The Psychology of Human-Computer\\nInteraction(Hillsdale, NJ: Lawrence Erlbaum Associates, Inc).\\nCarlson,R.A.,Sullivan,M.A. and Schneider,W. 1989, Practice and working memory\\neﬀects in building procedural skill, Journal of Experimental Psychology: Learning,\\nMemoryandCognition , 3, 517–526.\\nCarpenter, P. A., Just, M. A.and Reichle, E. D.2000, Working memory and executive\\nfunction: evidence from neuroimaging,CurrentOpinioninNeurobiology , 10, 195–199.\\nCarpenter,P.A.,Just,M.A. and Shell,P. 1990, What one intelligence test measures: a\\ntheoretical account of the processing in the Raven Progressive Matrices Test,\\nPsychological Review, 97, 404–431.\\nCarskadon,M.A. and Rechtschaffen,A. 1989, Monitoring and staging human sleep, in\\nM. H. Kryger, T. Roth and W. C. Dement (eds), Principles and Practice of Sleep\\nMedicine,2 nd edn (Philadelphia: W.B. Saunders & Co), 943–960.\\nChapman, R. M., Armington, J. C.and Bragden, H. R.1962, A quantitative survey of\\nkappa and alpha EEG activity,Electroencephalography and Clinical Neurophysiology,\\n14, 858–868.\\nCohen, J. D., Forman, S. D., Braver, T. S., Casey, B. J., Servan-Schreiber, D.and\\nNoll, D. C.1994, Activation of prefrontal cortex in a non-spatial working memory\\ntask with functional MRI,HumanBrainMapping , 1, 293–304.\\nC o m s t o c k,J .R .and Arnegard, R. J.1992, The Multi-Attribute Task Battery for Human\\nOperator Workload and Strategic Behavior Research (104174: NASA Technical\\nMemorandum).\\nDu, W., Leong, H. M.and Gevins, A. S.1994, Ocular artifact minimization by adaptive\\nﬁltering, ProceedingsoftheSeventhIEEESPWorkshoponStatisticalSignalandArray\\nProcessing, Quebec City, Canada, 433–436.\\nEngle,R.W.,Tuholski,S. and Kane,M. 1999, Individual diﬀerences in working memory\\ncapacity and what they tell us about controlled attention, general ﬂuid intelligence and\\nfunctions of the prefrontal cortex, in A. Miyake and P. Shah (eds),ModelsofWorking\\nMemory(Cambridge: Cambridge University Press), 102–134.\\nF o u r n i e r ,L .R . ,W i l s o n ,G .F .and Swain, C. R.1999, Electrophysiological, behavioral,\\nand subjective indexes of workload when performing multiple tasks: manipulations of\\ntask diﬃculty and training,InternationalJournalofPsychophysiology , 31, 129–145.\\n126 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='Galin,D.,Johnstone,J. and Herron,J. 1978, Eﬀects of task diﬃculty on EEG measures of\\ncerebral engagement,Neuropsychologia, 16, 461–472.\\nGaravan,H.,Ross,T.J.,Li,S. and Stein,E.A. 2000, A parametric manipulation of central\\nexecutive functioning,CerebralCortex, 10, 585–592.\\nGevins, A. and Cutillo, B. 1993, Spatiotemporal dynamics of component processes in\\nhuman working memory, Electroencephalography and Clinical Neurophysiology, 87,\\n128–143.\\nGevins, A. and Morgan, N. 1986, Classiﬁer-directed signal processing in brain research,\\nIEEETransactionsonBiomedicalEngineering , 33, 1058–1064.\\nGevins, A. and Smith, M. E. 1999, Detecting transient cognitive impairment with EEG\\npattern recognition methods, Aviation Space and Environmental Medicine, 70, 1018–\\n1024.\\nGevins, A. and Smith, M. E.2000, Neurophysiological measures of working memory and\\nindividual diﬀerences in cognitive ability and cognitive style,CerebralCortex, 10, 829–\\n839.\\nG e v i n s ,A . ,L e o n g ,H . ,S m i t h ,M .E . ,L e ,J .and Du, R. 1995, Mapping cognitive\\nbrain function with modern high-resolution electroencephalography, Trends in\\nNeurosciences, 18, 429–436.\\nGev ins,A .,S mith,M.E.,L eong,H.,M cEv oy ,L. ,Wh itfield,S .,Du,R.and Rush,G.\\n1998, Monitoring working memory load during computer-based tasks with EEG\\npattern recognition methods,HumanFactors, 40, 79–91.\\nGevins,A.,Smith,M.E.,McEvoy,L. and Yu,D. 1997, High-resolution EEG mapping of\\ncortical activation related to working memory: eﬀects of task diﬃculty, type of process-\\ning, and practice,CerebralCortex, 7, 374–385.\\nGevins,A.S. 1980, Pattern recognition of brain electrical potentials,IEEE Transactions on\\nPatternAnalysisandMachineIntelligence , 2, 383–404.\\nGevins,A.S. and Morgan,N.H. 1988, Applications of neural-network (NN) signal proces-\\nsing in brain research,IEEETransactions onAcoustics, Speech, andSignalProcessing ,\\n36, 1152–1161.\\nGevins,A.S. and Schaffer,R.E. 1980, A critical review of electroencephalographic EEG\\ncorrelates of higher cortical functions,CRCCriticalReviewsinBioengineering , 4, 113–\\n164.\\nGevins,A.S.,Bressler,S.L.,Cutillo,B.A.,Illes,J.,Miller,J.C.,Stern,J. and Jex,\\nH. R. 1990, Eﬀects of prolonged mental work on functional brain topography,\\nElectroencephalographyandClinicalNeurophysiology , 76, 339–350.\\nGevins, A. S., Doyle, J. C., Schaffer, R. E., Callaway, E. and Yeager, C. 1980,\\nLateralized cognitive processes and the electroencephalogram,Science, 207, 1005–1008.\\nGevins,A.S.,Smith,M.E.,Le,J.,Leong,H.,Bennett,J.,Martin,N.,McEvoy,L.,Du,\\nR. and Whitfield,S. 1996, High resolution evoked potential imaging of the cortical\\ndynamics of human working memory, Electroencephalography and Clinical Neuro-\\nphysiology, 98, 327-348.\\nGevins,A.S.,Yeager,C.L.,Diamond,S.L.,Spire,J.P.,Zeitlin,G.M. and Gevins,A.\\nH. 1975, Automated analysis of the electrical activity of the human brain (EEG): a\\nprogress report,ProceedingsoftheInstituteofElectricalandElectronicsEngineers , 63,\\n1382–1399.\\nGevins, A. S., Yeager, C. L., Diamond, S. L., Spire, J. P., Zeitlin, G. M.and Gevins,\\nA.H. 1976, Sharp-transient analysis and thresholded linear coherence spectra of par-\\noxysmal EEGs, in P. Kellaway and I. Petersen (eds), Quantitative Analytic Studies in\\nEpilepsy(New York: Raven Press), 463–481.\\nGevins, A. S., Yeager, C. L., Zeitlin, G. M., Ancoli, S.and Dedon, M. 1977, On-line\\ncomputer rejection of EEG artifact, Electroencephalography and Clinical\\nNeurophysiology, 42, 267–274.\\nG e v i n s ,A .S . ,Z e i t l i n ,G .M . ,D o y l e ,J .C . ,S c h a f f e r ,R .E .and Callaway, E. 1979a,\\nEEG patterns during ‘cognitive’ tasks. II. Analysis of controlled tasks,\\nElectroencephalographyandClinicalNeurophysiology , 47, 704–710.\\nGevins, A. S., Zeitlin, G. M., Doyle, J. C., Yingling, C. D., Schaffer, R. E.,\\nCallaway, E. and Y e a g e r ,C .L .1979b, Electroencephalogram correlates of higher\\ncortical functions,Science, 203, 665–668.\\nEEGmonitoringofcognitiveworkload 127'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='Gevins, A. S., Zeitlin, G. M., Yingling, C. D., Doyle, J. C., Dedon, M. F., Schaffer,\\nR. E., Roumasset, J. T.and Yeager, C. L.1979c, EEG patterns during ‘cognitive’\\ntasks. I. Methodology and analysis of complex behaviors,Electroencephalography and\\nClinicalNeurophysiology, 47, 693–703.\\nGilliam,F.,Kuzniecky,R. and Faught,E. 1999, Ambulatory EEG monitoring,Journalof\\nClinicalNeurophysiology, 16, 111–115.\\nGoldman-Rakic,P. 1987, Circuitry of primate prefrontal cortex and regulation of behavior\\nby representational memory, in F. Plum and V. Mountcastle (ed.), Handbook of\\nPhysiology, The Nervous System—Higher Functions of the Brain,1 st edn, Vol. 5\\n(Bethesda, MD: American Physiological Society), 373–417.\\nGoldman-Rakic,P. 1988, Topography of cognition: parallel distributed networks in primate\\nassociation cortex,AnnualReviewofNeuroscience , 11, 137–156.\\nGundel,A. and Wilson,G.F. 1992, Topographical changes in the ongoing EEG related to\\nthe diﬃculty of mental tasks,BrainTopography, 5, 17–25.\\nHumphrey, D. and K r a m e r ,A .F .1994, Toward a psychophysiological assessment of\\ndynamic changes in mental workload,HumanFactors, 36, 3–26.\\nInouye, T., Shinosaki, K., Iyama, A., Matsumoto, Y., Toi, S.and Ishihara, T. 1994,\\nPotential ﬂow of frontal midline theta activity during a mental task in the human\\nelectroencephalogram, NeuroscienceLetters, 169, 145–148.\\nIshii,R.,Shinosaki,K.,Ukai,S.,Inouye,T.,Ishihara,T.,Yoshimine,T.,Hirabuki,N.,\\nAsada, H., Kihara, T., Robinson, S. E.and Takeda, M. 1999, Medial prefrontal\\ncortex generates frontal midline theta rhythm,Neuroreport, 10, 675–679.\\nI s r e a l ,J .B . ,W i c ke n s ,C .D . ,C h e s n e y ,G .L .and Donchin, E. 1980, The event-related\\nbrain potential as an index of display-monitoring workload,Human Factors, 22, 211–\\n224.\\nJansma,J.M.,Ramsey,N.F.,Coppola,R. andKahn,R.S. 2000, Speciﬁc versus nonspeciﬁc\\nbrain activity in a parametric n-back task,Neuroimage, 12, 688–697.\\nJohn, E. R., Prichep, L. S., Kox, W., Valdes-Sosa, P., Bosch-Bayard, J., Aubert, E.,\\nTom,M.,diMichele,F. and Gugino,L.D. 2001, Invariant reversible QEEG eﬀects\\nof anesthetics,ConsciousnessandCognition , 10, 165–183.\\nJonides,J.,Smith,E.E.,Koeppe,R.A.,Awh,E.,Minoshima,S. and Mintun,M. 1993,\\nSpatial working memory in humans as revealed by PET,Nature, 363, 623–625.\\nJ o s e p h ,R .D .1961, Contributions of perceptron theory, unpublished PhD thesis, Cornell\\nUniversity, Ithaca, New York.\\nJ u n g ,T .P . ,M a ke i g ,S . ,H u m p h r i e s ,C . ,L e e ,T .W . ,M c K e o w n ,M .J . ,I r a g u i ,V .and\\nSejnowski, T. J. 2000, Removing electroencephalographic artifacts by blind source\\nseparation, Psychophysiology, 37, 163–178.\\nKennedy, J. L., Gottsdanker, R. M., Arinington, J. C.and Gray, F. E.1948, A new\\nelectroencephalogram associated with thinking,Science, 108, 527.\\nKieras,D.E. 1988, Towards a practical GOMS model methodology for user interface design,\\nin M. Helander (ed.), The Handbook of Human–Computer Interaction(Amsterdam:\\nNorth-Holland), 135–158.\\nKoivisto, M., Krause, C. M., Revonsuo, A., Laine, M.and Hamalainen, H. 2000, The\\neﬀects of electromagnetic ﬁeld emitted by GSM phones on working memory,\\nNeuroreport, 11, 1641–1643.\\nKok, A. 2001, On the utility of P3 amplitude as a measure of processing capacity,\\nPsychophysiology, 38, 557–577.\\nKramer,A.F.,Trejo,L.J. and Humphrey,D. 1995, Assessment of mental workload with\\ntask-irrelevant auditory probes,BiologicalPsychology, 40, 83–100.\\nKyllonen, P. C.and Christal, R. E.1990, Reasoning ability is little more than working\\nmemory capacity?!,Intelligence, 14, 389–433.\\nKyllonen,P.C. and Shute,V.J. 1989, A taxonomy of learning skills, in P. L. Ackerman\\n(ed.), LearningandIndividualDiﬀerences (New York: Freeman), 117–163.\\nLarson, C. L., Davidson, R. J., Abercrombie, H. C., Ward, R. T., Schaefer, S. M.,\\nJackson, D. C., Holden, J. E.and Perlman, S. B.1998, Relations between PET-\\nderived measures of thalamic glucose metabolism and EEG alpha power,\\nPsychophysiology, 35, 162–169.\\n128 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='McCallum, W. C., Cooper, R.and Pocock, P. V.1988, Brain slow potential and ERP\\nchanges associated with operator load in a visual tracking task,Electroencephalography\\nandclinicalNeurophysiology , 69, 453–468.\\nM c C a r t h y ,G . ,B l a m i r e ,A .M . ,P u c e ,A . ,N o b r e ,A .C . ,B l o c h ,G . ,H y d e r ,F . ,\\nGoldman-Rakic, P. and Shulman, R. G. 1994, Functional magnetic resonance\\nimaging of human prefrontal cortex activation during a spatial working memory\\ntask, ProceedingsoftheNationalAcademyofScience(USA) , 91, 8690–8694.\\nMcElree, B. 2001, Working memory and focal attention, Journal of Experimental\\nPsychology:Learning, MemoryandCognition , 27, 817–835.\\nMcEvoy,L.K.,Pellouchoud,E.,Smith,M.E. and Gevins,A. 2001, Neurophysiological\\nsignals of working memory in normal aging,CognitiveBrainResearch , 11, 363–376.\\nMcEvoy,L.K.,Smith,M.E. and Gevins,A. 1998, Dynamic cortical networks of verbal and\\nspatial working memory: eﬀects of memory load and task practice,CerebralCortex, 8,\\n563–574.\\nMcEvoy,L.K.,Smith,M.E. and Gevins,A. 2000, Test-retest reliability of cognitive EEG,\\nClinicalNeurophysiology, 111, 457–463.\\nMiyata, Y., Tanaka, Y.and Hono, T.1990, Long term observation on Fm-theta during\\nmental eﬀort,Neuroscience, 16, 145–148.\\nMizuki,Y.,Tanaka,M.,Iogaki,H.,Nishijima,H. andInanaga,K. 1980, Periodic appear-\\nances of theta rhythm in the frontal midline area during performance of a mental task,\\nElectroencephalographyandClinicalNeurophysiology , 49, 345–351.\\nMolloy, R. and Parasuraman, R. 1996, Monitoring an automated system for a single\\nfailure: vigilance and task complexity eﬀects,HumanFactors, 38, 311–322.\\nMorrison,J.G. and Gluckman,J.P. 1994, Deﬁnitions and prospective guidelines for the\\napplication of automation, in M. Mouloua and R. Parasuraman (eds),Human perfor-\\nmance in automated systems: Current research and trends(Hillsdale, NJ: Lawrence\\nErlbaum Associates), 256–263.\\nMulholland, T. 1995, Human EEG, behavioral stillness and biofeedback, International\\nJournalofPsychology , 19, 263–279.\\nO’Connor,M.F.,Daves,S.M.,Tung,A.,Cook,R.I.,Thisted,R. and Appelbaum,J.\\n2001, BIS monitoring to prevent awareness during general anesthesia,Anesthesiology,\\n94, 520–522.\\nO l i v e r i ,M . ,T u r r i z i a n i ,P . ,C a r l e s i m o ,G .A . ,K o c h ,G . ,T o m a i u o l o ,F . ,P a n e l l a ,M .\\nand Caltagirone, C. 2001, Parieto-frontal interactions in visual-object and visual-\\nspatial working memory: evidence for transcranial magnetic stimulation, Cerebral\\nCortex, 11, 606–618.\\nOlson,J.R. and Olson,G.M. 1990, The growth of cognitive modeling in human-computer\\ninteraction since GOMS,Human-ComputerInteraction, 5, 221-265.\\nParasuraman,R.,Molloy,R. and Singh,I.L. 1993, Performance consequences of auto-\\nmation-induced ‘complacency’,InternationalJournalofAviationPsychology , 3, 1–23.\\nParasuraman,R.,Mouloua,M. and Molloy,R. 1996, Eﬀects of adaptive task allocation\\non monitoring of automated systems,HumanFactors, 38, 665–679.\\nParasuraman,R.,Sheridan,T.B. and Wickens,C.D. 2000, A model for types and levels\\nof human interaction with automation, IEEE Transactions Systems, Man, and\\nCybernetics-Part A:SystemsandHumans , 30, 286–297.\\nPaus, T., Koski, L., Caramanos, Z.and Westbury, C. 1998, Regional diﬀerences in the\\neﬀects of task diﬃculty and motor output on blood ﬂow response in the human anterior\\ncingulate cortex: a review of 107 PET activation studies,Neuroreport, 9, R37–R47.\\nP e l l o u c h o u d ,E . ,S m i t h ,M .E . ,M c E v o y ,L .and Gevins, A.1999, Mental eﬀort-related\\nEEG modulation during video-game play: comparison between juvenile subjects with\\nepilepsy and normal control subjects,Epilepsia, 40 (Suppl 4), 38–43.\\nPfurtscheller, G. and Klimesch, W. 1992, Functional topography during a visuoverbal\\njudgment task studied with event-related desynchronization mapping, Journal of\\nClinicalNeurophysiology, 9, 120–131.\\nPosner,M.I. and Peterson,S.E. 1990, The attention system of the human brain,Annual\\nReview ofNeuroscience, 13, 25–42.\\nEEGmonitoringofcognitiveworkload 129'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='Posner,M.I. andRothbart,M.K. 1992, Attentional mechanisms and conscious experience,\\nin A. D. Milner and M. D. Rugg (eds),The Neuropsychology of Consciousness(San\\nDiego: Academic Press), 91–111.\\nRaskin,J. 2000, HumaneInterface:NewDirectionsforDesigningInteractiveSystems (Boston,\\nMA: Addison-Wesley).\\nRockstroh, B., Elbert, T., Canavan, A., Lutzenberger, W.and Birbaumer, N. 1989,\\nSlowcorticalpotentialsandbehavior (Baltimore: Urban & Schwarzenberg).\\nRoss, P. and Segalowitz, S. J.2000, An EEG coherence test of the frontal dorsal versus\\nventral hypothesis of n-back working memory,BrainandCognition , 43, 375–379.\\nSadato, N., Nakamura, S., Oohashi, T., Nishina, E., Fuwamoto, Y., Waki, A. and\\nYonekura, Y. 1998, Neural networks for generation and suppression of alpha\\nrhythm: a PET study,Neuroreport, 30, 893–897.\\nSheer,D.E. 1989, Sensory and cognitive 40Hz event-related potentials, in E. Basar and T. H.\\nBullock (eds),BrainDynamics, Vol. 2 (Berlin: Springer), 339–374.\\nSirevaag,E.J.,Kramer,A.F. and Wickens,C.D. 1993, Assessment of pilot performance\\nand mental workload in rotary wing aircraft,Ergonomics, 36, 1121–1140.\\nSmith,M.E.,Gevins,A.,Brown,H.,Karnik,A. and Du,R. 2001, Monitoring task load\\nwith multivariate EEG measures during complex forms of human computer interaction,\\nHumanFactors, 43, 366–380.\\nSmith,M.E.,McEvoy,L.K. and Gevins,A. 1999, Neurophysiological indices of strategy\\ndevelopment and skill acquisition,BrainResearchCognitiveBrainResearch , 7, 389–404.\\nThompson,J.L. and Ebersole,J.S. 1999, Longterm inpatient audiovisual scalp EEG mon-\\nitoring, JournalofClinicalNeurophysiology , 16, 91–99.\\nTou, J. T. and G o n z a l e z ,R .C .1974, Pattern Recognition Principles(Reading, MA:\\nAddison-Wesley Publishing Co).\\nUllsperger,P.,Freude,G. and Erdmann,U. 2001, Auditory probe sensitivity to mental\\nworkload changes—an event-related potential study, International Journal of\\nPsychophysiology, 40, 201–209.\\nVandenBerg-Lensssen,M.M.,Brunia,C.H. and Blom,J.A. 1989, Correction of ocular\\nartifacts in EEGs using an autoregressive model to describe the EEG: a pilot study,\\nElectroencephalographyandClinicalNeurophysiology , 73, 72–83.\\nVespa,P.,Nenov,V. and Nuwer,M.R. 1999, Continuous EEG monitoring in the intensive\\ncare unit: early ﬁndings and clinical eﬃcacy,Journal of Clinical Neurophysiology, 16,\\n1–13.\\nViglione, S.S. 1970, Applications of pattern recognition technology, in J. M. Mendel and\\nK. S. Fu (eds), Adaptive Learning and Pattern Recognition Systems(New York:\\nAcademic Press), 115–161.\\nWilson, G. F.and Fisher, F. 1995, Cognitive task classiﬁcation based upon topographic\\nEEG data,BiologicalPsychology, 40, 239–250.\\nWilson,G.F.,Fullenkamp,B.S. and Davis,I. 1994, Evoked potential, cardiac, blink, and\\nrespiration measures of pilot workload in air-to-ground missions,Aviation, Space and\\nEnvironmentalMedicine, 65, 100–105.\\nWintink,A.J.,Segalowitz,S.J. and Cudmore,L.J. 2001, Task complexity and habitua-\\ntion eﬀects on frontal P300 topography,BrainandCognition , 46, 307–311.\\nW o l t z ,D .J .1988, An investigation of the role of working memory in procedural skill\\nacquisition, JournalofExperimentalPsychology:General , 117, 319–331.\\nYamamoto,S. and Matsuoka,S. 1990, Topographic EEG study of visual display terminal\\nVDT performance with special reference to frontal midline theta waves, Brain\\nTopography, 2, 257–267.\\nAbout the authors\\nAlan Gevinsis the founder and Executive Director of the San Francisco Brain Research\\nInstitute and the founder and President of SAM Technology, Inc., both in San Francisco.\\nHe is internationally known for developing algorithms and systems for analysing human brain\\nfunction, and for basic science studies of human neurocognitive functions. He is the author of\\nover 125 scientiﬁc papers and of 16 US patents.\\n130 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 19, 'page_label': '18'}, page_content='MichaelE.Smith is a cognitive neuroscientist who specializes in basic and applied research on\\nthe neural systems mediating human attention and memory. He has authored over 50 scientiﬁc\\npapers on related topics. He holds an undergraduate degree from the University of Michigan,\\na PhD from the University of California, Los Angeles, and an MBA from the University of\\nCalifornia, Berkeley. He currently directs the research department of SAM Technology, Inc.\\nEEGmonitoringofcognitiveworkload 131'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content=\"R-CNN: Revolutionizing \\nObject Detection\\nThis presentation introduces R-CNN (Regions with Convolutional Neural \\nNetworks), a pioneering deep learning approach that transformed object \\ndetection. We'll explore its innovative architecture and the profound impact it \\nhad on the field.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='The Fundamental Challenge: \\n\"What + Where\"\\nObject detection is more than just classification; it\\'s about simultaneously \\nidentifying \"what\" objects are present in an image and \"where\" they are located. \\nBefore R-CNN, performance in object localization struggled to advance, facing a \\nsignificant plateau. The key challenge was efficiently locating objects within an \\nimage using deep learning.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='Evolution of Approaches\\nBrute Force: The Sliding Window\\nEarly methods relied on a sliding window approach, \\nexhaustively checking every possible region. While conceptually \\nsimple, this method proved computationally prohibitive for \\ncomplex deep learning models like CNNs due to the immense \\nnumber of regions to process.\\nSmarter Approach: Region Proposals\\nR-CNN introduced a paradigm shift: Region Proposals. Instead \\nof brute force, it generates a sparse set of potential object \\nlocations, dramatically reducing computational load. Combined \\nwith deep learning, this paved the way for modern object \\ndetection.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content=\"R-CNN: A Three-Module Pipeline\\nThe R-CNN architecture is elegantly structured into three distinct, yet interconnected, modules. This pipeline processes an input image to \\nultimately identify and precisely localize objects. Let's delve into each module to understand its contribution.\\n01\\n1. Region Proposals\\nGenerating candidate object locations.\\n02\\n2. Feature Extraction\\nExtracting rich features for each proposal.\\n03\\n3. Classification & Refinement\\nClassifying objects and fine-tuning their \\npositions.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='Module 1: Region Proposals & IoU\\nGenerating Candidate Regions with Selective \\nSearch\\nThe first module employs Selective Search to generate approximately \\n2000 region proposals per image. This algorithm intelligently groups \\nsimilar pixels into potential object segments, acting as \"intelligent \\nguesses\" for object locations.\\nIntersection over Union (IoU)\\nT o evaluate the quality of these proposals and the accuracy of our \\ndetections, we use Intersection over Union (IoU). This metric quantifies \\nthe overlap between a predicted bounding box and the ground-truth \\nbounding box.\\nIoU is crucial for both training (determining \\npositive/negative samples) and evaluation (assessing \\ndetection accuracy).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Module 2: Feature Extraction \\nwith AlexNet\\nOnce region proposals are generated, they are warped to a fixed size and fed \\ninto a powerful Convolutional Neural Network (CNN). R-CNN famously \\nleveraged AlexNet, a groundbreaking CNN architecture.\\nTransfer Learning: AlexNet, pre-trained on the vast ImageNet dataset, was \\nfine-tuned on the Pascal VOC dataset for object detection.\\nFeature Vector Generation: Each warped region proposal is passed through \\nthe CNN, yielding a 4096-dimensional feature vector. These high-level \\nfeatures capture semantic information about the potential object.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content=\"Module 3: Classification & Bounding Box Regression\\nObject Classification with SoftMax\\nThe 4096-dimensional feature vector for each region proposal \\nis then fed into a class-specific linear SVM classifier. This \\nclassifier determines the presence and specific class of an \\nobject within that proposal. A SoftMax layer outputs the \\nprobability distribution over all possible object classes.\\nBounding Box Regression for Precision\\nT o achieve highly accurate localization, R-CNN incorporates a \\nbounding box regressor. This linear regression model refines \\nthe initial region proposal's coordinates, predicting offsets that \\nadjust the box to tightly enclose the object. This step \\nsignificantly improves localization precision.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content=\"Datasets & Strategic Training\\nR-CNN's success was greatly attributed to its sophisticated training strategy, \\nleveraging large, diverse datasets and the power of transfer learning.\\nImageNet (1.2 Million Images): Initial pre-training of the AlexNet CNN on \\nthis massive dataset allowed the model to learn robust, generalizable visual \\nfeatures.\\nPascal VOC (10,000 Images): This smaller, object detection-specific dataset \\nwas then used for fine-tuning the pre-trained CNN, adapting its learned \\nfeatures to the nuances of object localization.\\nTransfer learning was a critical component, enabling R-CNN to achieve high \\naccuracy with relatively less training data compared to training from scratch.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='Impact & Legacy: A New Era of Detection\\n53.7%\\nmAP Improvement\\nR-CNN achieved a remarkable 53.7% mean Average Precision \\n(mAP) on the Pascal VOC 2012 dataset, a massive 30% relative \\nimprovement over prior state-of-the-art methods.\\n4\\nFoundational Work\\nThis groundbreaking performance cemented R-CNN as a \\nfoundational work, inspiring a wave of subsequent research and \\nforming the basis for modern object detection architectures like \\nFast R-CNN and Faster R-CNN.\\nR-CNN demonstrated that deep learning could effectively tackle the complex problem of object detection, setting a new benchmark and \\nopening up vast research avenues.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='Further Exploration\\nThe techniques introduced in R-CNN, particularly region proposals and fine-\\ntuning CNNs for detection, remain highly influential.\\n\"Rich feature hierarchies for accurate object detection and semantic \\nsegmentation\" (R. Girshick et al.)\\nWe encourage you to explore the original research paper for a deeper dive into \\nthe mathematical and implementation details of this seminal work.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "## Load all the files form a directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", # Pattern to match files\n",
    "    loader_cls= PyPDFLoader, # loader class to use can provide multiple cls by specifying it in list eg:[TextLoader,PyPDFLoader]\n",
    "    # loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=True\n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4303b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42facdb5",
   "metadata": {},
   "source": [
    "### Text splitting get into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c0a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Data Chunks \n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Maximum characters per chunk (adjust based on your LLM)\n",
    "    - chunk_overlap: Characters to overlap between chunks (preserves context)\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Each chunk: ~1000 characters\n",
    "        chunk_overlap=chunk_overlap, # 200 chars overlap for context\n",
    "        length_function=len, # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Split hierarchy\n",
    "    )\n",
    "    # Actually split the documents\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show what a chunk looks like\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac4a01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 51 documents into 203 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Rich feature hierarchies for accurate object detection and semantic segmentation\n",
      "Tech report (v5)\n",
      "Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik\n",
      "UC Berkeley\n",
      "{rbg,jdonahue,trevor,malik}@eecs....\n",
      "Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='works (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,\\nfollowed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant\\nperformance boost. Since we combine region proposals\\nwith CNNs, we call our method R-CNN: Regions with CNN\\nfeatures. We also compare R-CNN to OverFeat, a recently\\nproposed sliding-window detector based on a similar CNN\\narchitecture. We ﬁnd that R-CNN outperforms OverFeat\\nby a large margin on the 200-class ILSVRC2013 detection\\ndataset. Source code for the complete system is available at\\nhttp://www.cs.berkeley.edu/˜rbg/rcnn.\\n1. Introduction\\nFeatures matter. The last decade of progress on various\\nvisual recognition tasks has been based considerably on the\\nuse of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='use of SIFT [29] and HOG [7]. But if we look at perfor-\\nmance on the canonical visual recognition task, PASCAL\\nVOC object detection [15], it is generally acknowledged\\nthat progress has been slow during 2010-2012, with small\\ngains obtained by building ensemble systems and employ-\\ning minor variants of successful methods.\\nSIFT and HOG are blockwise orientation histograms,\\na representation we could associate roughly with complex\\ncells in V1, the ﬁrst cortical area in the primate visual path-\\nway. But we also know that recognition occurs several\\nstages downstream, which suggests that there might be hier-\\n1. Input \\nimage\\n2. Extract region \\nproposals (~2k)\\n3. Compute \\nCNN features\\naeroplane? no.\\n...\\nperson? yes.\\ntvmonitor? no.\\n4. Classify \\nregions\\nwarped region\\n ...\\nCNN\\nR-CNN: Regions with CNN features\\nFigure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Figure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN achieves a mean\\naverage precision (mAP) of 53.7% on PASCAL VOC 2010. For\\ncomparison, [39] reports 35.1% mAP using the same region pro-\\nposals, but with a spatial pyramid and bag-of-visual-words ap-\\nproach. The popular deformable part models perform at 33.4%.\\nOn the 200-class ILSVRC2013 detection dataset, R-CNN’s\\nmAP is 31.4% , a large improvement over OverFeat [34], which\\nhad the previous best result at 24.3%.\\narchical, multi-stage processes for computing features that\\nare even more informative for visual recognition.\\nFukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Fukushima’s “neocognitron” [19], a biologically-\\ninspired hierarchical and shift-invariant model for pattern\\nrecognition, was an early attempt at just such a process.\\nThe neocognitron, however, lacked a supervised training\\nalgorithm. Building on Rumelhart et al. [33], LeCun et\\nal. [26] showed that stochastic gradient descent via back-\\npropagation was effective for training convolutional neural\\nnetworks (CNNs), a class of models that extend the neocog-\\nnitron.\\nCNNs saw heavy use in the 1990s (e.g., [27]), but then\\nfell out of fashion with the rise of support vector machines.\\nIn 2012, Krizhevsky et al. [25] rekindled interest in CNNs\\nby showing substantially higher image classiﬁcation accu-\\nracy on the ImageNet Large Scale Visual Recognition Chal-\\nlenge (ILSVRC) [9, 10]. Their success resulted from train-\\ning a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='ing a large CNN on 1.2 million labeled images, together\\nwith a few twists on LeCun’s CNN (e.g.,max(x,0) rectify-\\ning non-linearities and “dropout” regularization).\\nThe signiﬁcance of the ImageNet result was vigorously\\n1\\narXiv:1311.2524v5  [cs.CV]  22 Oct 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='debated during the ILSVRC 2012 workshop. The central\\nissue can be distilled to the following: To what extent do\\nthe CNN classiﬁcation results on ImageNet generalize to\\nobject detection results on the PASCAL VOC Challenge?\\nWe answer this question by bridging the gap between\\nimage classiﬁcation and object detection. This paper is the\\nﬁrst to show that a CNN can lead to dramatically higher ob-\\nject detection performance on PASCAL VOC as compared\\nto systems based on simpler HOG-like features. To achieve\\nthis result, we focused on two problems: localizing objects\\nwith a deep network and training a high-capacity model\\nwith only a small quantity of annotated detection data.\\nUnlike image classiﬁcation, detection requires localiz-\\ning (likely many) objects within an image. One approach\\nframes localization as a regression problem. However, work\\nfrom Szegedy et al. [38], concurrent with our own, indi-\\ncates that this strategy may not fare well in practice (they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='frames localization as a regression problem. However, work\\nfrom Szegedy et al. [38], concurrent with our own, indi-\\ncates that this strategy may not fare well in practice (they\\nreport a mAP of 30.5% on VOC 2007 compared to the\\n58.5% achieved by our method). An alternative is to build a\\nsliding-window detector. CNNs have been used in this way\\nfor at least two decades, typically on constrained object cat-\\negories, such as faces [32, 40] and pedestrians [35]. In order\\nto maintain high spatial resolution, these CNNs typically\\nonly have two convolutional and pooling layers. We also\\nconsidered adopting a sliding-window approach. However,\\nunits high up in our network, which has ﬁve convolutional\\nlayers, have very large receptive ﬁelds ( 195 ×195 pixels)\\nand strides (32×32 pixels) in the input image, which makes\\nprecise localization within the sliding-window paradigm an\\nopen technical challenge.\\nInstead, we solve the CNN localization problem by oper-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='and strides (32×32 pixels) in the input image, which makes\\nprecise localization within the sliding-window paradigm an\\nopen technical challenge.\\nInstead, we solve the CNN localization problem by oper-\\nating within the “recognition using regions” paradigm [21],\\nwhich has been successful for both object detection [39] and\\nsemantic segmentation [5]. At test time, our method gener-\\nates around 2000 category-independent region proposals for\\nthe input image, extracts a ﬁxed-length feature vector from\\neach proposal using a CNN, and then classiﬁes each region\\nwith category-speciﬁc linear SVMs. We use a simple tech-\\nnique (afﬁne image warping) to compute a ﬁxed-size CNN\\ninput from each region proposal, regardless of the region’s\\nshape. Figure 1 presents an overview of our method and\\nhighlights some of our results. Since our system combines\\nregion proposals with CNNs, we dub the method R-CNN:\\nRegions with CNN features.\\nIn this updated version of this paper, we provide a head-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='highlights some of our results. Since our system combines\\nregion proposals with CNNs, we dub the method R-CNN:\\nRegions with CNN features.\\nIn this updated version of this paper, we provide a head-\\nto-head comparison of R-CNN and the recently proposed\\nOverFeat [34] detection system by running R-CNN on the\\n200-class ILSVRC2013 detection dataset. OverFeat uses a\\nsliding-window CNN for detection and until now was the\\nbest performing method on ILSVRC2013 detection. We\\nshow that R-CNN signiﬁcantly outperforms OverFeat, with\\na mAP of 31.4% versus 24.3%.\\nA second challenge faced in detection is that labeled data\\nis scarce and the amount currently available is insufﬁcient\\nfor training a large CNN. The conventional solution to this\\nproblem is to useunsupervised pre-training, followed by su-\\npervised ﬁne-tuning (e.g., [35]). The second principle con-\\ntribution of this paper is to show thatsupervised pre-training\\non a large auxiliary dataset (ILSVRC), followed by domain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='pervised ﬁne-tuning (e.g., [35]). The second principle con-\\ntribution of this paper is to show thatsupervised pre-training\\non a large auxiliary dataset (ILSVRC), followed by domain-\\nspeciﬁc ﬁne-tuning on a small dataset (PASCAL), is an\\neffective paradigm for learning high-capacity CNNs when\\ndata is scarce. In our experiments, ﬁne-tuning for detection\\nimproves mAP performance by 8 percentage points. After\\nﬁne-tuning, our system achieves a mAP of 54% on VOC\\n2010 compared to 33% for the highly-tuned, HOG-based\\ndeformable part model (DPM) [17, 20]. We also point read-\\ners to contemporaneous work by Donahue et al. [12], who\\nshow that Krizhevsky’s CNN can be used (without ﬁne-\\ntuning) as a blackbox feature extractor, yielding excellent\\nperformance on several recognition tasks including scene\\nclassiﬁcation, ﬁne-grained sub-categorization, and domain\\nadaptation.\\nOur system is also quite efﬁcient. The only class-speciﬁc\\ncomputations are a reasonably small matrix-vector product'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='classiﬁcation, ﬁne-grained sub-categorization, and domain\\nadaptation.\\nOur system is also quite efﬁcient. The only class-speciﬁc\\ncomputations are a reasonably small matrix-vector product\\nand greedy non-maximum suppression. This computational\\nproperty follows from features that are shared across all cat-\\negories and that are also two orders of magnitude lower-\\ndimensional than previously used region features (cf. [39]).\\nUnderstanding the failure modes of our approach is also\\ncritical for improving it, and so we report results from the\\ndetection analysis tool of Hoiem et al. [23]. As an im-\\nmediate consequence of this analysis, we demonstrate that\\na simple bounding-box regression method signiﬁcantly re-\\nduces mislocalizations, which are the dominant error mode.\\nBefore developing technical details, we note that because\\nR-CNN operates on regions it is natural to extend it to the\\ntask of semantic segmentation. With minor modiﬁcations,\\nwe also achieve competitive results on the PASCAL VOC'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='R-CNN operates on regions it is natural to extend it to the\\ntask of semantic segmentation. With minor modiﬁcations,\\nwe also achieve competitive results on the PASCAL VOC\\nsegmentation task, with an average segmentation accuracy\\nof 47.9% on the VOC 2011 test set.\\n2. Object detection with R-CNN\\nOur object detection system consists of three modules.\\nThe ﬁrst generates category-independent region proposals.\\nThese proposals deﬁne the set of candidate detections avail-\\nable to our detector. The second module is a large convo-\\nlutional neural network that extracts a ﬁxed-length feature\\nvector from each region. The third module is a set of class-\\nspeciﬁc linear SVMs. In this section, we present our design\\ndecisions for each module, describe their test-time usage,\\ndetail how their parameters are learned, and show detection\\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\\n2.1. Module design\\nRegion proposals. A variety of recent papers offer meth-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2'}, page_content='detail how their parameters are learned, and show detection\\nresults on PASCAL VOC 2010-12 and on ILSVRC2013.\\n2.1. Module design\\nRegion proposals. A variety of recent papers offer meth-\\nods for generating category-independent region proposals.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='aeroplane bicycle bird car\\nFigure 2: Warped training samples from VOC 2007 train.\\nExamples include: objectness [1], selective search [39],\\ncategory-independent object proposals [14], constrained\\nparametric min-cuts (CPMC) [5], multi-scale combinatorial\\ngrouping [3], and Cires ¸an et al. [6], who detect mitotic cells\\nby applying a CNN to regularly-spaced square crops, which\\nare a special case of region proposals. While R-CNN is ag-\\nnostic to the particular region proposal method, we use se-\\nlective search to enable a controlled comparison with prior\\ndetection work (e.g., [39, 41]).\\nFeature extraction. We extract a 4096-dimensional fea-\\nture vector from each region proposal using the Caffe [24]\\nimplementation of the CNN described by Krizhevsky et\\nal. [25]. Features are computed by forward propagating\\na mean-subtracted 227 ×227 RGB image through ﬁve con-\\nvolutional layers and two fully connected layers. We refer\\nreaders to [24, 25] for more network architecture details.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='a mean-subtracted 227 ×227 RGB image through ﬁve con-\\nvolutional layers and two fully connected layers. We refer\\nreaders to [24, 25] for more network architecture details.\\nIn order to compute features for a region proposal, we\\nmust ﬁrst convert the image data in that region into a form\\nthat is compatible with the CNN (its architecture requires\\ninputs of a ﬁxed 227 ×227 pixel size). Of the many possi-\\nble transformations of our arbitrary-shaped regions, we opt\\nfor the simplest. Regardless of the size or aspect ratio of the\\ncandidate region, we warp all pixels in a tight bounding box\\naround it to the required size. Prior to warping, we dilate the\\ntight bounding box so that at the warped size there are ex-\\nactly ppixels of warped image context around the original\\nbox (we use p = 16). Figure 2 shows a random sampling\\nof warped training regions. Alternatives to warping are dis-\\ncussed in Appendix A.\\n2.2. Test-time detection\\nAt test time, we run selective search on the test image'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='of warped training regions. Alternatives to warping are dis-\\ncussed in Appendix A.\\n2.2. Test-time detection\\nAt test time, we run selective search on the test image\\nto extract around 2000 region proposals (we use selective\\nsearch’s “fast mode” in all experiments). We warp each\\nproposal and forward propagate it through the CNN in or-\\nder to compute features. Then, for each class, we score\\neach extracted feature vector using the SVM trained for that\\nclass. Given all scored regions in an image, we apply a\\ngreedy non-maximum suppression (for each class indepen-\\ndently) that rejects a region if it has an intersection-over-\\nunion (IoU) overlap with a higher scoring selected region\\nlarger than a learned threshold.\\nRun-time analysis. Two properties make detection efﬁ-\\ncient. First, all CNN parameters are shared across all cate-\\ngories. Second, the feature vectors computed by the CNN\\nare low-dimensional when compared to other common ap-\\nproaches, such as spatial pyramids with bag-of-visual-word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='gories. Second, the feature vectors computed by the CNN\\nare low-dimensional when compared to other common ap-\\nproaches, such as spatial pyramids with bag-of-visual-word\\nencodings. The features used in the UV A detection system\\n[39], for example, are two orders of magnitude larger than\\nours (360k vs. 4k-dimensional).\\nThe result of such sharing is that the time spent com-\\nputing region proposals and features (13s/image on a GPU\\nor 53s/image on a CPU) is amortized over all classes. The\\nonly class-speciﬁc computations are dot products between\\nfeatures and SVM weights and non-maximum suppression.\\nIn practice, all dot products for an image are batched into\\na single matrix-matrix product. The feature matrix is typi-\\ncally 2000×4096 and the SVM weight matrix is4096×N,\\nwhere N is the number of classes.\\nThis analysis shows that R-CNN can scale to thousands\\nof object classes without resorting to approximate tech-\\nniques, such as hashing. Even if there were 100k classes,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='This analysis shows that R-CNN can scale to thousands\\nof object classes without resorting to approximate tech-\\nniques, such as hashing. Even if there were 100k classes,\\nthe resulting matrix multiplication takes only 10 seconds on\\na modern multi-core CPU. This efﬁciency is not merely the\\nresult of using region proposals and shared features. The\\nUV A system, due to its high-dimensional features, would\\nbe two orders of magnitude slower while requiring 134GB\\nof memory just to store 100k linear predictors, compared to\\njust 1.5GB for our lower-dimensional features.\\nIt is also interesting to contrast R-CNN with the recent\\nwork from Dean et al. on scalable detection using DPMs\\nand hashing [8]. They report a mAP of around 16% on VOC\\n2007 at a run-time of 5 minutes per image when introducing\\n10k distractor classes. With our approach, 10k detectors can\\nrun in about a minute on a CPU, and because no approxi-\\nmations are made mAP would remain at 59% (Section 3.2).\\n2.3. Training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='10k distractor classes. With our approach, 10k detectors can\\nrun in about a minute on a CPU, and because no approxi-\\nmations are made mAP would remain at 59% (Section 3.2).\\n2.3. Training\\nSupervised pre-training. We discriminatively pre-trained\\nthe CNN on a large auxiliary dataset (ILSVRC2012 clas-\\nsiﬁcation) using image-level annotations only (bounding-\\nbox labels are not available for this data). Pre-training\\nwas performed using the open source Caffe CNN library\\n[24]. In brief, our CNN nearly matches the performance\\nof Krizhevsky et al. [25], obtaining a top-1 error rate 2.2\\npercentage points higher on the ILSVRC2012 classiﬁcation\\nvalidation set. This discrepancy is due to simpliﬁcations in\\nthe training process.\\nDomain-speciﬁc ﬁne-tuning. To adapt our CNN to the\\nnew task (detection) and the new domain (warped proposal\\nwindows), we continue stochastic gradient descent (SGD)\\ntraining of the CNN parameters using only warped region\\nproposals. Aside from replacing the CNN’s ImageNet-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3'}, page_content='windows), we continue stochastic gradient descent (SGD)\\ntraining of the CNN parameters using only warped region\\nproposals. Aside from replacing the CNN’s ImageNet-\\nspeciﬁc 1000-way classiﬁcation layer with a randomly ini-\\ntialized (N + 1)-way classiﬁcation layer (where N is the\\nnumber of object classes, plus 1 for background), the CNN\\narchitecture is unchanged. For VOC, N = 20 and for\\nILSVRC2013, N = 200. We treat all region proposals with\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='≥0.5 IoU overlap with a ground-truth box as positives for\\nthat box’s class and the rest as negatives. We start SGD at\\na learning rate of 0.001 (1/10th of the initial pre-training\\nrate), which allows ﬁne-tuning to make progress while not\\nclobbering the initialization. In each SGD iteration, we uni-\\nformly sample 32 positive windows (over all classes) and\\n96 background windows to construct a mini-batch of size\\n128. We bias the sampling towards positive windows be-\\ncause they are extremely rare compared to background.\\nObject category classiﬁers. Consider training a binary\\nclassiﬁer to detect cars. It’s clear that an image region\\ntightly enclosing a car should be a positive example. Simi-\\nlarly, it’s clear that a background region, which has nothing\\nto do with cars, should be a negative example. Less clear\\nis how to label a region that partially overlaps a car. We re-\\nsolve this issue with an IoU overlap threshold, below which\\nregions are deﬁned as negatives. The overlap threshold,0.3,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='is how to label a region that partially overlaps a car. We re-\\nsolve this issue with an IoU overlap threshold, below which\\nregions are deﬁned as negatives. The overlap threshold,0.3,\\nwas selected by a grid search over {0,0.1,..., 0.5}on a\\nvalidation set. We found that selecting this threshold care-\\nfully is important. Setting it to 0.5, as in [39], decreased\\nmAP by 5 points. Similarly, setting it to 0 decreased mAP\\nby 4 points. Positive examples are deﬁned simply to be the\\nground-truth bounding boxes for each class.\\nOnce features are extracted and training labels are ap-\\nplied, we optimize one linear SVM per class. Since the\\ntraining data is too large to ﬁt in memory, we adopt the\\nstandard hard negative mining method [17, 37]. Hard neg-\\native mining converges quickly and in practice mAP stops\\nincreasing after only a single pass over all images.\\nIn Appendix B we discuss why the positive and negative\\nexamples are deﬁned differently in ﬁne-tuning versus SVM'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='increasing after only a single pass over all images.\\nIn Appendix B we discuss why the positive and negative\\nexamples are deﬁned differently in ﬁne-tuning versus SVM\\ntraining. We also discuss the trade-offs involved in training\\ndetection SVMs rather than simply using the outputs from\\nthe ﬁnal softmax layer of the ﬁne-tuned CNN.\\n2.4. Results on PASCAL VOC 2010-12\\nFollowing the PASCAL VOC best practices [15], we\\nvalidated all design decisions and hyperparameters on the\\nVOC 2007 dataset (Section 3.2). For ﬁnal results on the\\nVOC 2010-12 datasets, we ﬁne-tuned the CNN on VOC\\n2012 train and optimized our detection SVMs on VOC 2012\\ntrainval. We submitted test results to the evaluation server\\nonly once for each of the two major algorithm variants (with\\nand without bounding-box regression).\\nTable 1 shows complete results on VOC 2010. We com-\\npare our method against four strong baselines, including\\nSegDPM [18], which combines DPM detectors with the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='and without bounding-box regression).\\nTable 1 shows complete results on VOC 2010. We com-\\npare our method against four strong baselines, including\\nSegDPM [18], which combines DPM detectors with the\\noutput of a semantic segmentation system [4] and uses ad-\\nditional inter-detector context and image-classiﬁer rescor-\\ning. The most germane comparison is to the UV A system\\nfrom Uijlings et al. [39], since our systems use the same re-\\ngion proposal algorithm. To classify regions, their method\\nbuilds a four-level spatial pyramid and populates it with\\ndensely sampled SIFT, Extended OpponentSIFT, and RGB-\\nSIFT descriptors, each vector quantized with 4000-word\\ncodebooks. Classiﬁcation is performed with a histogram\\nintersection kernel SVM. Compared to their multi-feature,\\nnon-linear kernel SVM approach, we achieve a large im-\\nprovement in mAP, from 35.1% to 53.7% mAP, while also\\nbeing much faster (Section 2.2). Our method achieves sim-\\nilar performance (53.3% mAP) on VOC 2011/12 test.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='provement in mAP, from 35.1% to 53.7% mAP, while also\\nbeing much faster (Section 2.2). Our method achieves sim-\\nilar performance (53.3% mAP) on VOC 2011/12 test.\\n2.5. Results on ILSVRC2013 detection\\nWe ran R-CNN on the 200-class ILSVRC2013 detection\\ndataset using the same system hyperparameters that we used\\nfor PASCAL VOC. We followed the same protocol of sub-\\nmitting test results to the ILSVRC2013 evaluation server\\nonly twice, once with and once without bounding-box re-\\ngression.\\nFigure 3 compares R-CNN to the entries in the ILSVRC\\n2013 competition and to the post-competition OverFeat re-\\nsult [34]. R-CNN achieves a mAP of 31.4%, which is sig-\\nniﬁcantly ahead of the second-best result of 24.3% from\\nOverFeat. To give a sense of the AP distribution over\\nclasses, box plots are also presented and a table of per-\\nclass APs follows at the end of the paper in Table 8. Most\\nof the competing submissions (OverFeat, NEC-MU, UvA-\\nEuvision, Toronto A, and UIUC-IFP) used convolutional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='class APs follows at the end of the paper in Table 8. Most\\nof the competing submissions (OverFeat, NEC-MU, UvA-\\nEuvision, Toronto A, and UIUC-IFP) used convolutional\\nneural networks, indicating that there is signiﬁcant nuance\\nin how CNNs can be applied to object detection, leading to\\ngreatly varying outcomes.\\nIn Section 4, we give an overview of the ILSVRC2013\\ndetection dataset and provide details about choices that we\\nmade when running R-CNN on it.\\n3. Visualization, ablation, and modes of error\\n3.1. Visualizing learned features\\nFirst-layer ﬁlters can be visualized directly and are easy\\nto understand [25]. They capture oriented edges and oppo-\\nnent colors. Understanding the subsequent layers is more\\nchallenging. Zeiler and Fergus present a visually attrac-\\ntive deconvolutional approach in [42]. We propose a simple\\n(and complementary) non-parametric method that directly\\nshows what the network learned.\\nThe idea is to single out a particular unit (feature) in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4'}, page_content='(and complementary) non-parametric method that directly\\nshows what the network learned.\\nThe idea is to single out a particular unit (feature) in the\\nnetwork and use it as if it were an object detector in its own\\nright. That is, we compute the unit’s activations on a large\\nset of held-out region proposals (about 10 million), sort the\\nproposals from highest to lowest activation, perform non-\\nmaximum suppression, and then display the top-scoring re-\\ngions. Our method lets the selected unit “speak for itself”\\nby showing exactly which inputs it ﬁres on. We avoid aver-\\naging in order to see different visual modes and gain insight\\ninto the invariances computed by the unit.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='VOC 2010 testaero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nDPM v5 [20]† 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4\\nUV A [39] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1\\nRegionlets [41]65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7\\nSegDPM [18]† 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4\\nR-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2\\nR-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7\\nTable 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UV A and Regionlets since all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='Table 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UV A and Regionlets since all\\nmethods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM\\nwas the top-performer on the PASCAL VOC leaderboard. †DPM and SegDPM use context rescoring not used by the other methods.\\n0 20 40 60 80 100\\nUIUC−IFP \\nDelta \\nGPU_UCLA \\nSYSU_Vision \\nToronto A \\n*OverFeat (1) \\n*NEC−MU \\nUvA−Euvision \\n*OverFeat (2) \\n*R−CNN BB \\nmean average precision (mAP) in %\\nILSVRC2013 detection test set mAP\\n \\n \\n1.0%\\n6.1%\\n9.8%\\n10.5%\\n11.5%\\n19.4%\\n20.9%\\n22.6%\\n24.3%\\n31.4%\\ncompetition result\\npost competition result\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n*R−CNN BB\\nUvA−Euvision\\n*NEC−MU\\n*OverFeat (1)\\nToronto A\\nSYSU_Vision\\nGPU_UCLA\\nDelta\\nUIUC−IFP\\naverage precision (AP) in %\\nILSVRC2013 detection test set class AP box plots'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\n*R−CNN BB\\nUvA−Euvision\\n*NEC−MU\\n*OverFeat (1)\\nToronto A\\nSYSU_Vision\\nGPU_UCLA\\nDelta\\nUIUC−IFP\\naverage precision (AP) in %\\nILSVRC2013 detection test set class AP box plots\\nFigure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data\\n(images and labels from the ILSVRC classiﬁcation dataset in all cases). (Right) Box plots for the 200 average precision values per\\nmethod. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for\\nR-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; seeR-CNN-ILSVRC2013-APs.txt). The red\\nline marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each\\nmethod. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5'}, page_content='method. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).\\n1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9\\n1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\nFigure 4: Top regions for six pool5 units. Receptive ﬁelds and activation values are drawn in white. Some units are aligned to concepts,\\nsuch as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reﬂections (6).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='VOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN pool5 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2\\nR-CNN fc6 59.3 61.8 43.1 34.0 25.1 53.1 60.6 52.8 21.7 47.8 42.7 47.8 52.5 58.5 44.6 25.6 48.3 34.0 53.1 58.0 46.2\\nR-CNN fc7 57.6 57.9 38.5 31.8 23.7 51.2 58.9 51.4 20.0 50.5 40.9 46.0 51.6 55.9 43.3 23.3 48.1 35.3 51.0 57.4 44.7\\nR-CNN FT pool5 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3\\nR-CNN FT fc6 63.5 66.0 47.9 37.7 29.9 62.5 70.2 60.2 32.0 57.9 47.0 53.5 60.1 64.2 52.2 31.3 55.0 50.0 57.7 63.0 53.1\\nR-CNN FT fc7 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='R-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nDPM v5 [20] 33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7\\nDPM ST [28] 23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1\\nDPM HSC [31] 32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3\\nTable 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without ﬁne-tuning. Rows 4-6 show\\nresults for the CNN pre-trained on ILSVRC 2012 and then ﬁne-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box\\nregression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The ﬁrst uses\\nonly HOG, while the next two use different feature learning approaches to augment or replace HOG.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='only HOG, while the next two use different feature learning approaches to augment or replace HOG.\\nVOC 2007 test aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP\\nR-CNN T-Net 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2\\nR-CNN T-Net BB68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5\\nR-CNN O-Net 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2\\nR-CNN O-Net BB73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0\\nTable 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The ﬁrst two rows are results from\\nTable 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\\nand Zisserman (O-Net) [43].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='Table 2 using Krizhevsky et al.’s architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan\\nand Zisserman (O-Net) [43].\\nWe visualize units from layer pool 5, which is the max-\\npooled output of the network’s ﬁfth and ﬁnal convolutional\\nlayer. The pool 5 feature map is 6 ×6 ×256 = 9216-\\ndimensional. Ignoring boundary effects, each pool5 unit has\\na receptive ﬁeld of195×195 pixels in the original227×227\\npixel input. A central pool 5 unit has a nearly global view,\\nwhile one near the edge has a smaller, clipped support.\\nEach row in Figure 4 displays the top 16 activations for\\na pool5 unit from a CNN that we ﬁne-tuned on VOC 2007\\ntrainval. Six of the 256 functionally unique units are visu-\\nalized (Appendix D includes more). These units were se-\\nlected to show a representative sample of what the network\\nlearns. In the second row, we see a unit that ﬁres on dog\\nfaces and dot arrays. The unit corresponding to the third row'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='lected to show a representative sample of what the network\\nlearns. In the second row, we see a unit that ﬁres on dog\\nfaces and dot arrays. The unit corresponding to the third row\\nis a red blob detector. There are also detectors for human\\nfaces and more abstract patterns such as text and triangular\\nstructures with windows. The network appears to learn a\\nrepresentation that combines a small number of class-tuned\\nfeatures together with a distributed representation of shape,\\ntexture, color, and material properties. The subsequent fully\\nconnected layer fc 6 has the ability to model a large set of\\ncompositions of these rich features.\\n3.2. Ablation studies\\nPerformance layer-by-layer, without ﬁne-tuning. To un-\\nderstand which layers are critical for detection performance,\\nwe analyzed results on the VOC 2007 dataset for each of the\\nCNN’s last three layers. Layer pool5 was brieﬂy described\\nin Section 3.1. The ﬁnal two layers are summarized below.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='we analyzed results on the VOC 2007 dataset for each of the\\nCNN’s last three layers. Layer pool5 was brieﬂy described\\nin Section 3.1. The ﬁnal two layers are summarized below.\\nLayer fc6 is fully connected to pool 5. To compute fea-\\ntures, it multiplies a4096×9216 weight matrix by the pool5\\nfeature map (reshaped as a 9216-dimensional vector) and\\nthen adds a vector of biases. This intermediate vector is\\ncomponent-wise half-wave rectiﬁed (x←max(0,x)).\\nLayer fc7 is the ﬁnal layer of the network. It is imple-\\nmented by multiplying the features computed by fc 6 by a\\n4096 ×4096 weight matrix, and similarly adding a vector\\nof biases and applying half-wave rectiﬁcation.\\nWe start by looking at results from the CNN without\\nﬁne-tuning on PASCAL, i.e. all CNN parameters were\\npre-trained on ILSVRC 2012 only. Analyzing performance\\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\\nfc7 generalize worse than features from fc 6. This means'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6'}, page_content='pre-trained on ILSVRC 2012 only. Analyzing performance\\nlayer-by-layer (Table 2 rows 1-3) reveals that features from\\nfc7 generalize worse than features from fc 6. This means\\nthat 29%, or about 16.8 million, of the CNN’s parameters\\ncan be removed without degrading mAP. More surprising is\\nthat removing both fc7 and fc6 produces quite good results\\neven though pool5 features are computed using only 6% of\\nthe CNN’s parameters. Much of the CNN’s representational\\npower comes from its convolutional layers, rather than from\\nthe much larger densely connected layers. This ﬁnding sug-\\ngests potential utility in computing a dense feature map, in\\nthe sense of HOG, of an arbitrary-sized image by using only\\nthe convolutional layers of the CNN. This representation\\nwould enable experimentation with sliding-window detec-\\ntors, including DPM, on top of pool5 features.\\nPerformance layer-by-layer, with ﬁne-tuning. We now\\nlook at results from our CNN after having ﬁne-tuned its pa-\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='rameters on VOC 2007 trainval. The improvement is strik-\\ning (Table 2 rows 4-6): ﬁne-tuning increases mAP by 8.0\\npercentage points to 54.2%. The boost from ﬁne-tuning is\\nmuch larger for fc6 and fc7 than for pool5, which suggests\\nthat the pool 5 features learned from ImageNet are general\\nand that most of the improvement is gained from learning\\ndomain-speciﬁc non-linear classiﬁers on top of them.\\nComparison to recent feature learning methods. Rela-\\ntively few feature learning methods have been tried on PAS-\\nCAL VOC detection. We look at two recent approaches that\\nbuild on deformable part models. For reference, we also in-\\nclude results for the standard HOG-based DPM [20].\\nThe ﬁrst DPM feature learning method, DPM ST [28],\\naugments HOG features with histograms of “sketch token”\\nprobabilities. Intuitively, a sketch token is a tight distri-\\nbution of contours passing through the center of an image\\npatch. Sketch token probabilities are computed at each pixel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='probabilities. Intuitively, a sketch token is a tight distri-\\nbution of contours passing through the center of an image\\npatch. Sketch token probabilities are computed at each pixel\\nby a random forest that was trained to classify35×35 pixel\\npatches into one of 150 sketch tokens or background.\\nThe second method, DPM HSC [31], replaces HOG with\\nhistograms of sparse codes (HSC). To compute an HSC,\\nsparse code activations are solved for at each pixel using\\na learned dictionary of 100 7 ×7 pixel (grayscale) atoms.\\nThe resulting activations are rectiﬁed in three ways (full and\\nboth half-waves), spatially pooled, unit ℓ2 normalized, and\\nthen power transformed (x←sign(x)|x|α).\\nAll R-CNN variants strongly outperform the three DPM\\nbaselines (Table 2 rows 8-10), including the two that use\\nfeature learning. Compared to the latest version of DPM,\\nwhich uses only HOG features, our mAP is more than 20\\npercentage points higher: 54.2% vs. 33.7%— a 61% rela-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='feature learning. Compared to the latest version of DPM,\\nwhich uses only HOG features, our mAP is more than 20\\npercentage points higher: 54.2% vs. 33.7%— a 61% rela-\\ntive improvement. The combination of HOG and sketch to-\\nkens yields 2.5 mAP points over HOG alone, while HSC\\nimproves over HOG by 4 mAP points (when compared\\ninternally to their private DPM baselines—both use non-\\npublic implementations of DPM that underperform the open\\nsource version [20]). These methods achieve mAPs of\\n29.1% and 34.3%, respectively.\\n3.3. Network architectures\\nMost results in this paper use the network architecture\\nfrom Krizhevsky et al. [25]. However, we have found that\\nthe choice of architecture has a large effect on R-CNN de-\\ntection performance. In Table 3 we show results on VOC\\n2007 test using the 16-layer deep network recently proposed\\nby Simonyan and Zisserman [43]. This network was one of\\nthe top performers in the recent ILSVRC 2014 classiﬁca-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='2007 test using the 16-layer deep network recently proposed\\nby Simonyan and Zisserman [43]. This network was one of\\nthe top performers in the recent ILSVRC 2014 classiﬁca-\\ntion challenge. The network has a homogeneous structure\\nconsisting of 13 layers of 3 ×3 convolution kernels, with\\nﬁve max pooling layers interspersed, and topped with three\\nfully-connected layers. We refer to this network as “O-Net”\\nfor OxfordNet and the baseline as “T-Net” for TorontoNet.\\nTo use O-Net in R-CNN, we downloaded the pub-\\nlicly available pre-trained network weights for the\\nVGG ILSVRC 16 layers model from the Caffe Model\\nZoo.1 We then ﬁne-tuned the network using the same pro-\\ntocol as we used for T-Net. The only difference was to use\\nsmaller minibatches (24 examples) as required in order to\\nﬁt within GPU memory. The results in Table 3 show that R-\\nCNN with O-Net substantially outperforms R-CNN with T-\\nNet, increasing mAP from 58.5% to 66.0%. However there'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='ﬁt within GPU memory. The results in Table 3 show that R-\\nCNN with O-Net substantially outperforms R-CNN with T-\\nNet, increasing mAP from 58.5% to 66.0%. However there\\nis a considerable drawback in terms of compute time, with\\nthe forward pass of O-Net taking roughly 7 times longer\\nthan T-Net.\\n3.4. Detection error analysis\\nWe applied the excellent detection analysis tool from\\nHoiem et al. [23] in order to reveal our method’s error\\nmodes, understand how ﬁne-tuning changes them, and to\\nsee how our error types compare with DPM. A full sum-\\nmary of the analysis tool is beyond the scope of this pa-\\nper and we encourage readers to consult [23] to understand\\nsome ﬁner details (such as “normalized AP”). Since the\\nanalysis is best absorbed in the context of the associated\\nplots, we present the discussion within the captions of Fig-\\nure 5 and Figure 6.\\n3.5. Bounding-box regression\\nBased on the error analysis, we implemented a sim-\\nple method to reduce localization errors. Inspired by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='ure 5 and Figure 6.\\n3.5. Bounding-box regression\\nBased on the error analysis, we implemented a sim-\\nple method to reduce localization errors. Inspired by the\\nbounding-box regression employed in DPM [17], we train a\\nlinear regression model to predict a new detection window\\ngiven the pool 5 features for a selective search region pro-\\nposal. Full details are given in Appendix C. Results in Ta-\\nble 1, Table 2, and Figure 5 show that this simple approach\\nﬁxes a large number of mislocalized detections, boosting\\nmAP by 3 to 4 points.\\n3.6. Qualitative results\\nQualitative detection results on ILSVRC2013 are pre-\\nsented in Figure 8 and Figure 9 at the end of the paper. Each\\nimage was sampled randomly from the val 2 set and all de-\\ntections from all detectors with a precision greater than 0.5\\nare shown. Note that these are not curated and give a re-\\nalistic impression of the detectors in action. More qualita-\\ntive results are presented in Figure 10 and Figure 11, but'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7'}, page_content='are shown. Note that these are not curated and give a re-\\nalistic impression of the detectors in action. More qualita-\\ntive results are presented in Figure 10 and Figure 11, but\\nthese have been curated. We selected each image because it\\ncontained interesting, surprising, or amusing results. Here,\\nalso, all detections at precision greater than 0.5 are shown.\\n4. The ILSVRC2013 detection dataset\\nIn Section 2 we presented results on the ILSVRC2013\\ndetection dataset. This dataset is less homogeneous than\\n1https://github.com/BVLC/caffe/wiki/Model-Zoo\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='occ trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.212\\n0.612\\n0.420\\n0.557\\n0.201\\n0.720\\n0.344\\n0.606\\n0.351\\n0.677\\n0.244\\n0.609\\n0.516\\nnormalized AP\\nR−CNN fc6: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.179\\n0.701\\n0.498\\n0.634\\n0.335\\n0.766\\n0.442\\n0.672\\n0.429\\n0.723\\n0.325\\n0.685\\n0.593\\nnormalized AP\\nR−CNN FT fc7: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.211\\n0.731\\n0.542\\n0.676\\n0.385\\n0.786\\n0.484\\n0.709\\n0.453\\n0.779\\n0.368\\n0.720\\n0.633\\nnormalized AP\\nR−CNN FT fc7 BB: sensitivity and impact\\nocc trn size asp view part0\\n0.2\\n0.4\\n0.6\\n0.8\\n0.132\\n0.339\\n0.216\\n0.347\\n0.056\\n0.487\\n0.126\\n0.453\\n0.137\\n0.391\\n0.094\\n0.388\\n0.297\\nnormalized AP\\nDPM voc−release5: sensitivity and impact\\nFigure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and\\nlowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='lowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part\\nvisibility). We show plots for our method (R-CNN) with and without ﬁne-tuning (FT) and bounding-box regression (BB) as well as for\\nDPM voc-release5. Overall, ﬁne-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve\\nboth the highest and lowest performing subsets for nearly all characteristics. This indicates that ﬁne-tuning does more than simply improve\\nthe lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.\\nInstead, ﬁne-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: animals'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='total false positives\\npercentage of each type\\nR−CNN fc6: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: animals\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN fc6: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\ntotal false positives\\npercentage of each type\\nR−CNN FT fc7 BB: furniture\\n \\n \\n25 100 400 1600 64000\\n20\\n40\\n60\\n80\\n100\\nLoc\\nSim\\nOth\\nBG\\nFigure 5: Distribution of top-ranked false positive (FP) types.\\nEach plot shows the evolving distribution of FP types as more FPs\\nare considered in order of decreasing score. Each FP is catego-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='BG\\nFigure 5: Distribution of top-ranked false positive (FP) types.\\nEach plot shows the evolving distribution of FP types as more FPs\\nare considered in order of decreasing score. Each FP is catego-\\nrized into 1 of 4 types: Loc—poor localization (a detection with\\nan IoU overlap with the correct class between 0.1 and 0.5, or a du-\\nplicate); Sim—confusion with a similar category; Oth—confusion\\nwith a dissimilar object category; BG—a FP that ﬁred on back-\\nground. Compared with DPM (see [23]), signiﬁcantly more of\\nour errors result from poor localization, rather than confusion with\\nbackground or other object classes, indicating that the CNN fea-\\ntures are much more discriminative than HOG. Loose localiza-\\ntion likely results from our use of bottom-up region proposals and\\nthe positional invariance learned from pre-training the CNN for\\nwhole-image classiﬁcation. Column three shows how our simple\\nbounding-box regression method ﬁxes many localization errors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='the positional invariance learned from pre-training the CNN for\\nwhole-image classiﬁcation. Column three shows how our simple\\nbounding-box regression method ﬁxes many localization errors.\\nPASCAL VOC, requiring choices about how to use it. Since\\nthese decisions are non-trivial, we cover them in this sec-\\ntion.\\n4.1. Dataset overview\\nThe ILSVRC2013 detection dataset is split into three\\nsets: train (395,918), val (20,121), and test (40,152), where\\nthe number of images in each set is in parentheses. The\\nval and test splits are drawn from the same image distribu-\\ntion. These images are scene-like and similar in complexity\\n(number of objects, amount of clutter, pose variability, etc.)\\nto PASCAL VOC images. The val and test splits are exhaus-\\ntively annotated, meaning that in each image all instances\\nfrom all 200 classes are labeled with bounding boxes. The\\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\\nsiﬁcation image distribution. These images have more vari-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='from all 200 classes are labeled with bounding boxes. The\\ntrain set, in contrast, is drawn from the ILSVRC2013 clas-\\nsiﬁcation image distribution. These images have more vari-\\nable complexity with a skew towards images of a single cen-\\ntered object. Unlike val and test, the train images (due to\\ntheir large number) are not exhaustively annotated. In any\\ngiven train image, instances from the 200 classes may or\\nmay not be labeled. In addition to these image sets, each\\nclass has an extra set of negative images. Negative images\\nare manually checked to validate that they do not contain\\nany instances of their associated class. The negative im-\\nage sets were not used in this work. More information on\\nhow ILSVRC was collected and annotated can be found in\\n[11, 36].\\nThe nature of these splits presents a number of choices\\nfor training R-CNN. The train images cannot be used for\\nhard negative mining, because annotations are not exhaus-\\ntive. Where should negative examples come from? Also,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8'}, page_content='for training R-CNN. The train images cannot be used for\\nhard negative mining, because annotations are not exhaus-\\ntive. Where should negative examples come from? Also,\\nthe train images have different statistics than val and test.\\nShould the train images be used at all, and if so, to what\\nextent? While we have not thoroughly evaluated a large\\nnumber of choices, we present what seemed like the most\\nobvious path based on previous experience.\\nOur general strategy is to rely heavily on the val set and\\nuse some of the train images as an auxiliary source of pos-\\nitive examples. To use val for both training and valida-\\ntion, we split it into roughly equally sized “val1” and “val2”\\nsets. Since some classes have very few examples in val (the\\nsmallest has only 31 and half have fewer than 110), it is\\nimportant to produce an approximately class-balanced par-\\ntition. To do this, a large number of candidate splits were\\ngenerated and the one with the smallest maximum relative\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='class imbalance was selected. 2 Each candidate split was\\ngenerated by clustering val images using their class counts\\nas features, followed by a randomized local search that may\\nimprove the split balance. The particular split used here has\\na maximum relative imbalance of about 11% and a median\\nrelative imbalance of 4%. The val1/val2 split and code used\\nto produce them will be publicly available to allow other re-\\nsearchers to compare their methods on the val splits used in\\nthis report.\\n4.2. Region proposals\\nWe followed the same region proposal approach that was\\nused for detection on PASCAL. Selective search [39] was\\nrun in “fast mode” on each image in val1, val2, and test (but\\nnot on images in train). One minor modiﬁcation was re-\\nquired to deal with the fact that selective search is not scale\\ninvariant and so the number of regions produced depends\\non the image resolution. ILSVRC image sizes range from\\nvery small to a few that are several mega-pixels, and so we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='invariant and so the number of regions produced depends\\non the image resolution. ILSVRC image sizes range from\\nvery small to a few that are several mega-pixels, and so we\\nresized each image to a ﬁxed width (500 pixels) before run-\\nning selective search. On val, selective search resulted in an\\naverage of 2403 region proposals per image with a 91.6%\\nrecall of all ground-truth bounding boxes (at 0.5 IoU thresh-\\nold). This recall is notably lower than in PASCAL, where\\nit is approximately 98%, indicating signiﬁcant room for im-\\nprovement in the region proposal stage.\\n4.3. Training data\\nFor training data, we formed a set of images and boxes\\nthat includes all selective search and ground-truth boxes\\nfrom val 1 together with up to N ground-truth boxes per\\nclass from train (if a class has fewer than N ground-truth\\nboxes in train, then we take all of them). We’ll call this\\ndataset of images and boxes val 1+trainN. In an ablation\\nstudy, we show mAP on val2 for N ∈{0,500,1000}(Sec-\\ntion 4.5).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='boxes in train, then we take all of them). We’ll call this\\ndataset of images and boxes val 1+trainN. In an ablation\\nstudy, we show mAP on val2 for N ∈{0,500,1000}(Sec-\\ntion 4.5).\\nTraining data is required for three procedures in R-CNN:\\n(1) CNN ﬁne-tuning, (2) detector SVM training, and (3)\\nbounding-box regressor training. CNN ﬁne-tuning was run\\nfor 50k SGD iteration on val1+trainN using the exact same\\nsettings as were used for PASCAL. Fine-tuning on a sin-\\ngle NVIDIA Tesla K20 took 13 hours using Caffe. For\\nSVM training, all ground-truth boxes from val 1+trainN\\nwere used as positive examples for their respective classes.\\nHard negative mining was performed on a randomly se-\\nlected subset of 5000 images from val 1. An initial experi-\\nment indicated that mining negatives from all of val1, versus\\na 5000 image subset (roughly half of it), resulted in only a\\n0.5 percentage point drop in mAP, while cutting SVM train-\\ning time in half. No negative examples were taken from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='a 5000 image subset (roughly half of it), resulted in only a\\n0.5 percentage point drop in mAP, while cutting SVM train-\\ning time in half. No negative examples were taken from\\n2Relative imbalance is measured as |a −b|/(a + b) where a and b are\\nclass counts in each half of the split.\\ntrain because the annotations are not exhaustive. The ex-\\ntra sets of veriﬁed negative images were not used. The\\nbounding-box regressors were trained on val1.\\n4.4. Validation and evaluation\\nBefore submitting results to the evaluation server, we\\nvalidated data usage choices and the effect of ﬁne-tuning\\nand bounding-box regression on the val2 set using the train-\\ning data described above. All system hyperparameters (e.g.,\\nSVM C hyperparameters, padding used in region warp-\\ning, NMS thresholds, bounding-box regression hyperpa-\\nrameters) were ﬁxed at the same values used for PAS-\\nCAL. Undoubtedly some of these hyperparameter choices\\nare slightly suboptimal for ILSVRC, however the goal of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='rameters) were ﬁxed at the same values used for PAS-\\nCAL. Undoubtedly some of these hyperparameter choices\\nare slightly suboptimal for ILSVRC, however the goal of\\nthis work was to produce a preliminary R-CNN result on\\nILSVRC without extensive dataset tuning. After selecting\\nthe best choices on val 2, we submitted exactly two result\\nﬁles to the ILSVRC2013 evaluation server. The ﬁrst sub-\\nmission was without bounding-box regression and the sec-\\nond submission was with bounding-box regression. For\\nthese submissions, we expanded the SVM and bounding-\\nbox regressor training sets to use val +train1k and val, re-\\nspectively. We used the CNN that was ﬁne-tuned on\\nval1+train1k to avoid re-running ﬁne-tuning and feature\\ncomputation.\\n4.5. Ablation study\\nTable 4 shows an ablation study of the effects of differ-\\nent amounts of training data, ﬁne-tuning, and bounding-\\nbox regression. A ﬁrst observation is that mAP on val 2\\nmatches mAP on test very closely. This gives us conﬁ-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='ent amounts of training data, ﬁne-tuning, and bounding-\\nbox regression. A ﬁrst observation is that mAP on val 2\\nmatches mAP on test very closely. This gives us conﬁ-\\ndence that mAP on val 2 is a good indicator of test set per-\\nformance. The ﬁrst result, 20.9%, is what R-CNN achieves\\nusing a CNN pre-trained on the ILSVRC2012 classiﬁca-\\ntion dataset (no ﬁne-tuning) and given access to the small\\namount of training data in val1 (recall that half of the classes\\nin val 1 have between 15 and 55 examples). Expanding\\nthe training set to val 1+trainN improves performance to\\n24.1%, with essentially no difference between N = 500\\nand N = 1000. Fine-tuning the CNN using examples from\\njust val1 gives a modest improvement to 26.5%, however\\nthere is likely signiﬁcant overﬁtting due to the small number\\nof positive training examples. Expanding the ﬁne-tuning\\nset to val1+train1k, which adds up to 1000 positive exam-\\nples per class from the train set, helps signiﬁcantly, boosting'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9'}, page_content='of positive training examples. Expanding the ﬁne-tuning\\nset to val1+train1k, which adds up to 1000 positive exam-\\nples per class from the train set, helps signiﬁcantly, boosting\\nmAP to 29.7%. Bounding-box regression improves results\\nto 31.0%, which is a smaller relative gain that what was ob-\\nserved in PASCAL.\\n4.6. Relationship to OverFeat\\nThere is an interesting relationship between R-CNN and\\nOverFeat: OverFeat can be seen (roughly) as a special case\\nof R-CNN. If one were to replace selective search region\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='test set val2 val2 val2 val2 val2 val2 test test\\nSVM training set val1 val1+train.5k val1+train1k val1+train1k val1+train1k val1+train1k val+train1k val+train1k\\nCNN ﬁne-tuning set n/a n/a n/a val 1 val1+train1k val1+train1k val1+train1k val1+train1k\\nbbox reg set n/a n/a n/a n/a n/a val 1 n/a val\\nCNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7\\nmAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4\\nmedian AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3\\nTable 4: ILSVRC2013 ablation study of data usage choices, ﬁne-tuning, and bounding-box regression.\\nproposals with a multi-scale pyramid of regular square re-\\ngions and change the per-class bounding-box regressors to\\na single bounding-box regressor, then the systems would\\nbe very similar (modulo some potentially signiﬁcant differ-\\nences in how they are trained: CNN detection ﬁne-tuning,\\nusing SVMs, etc.). It is worth noting that OverFeat has\\na signiﬁcant speed advantage over R-CNN: it is about 9x'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='ences in how they are trained: CNN detection ﬁne-tuning,\\nusing SVMs, etc.). It is worth noting that OverFeat has\\na signiﬁcant speed advantage over R-CNN: it is about 9x\\nfaster, based on a ﬁgure of 2 seconds per image quoted from\\n[34]. This speed comes from the fact that OverFeat’s slid-\\ning windows (i.e., region proposals) are not warped at the\\nimage level and therefore computation can be easily shared\\nbetween overlapping windows. Sharing is implemented by\\nrunning the entire network in a convolutional fashion over\\narbitrary-sized inputs. Speeding up R-CNN should be pos-\\nsible in a variety of ways and remains as future work.\\n5. Semantic segmentation\\nRegion classiﬁcation is a standard technique for seman-\\ntic segmentation, allowing us to easily apply R-CNN to the\\nPASCAL VOC segmentation challenge. To facilitate a di-\\nrect comparison with the current leading semantic segmen-\\ntation system (called O 2P for “second-order pooling”) [4],\\nwe work within their open source framework. O 2P uses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='rect comparison with the current leading semantic segmen-\\ntation system (called O 2P for “second-order pooling”) [4],\\nwe work within their open source framework. O 2P uses\\nCPMC to generate 150 region proposals per image and then\\npredicts the quality of each region, for each class, using\\nsupport vector regression (SVR). The high performance of\\ntheir approach is due to the quality of the CPMC regions\\nand the powerful second-order pooling of multiple feature\\ntypes (enriched variants of SIFT and LBP). We also note\\nthat Farabet et al. [16] recently demonstrated good results\\non several dense scene labeling datasets (not including PAS-\\nCAL) using a CNN as a multi-scale per-pixel classiﬁer.\\nWe follow [2, 4] and extend the PASCAL segmentation\\ntraining set to include the extra annotations made available\\nby Hariharan et al. [22]. Design decisions and hyperparam-\\neters were cross-validated on the VOC 2011 validation set.\\nFinal test results were evaluated only once.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='by Hariharan et al. [22]. Design decisions and hyperparam-\\neters were cross-validated on the VOC 2011 validation set.\\nFinal test results were evaluated only once.\\nCNN features for segmentation. We evaluate three strate-\\ngies for computing features on CPMC regions, all of which\\nbegin by warping the rectangular window around the re-\\ngion to 227 ×227. The ﬁrst strategy ( full) ignores the re-\\ngion’s shape and computes CNN features directly on the\\nwarped window, exactly as we did for detection. However,\\nthese features ignore the non-rectangular shape of the re-\\ngion. Two regions might have very similar bounding boxes\\nwhile having very little overlap. Therefore, the second strat-\\negy (fg) computes CNN features only on a region’s fore-\\nground mask. We replace the background with the mean\\ninput so that background regions are zero after mean sub-\\ntraction. The third strategy ( full+fg) simply concatenates\\nthe full and fg features; our experiments validate their com-\\nplementarity.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='input so that background regions are zero after mean sub-\\ntraction. The third strategy ( full+fg) simply concatenates\\nthe full and fg features; our experiments validate their com-\\nplementarity.\\nfull R-CNN fg R-CNN full+fg R-CNN\\nO2P [4] fc6 fc7 fc6 fc7 fc6 fc7\\n46.4 43.0 42.5 43.7 42.1 47.9 45.8\\nTable 5: Segmentation mean accuracy (%) on VOC 2011 vali-\\ndation. Column 1 presents O2P; 2-7 use our CNN pre-trained on\\nILSVRC 2012.\\nResults on VOC 2011. Table 5 shows a summary of our\\nresults on the VOC 2011 validation set compared with O2P.\\n(See Appendix E for complete per-category results.) Within\\neach feature computation strategy, layer fc6 always outper-\\nforms fc7 and the following discussion refers to the fc6 fea-\\ntures. The fg strategy slightly outperforms full, indicating\\nthat the masked region shape provides a stronger signal,\\nmatching our intuition. However, full+fg achieves an aver-\\nage accuracy of 47.9%, our best result by a margin of 4.2%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10'}, page_content='that the masked region shape provides a stronger signal,\\nmatching our intuition. However, full+fg achieves an aver-\\nage accuracy of 47.9%, our best result by a margin of 4.2%\\n(also modestly outperforming O2P), indicating that the con-\\ntext provided by the full features is highly informative even\\ngiven the fg features. Notably, training the 20 SVRs on our\\nfull+fg features takes an hour on a single core, compared to\\n10+ hours for training on O2P features.\\nIn Table 6 we present results on the VOC 2011 test\\nset, comparing our best-performing method, fc 6 (full+fg),\\nagainst two strong baselines. Our method achieves the high-\\nest segmentation accuracy for 11 out of 21 categories, and\\nthe highest overall segmentation accuracy of 47.9%, aver-\\naged across categories (but likely ties with the O 2P result\\nunder any reasonable margin of error). Still better perfor-\\nmance could likely be achieved by ﬁne-tuning.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='VOC 2011 test bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nR&P [2] 83.446.8 18.9 36.6 31.2 42.7 57.3 47.4 44.1 8.1 39.436.136.3 49.5 48.3 50.7 26.3 47.2 22.1 42.0 43.2 40.8\\nO2P [4] 85.469.722.3 45.244.4 46.9 66.7 57.8 56.213.5 46.132.3 41.259.1 55.3 51.0 36.2 50.4 27.846.944.6 47.6\\nours(full+fgR-CNN fc6) 84.266.923.7 58.337.4 55.4 73.3 58.7 56.59.7 45.5 29.549.3 40.1 57.8 53.9 33.8 60.7 22.747.141.3 47.9\\nTable 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the “Regions and Parts” (R&P)\\nmethod of [2] and the second-order pooling (O 2P) method of [4]. Without any ﬁne-tuning, our CNN achieves top segmentation perfor-\\nmance, outperforming R&P and roughly matching O2P.\\n6. Conclusion\\nIn recent years, object detection performance had stag-\\nnated. The best performing systems were complex en-\\nsembles combining multiple low-level image features with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='6. Conclusion\\nIn recent years, object detection performance had stag-\\nnated. The best performing systems were complex en-\\nsembles combining multiple low-level image features with\\nhigh-level context from object detectors and scene classi-\\nﬁers. This paper presents a simple and scalable object de-\\ntection algorithm that gives a 30% relative improvement\\nover the best previous results on PASCAL VOC 2012.\\nWe achieved this performance through two insights. The\\nﬁrst is to apply high-capacity convolutional neural net-\\nworks to bottom-up region proposals in order to localize\\nand segment objects. The second is a paradigm for train-\\ning large CNNs when labeled training data is scarce. We\\nshow that it is highly effective to pre-train the network—\\nwith supervision—for a auxiliary task with abundant data\\n(image classiﬁcation) and then to ﬁne-tune the network for\\nthe target task where data is scarce (detection). We conjec-\\nture that the “supervised pre-training/domain-speciﬁc ﬁne-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='(image classiﬁcation) and then to ﬁne-tune the network for\\nthe target task where data is scarce (detection). We conjec-\\nture that the “supervised pre-training/domain-speciﬁc ﬁne-\\ntuning” paradigm will be highly effective for a variety of\\ndata-scarce vision problems.\\nWe conclude by noting that it is signiﬁcant that we\\nachieved these results by using a combination of classi-\\ncal tools from computer vision and deep learning (bottom-\\nup region proposals and convolutional neural networks).\\nRather than opposing lines of scientiﬁc inquiry, the two are\\nnatural and inevitable partners.\\nAcknowledgments. This research was supported in part\\nby DARPA Mind’s Eye and MSEE programs, by NSF\\nawards IIS-0905647, IIS-1134072, and IIS-1212798,\\nMURI N000014-10-1-0933, and by support from Toyota.\\nThe GPUs used in this research were generously donated\\nby the NVIDIA Corporation.\\nAppendix\\nA. Object proposal transformations\\nThe convolutional neural network used in this work re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='The GPUs used in this research were generously donated\\nby the NVIDIA Corporation.\\nAppendix\\nA. Object proposal transformations\\nThe convolutional neural network used in this work re-\\nquires a ﬁxed-size input of 227 ×227 pixels. For detec-\\ntion, we consider object proposals that are arbitrary image\\nrectangles. We evaluated two approaches for transforming\\nobject proposals into valid CNN inputs.\\nThe ﬁrst method (“tightest square with context”) en-\\ncloses each object proposal inside the tightest square and\\n(A) (B) (C) (D)\\n (A) (B) (C) (D)\\nFigure 7: Different object proposal transformations. (A) the\\noriginal object proposal at its actual scale relative to the trans-\\nformed CNN inputs; (B) tightest square with context; (C) tight-\\nest square without context; (D) warp. Within each column and\\nexample proposal, the top row corresponds top = 0pixels of con-\\ntext padding while the bottom row has p = 16 pixels of context\\npadding.\\nthen scales (isotropically) the image contained in that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='example proposal, the top row corresponds top = 0pixels of con-\\ntext padding while the bottom row has p = 16 pixels of context\\npadding.\\nthen scales (isotropically) the image contained in that\\nsquare to the CNN input size. Figure 7 column (B) shows\\nthis transformation. A variant on this method (“tightest\\nsquare without context”) excludes the image content that\\nsurrounds the original object proposal. Figure 7 column\\n(C) shows this transformation. The second method (“warp”)\\nanisotropically scales each object proposal to the CNN in-\\nput size. Figure 7 column (D) shows the warp transforma-\\ntion.\\nFor each of these transformations, we also consider in-\\ncluding additional image context around the original object\\nproposal. The amount of context padding (p) is deﬁned as a\\nborder size around the original object proposal in the trans-\\nformed input coordinate frame. Figure 7 shows p = 0pix-\\nels in the top row of each example and p = 16 pixels in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11'}, page_content='border size around the original object proposal in the trans-\\nformed input coordinate frame. Figure 7 shows p = 0pix-\\nels in the top row of each example and p = 16 pixels in\\nthe bottom row. In all methods, if the source rectangle ex-\\ntends beyond the image, the missing data is replaced with\\nthe image mean (which is then subtracted before inputing\\nthe image into the CNN). A pilot set of experiments showed\\nthat warping with context padding ( p = 16pixels) outper-\\nformed the alternatives by a large margin (3-5 mAP points).\\nObviously more alternatives are possible, including using\\nreplication instead of mean padding. Exhaustive evaluation\\nof these alternatives is left as future work.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='B. Positive vs. negative examples and softmax\\nTwo design choices warrant further discussion. The ﬁrst\\nis: Why are positive and negative examples deﬁned differ-\\nently for ﬁne-tuning the CNN versus training the object de-\\ntection SVMs? To review the deﬁnitions brieﬂy, for ﬁne-\\ntuning we map each object proposal to the ground-truth in-\\nstance with which it has maximum IoU overlap (if any) and\\nlabel it as a positive for the matched ground-truth class if the\\nIoU is at least 0.5. All other proposals are labeled “back-\\nground” (i.e., negative examples for all classes). For train-\\ning SVMs, in contrast, we take only the ground-truth boxes\\nas positive examples for their respective classes and label\\nproposals with less than 0.3 IoU overlap with all instances\\nof a class as a negative for that class. Proposals that fall\\ninto the grey zone (more than 0.3 IoU overlap, but are not\\nground truth) are ignored.\\nHistorically speaking, we arrived at these deﬁnitions be-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='into the grey zone (more than 0.3 IoU overlap, but are not\\nground truth) are ignored.\\nHistorically speaking, we arrived at these deﬁnitions be-\\ncause we started by training SVMs on features computed\\nby the ImageNet pre-trained CNN, and so ﬁne-tuning was\\nnot a consideration at that point in time. In that setup, we\\nfound that our particular label deﬁnition for training SVMs\\nwas optimal within the set of options we evaluated (which\\nincluded the setting we now use for ﬁne-tuning). When we\\nstarted using ﬁne-tuning, we initially used the same positive\\nand negative example deﬁnition as we were using for SVM\\ntraining. However, we found that results were much worse\\nthan those obtained using our current deﬁnition of positives\\nand negatives.\\nOur hypothesis is that this difference in how positives\\nand negatives are deﬁned is not fundamentally important\\nand arises from the fact that ﬁne-tuning data is limited.\\nOur current scheme introduces many “jittered” examples'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='and negatives are deﬁned is not fundamentally important\\nand arises from the fact that ﬁne-tuning data is limited.\\nOur current scheme introduces many “jittered” examples\\n(those proposals with overlap between 0.5 and 1, but not\\nground truth), which expands the number of positive exam-\\nples by approximately 30x. We conjecture that this large\\nset is needed when ﬁne-tuning the entire network to avoid\\noverﬁtting. However, we also note that using these jittered\\nexamples is likely suboptimal because the network is not\\nbeing ﬁne-tuned for precise localization.\\nThis leads to the second issue: Why, after ﬁne-tuning,\\ntrain SVMs at all? It would be cleaner to simply apply the\\nlast layer of the ﬁne-tuned network, which is a 21-way soft-\\nmax regression classiﬁer, as the object detector. We tried\\nthis and found that performance on VOC 2007 dropped\\nfrom 54.2% to 50.9% mAP. This performance drop likely\\narises from a combination of several factors including that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='this and found that performance on VOC 2007 dropped\\nfrom 54.2% to 50.9% mAP. This performance drop likely\\narises from a combination of several factors including that\\nthe deﬁnition of positive examples used in ﬁne-tuning does\\nnot emphasize precise localization and the softmax classi-\\nﬁer was trained on randomly sampled negative examples\\nrather than on the subset of “hard negatives” used for SVM\\ntraining.\\nThis result shows that it’s possible to obtain close to\\nthe same level of performance without training SVMs af-\\nter ﬁne-tuning. We conjecture that with some additional\\ntweaks to ﬁne-tuning the remaining performance gap may\\nbe closed. If true, this would simplify and speed up R-CNN\\ntraining with no loss in detection performance.\\nC. Bounding-box regression\\nWe use a simple bounding-box regression stage to im-\\nprove localization performance. After scoring each selec-\\ntive search proposal with a class-speciﬁc detection SVM,\\nwe predict a new bounding box for the detection using a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='prove localization performance. After scoring each selec-\\ntive search proposal with a class-speciﬁc detection SVM,\\nwe predict a new bounding box for the detection using a\\nclass-speciﬁc bounding-box regressor. This is similar in\\nspirit to the bounding-box regression used in deformable\\npart models [17]. The primary difference between the two\\napproaches is that here we regress from features computed\\nby the CNN, rather than from geometric features computed\\non the inferred DPM part locations.\\nThe input to our training algorithm is a set of N train-\\ning pairs {(Pi,Gi)}i=1,...,N, where Pi = (Pi\\nx,Pi\\ny,Pi\\nw,Pi\\nh)\\nspeciﬁes the pixel coordinates of the center of proposalPi’s\\nbounding box together with Pi’s width and height in pixels.\\nHence forth, we drop the superscript iunless it is needed.\\nEach ground-truth bounding box Gis speciﬁed in the same\\nway: G = (Gx,Gy,Gw,Gh). Our goal is to learn a trans-\\nformation that maps a proposed boxP to a ground-truth box\\nG.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='Each ground-truth bounding box Gis speciﬁed in the same\\nway: G = (Gx,Gy,Gw,Gh). Our goal is to learn a trans-\\nformation that maps a proposed boxP to a ground-truth box\\nG.\\nWe parameterize the transformation in terms of four\\nfunctions dx(P), dy(P), dw(P), and dh(P). The ﬁrst\\ntwo specify a scale-invariant translation of the center of\\nP’s bounding box, while the second two specify log-space\\ntranslations of the width and height of P’s bounding box.\\nAfter learning these functions, we can transform an input\\nproposal P into a predicted ground-truth box ˆGby apply-\\ning the transformation\\nˆGx = Pwdx(P) +Px (1)\\nˆGy = Phdy(P) +Py (2)\\nˆGw = Pwexp(dw(P)) (3)\\nˆGh = Phexp(dh(P)). (4)\\nEach function d⋆(P) (where ⋆ is one of x,y,h,w ) is\\nmodeled as a linear function of the pool 5 features of pro-\\nposal P, denoted by φ5(P). (The dependence of φ5(P)\\non the image data is implicitly assumed.) Thus we have\\nd⋆(P) = wT\\n⋆φ5(P), where w⋆ is a vector of learnable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12'}, page_content='posal P, denoted by φ5(P). (The dependence of φ5(P)\\non the image data is implicitly assumed.) Thus we have\\nd⋆(P) = wT\\n⋆φ5(P), where w⋆ is a vector of learnable\\nmodel parameters. We learn w⋆ by optimizing the regu-\\nlarized least squares objective (ridge regression):\\nw⋆ = argmin\\nˆw⋆\\nN∑\\ni\\n(ti\\n⋆ −ˆwT\\n⋆φ5(Pi))2 + λ∥ˆw⋆∥2 . (5)\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='The regression targets t⋆ for the training pair (P,G) are de-\\nﬁned as\\ntx = (Gx −Px)/Pw (6)\\nty = (Gy −Py)/Ph (7)\\ntw = log(Gw/Pw) (8)\\nth = log(Gh/Ph). (9)\\nAs a standard regularized least squares problem, this can be\\nsolved efﬁciently in closed form.\\nWe found two subtle issues while implementing\\nbounding-box regression. The ﬁrst is that regularization\\nis important: we set λ = 1000 based on a validation set.\\nThe second issue is that care must be taken when selecting\\nwhich training pairs (P,G) to use. Intuitively, if P is far\\nfrom all ground-truth boxes, then the task of transforming\\nP to a ground-truth box Gdoes not make sense. Using ex-\\namples like P would lead to a hopeless learning problem.\\nTherefore, we only learn from a proposal P if it is nearby\\nat least one ground-truth box. We implement “nearness” by\\nassigning P to the ground-truth box G with which it has\\nmaximum IoU overlap (in case it overlaps more than one) if\\nand only if the overlap is greater than a threshold (which we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='assigning P to the ground-truth box G with which it has\\nmaximum IoU overlap (in case it overlaps more than one) if\\nand only if the overlap is greater than a threshold (which we\\nset to 0.6 using a validation set). All unassigned proposals\\nare discarded. We do this once for each object class in order\\nto learn a set of class-speciﬁc bounding-box regressors.\\nAt test time, we score each proposal and predict its new\\ndetection window only once. In principle, we could iterate\\nthis procedure (i.e., re-score the newly predicted bounding\\nbox, and then predict a new bounding box from it, and so\\non). However, we found that iterating does not improve\\nresults.\\nD. Additional feature visualizations\\nFigure 12 shows additional visualizations for 20 pool 5\\nunits. For each unit, we show the 24 region proposals that\\nmaximally activate that unit out of the full set of approxi-\\nmately 10 million regions in all of VOC 2007 test.\\nWe label each unit by its (y, x, channel) position in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='maximally activate that unit out of the full set of approxi-\\nmately 10 million regions in all of VOC 2007 test.\\nWe label each unit by its (y, x, channel) position in the\\n6 ×6 ×256 dimensional pool5 feature map. Within each\\nchannel, the CNN computes exactly the same function of\\nthe input region, with the (y, x) position changing only the\\nreceptive ﬁeld.\\nE. Per-category segmentation results\\nIn Table 7 we show the per-category segmentation ac-\\ncuracy on VOC 2011 val for each of our six segmentation\\nmethods in addition to the O 2P method [4]. These results\\nshow which methods are strongest across each of the 20\\nPASCAL classes, plus the background class.\\nF. Analysis of cross-dataset redundancy\\nOne concern when training on an auxiliary dataset is that\\nthere might be redundancy between it and the test set. Even\\nthough the tasks of object detection and whole-image clas-\\nsiﬁcation are substantially different, making such cross-set\\nredundancy much less worrisome, we still conducted a thor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='though the tasks of object detection and whole-image clas-\\nsiﬁcation are substantially different, making such cross-set\\nredundancy much less worrisome, we still conducted a thor-\\nough investigation that quantiﬁes the extent to which PAS-\\nCAL test images are contained within the ILSVRC 2012\\ntraining and validation sets. Our ﬁndings may be useful to\\nresearchers who are interested in using ILSVRC 2012 as\\ntraining data for the PASCAL image classiﬁcation task.\\nWe performed two checks for duplicate (and near-\\nduplicate) images. The ﬁrst test is based on exact matches\\nof ﬂickr image IDs, which are included in the VOC 2007\\ntest annotations (these IDs are intentionally kept secret for\\nsubsequent PASCAL test sets). All PASCAL images, and\\nabout half of ILSVRC, were collected from ﬂickr.com. This\\ncheck turned up 31 matches out of 4952 (0.63%).\\nThe second check uses GIST [30] descriptor matching,\\nwhich was shown in [13] to have excellent performance at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='check turned up 31 matches out of 4952 (0.63%).\\nThe second check uses GIST [30] descriptor matching,\\nwhich was shown in [13] to have excellent performance at\\nnear-duplicate image detection in large (>1 million) image\\ncollections. Following [13], we computed GIST descrip-\\ntors on warped 32 ×32 pixel versions of all ILSVRC 2012\\ntrainval and PASCAL 2007 test images.\\nEuclidean distance nearest-neighbor matching of GIST\\ndescriptors revealed 38 near-duplicate images (including all\\n31 found by ﬂickr ID matching). The matches tend to vary\\nslightly in JPEG compression level and resolution, and to a\\nlesser extent cropping. These ﬁndings show that the overlap\\nis small, less than 1%. For VOC 2012, because ﬂickr IDs\\nare not available, we used the GIST matching method only.\\nBased on GIST matches, 1.5% of VOC 2012 test images\\nare in ILSVRC 2012 trainval. The slightly higher rate for\\nVOC 2012 is likely due to the fact that the two datasets\\nwere collected closer together in time than VOC 2007 and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13'}, page_content='are in ILSVRC 2012 trainval. The slightly higher rate for\\nVOC 2012 is likely due to the fact that the two datasets\\nwere collected closer together in time than VOC 2007 and\\nILSVRC 2012 were.\\nG. Document changelog\\nThis document tracks the progress of R-CNN. To help\\nreaders understand how it has changed over time, here’s a\\nbrief changelog describing the revisions.\\nv1 Initial version.\\nv2 CVPR 2014 camera-ready revision. Includes substan-\\ntial improvements in detection performance brought about\\nby (1) starting ﬁne-tuning from a higher learning rate (0.001\\ninstead of 0.0001), (2) using context padding when prepar-\\ning CNN inputs, and (3) bounding-box regression to ﬁx lo-\\ncalization errors.\\nv3 Results on the ILSVRC2013 detection dataset and com-\\nparison with OverFeat were integrated into several sections\\n(primarily Section 2 and Section 4).\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='VOC 2011 val bg aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tvmean\\nO2P [4] 84.069.021.7 47.7 42.2 42.464.7 65.857.4 12.9 37.4 20.5 43.7 35.7 52.7 51.0 35.8 51.0 28.4 59.8 49.746.4\\nfullR-CNN fc6 81.356.2 23.9 42.9 40.7 38.8 59.2 56.5 53.2 11.4 34.6 16.7 48.1 37.0 51.4 46.0 31.5 44.0 24.3 53.7 51.143.0\\nfullR-CNN fc7 81.052.825.143.8 40.5 42.7 55.4 57.7 51.3 8.7 32.5 11.5 48.1 37.0 50.5 46.4 30.2 42.1 21.2 57.7 56.0 42.5\\nfgR-CNN fc6 81.454.1 21.1 40.6 38.753.6 59.9 57.2 52.5 9.1 36.5 23.6 46.4 38.1 53.2 51.3 32.2 38.7 29.053.0 47.543.7\\nfgR-CNN fc7 80.950.1 20.0 40.2 34.1 40.9 59.7 59.8 52.7 7.3 32.1 14.3 48.8 42.9 54.0 48.6 28.9 42.6 24.9 52.2 48.8 42.1\\nfull+fgR-CNN fc6 83.160.4 23.2 48.447.3 52.6 61.6 60.659.1 10.8 45.8 20.9 57.7 43.3 57.4 52.9 34.7 48.7 28.1 60.0 48.647.9\\nfull+fgR-CNN fc7 82.356.7 20.649.944.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.155.2 45.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='full+fgR-CNN fc7 82.356.7 20.649.944.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.155.2 45.7\\nTable 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.\\nv4 The softmax vs. SVM results in Appendix B contained\\nan error, which has been ﬁxed. We thank Sergio Guadar-\\nrama for helping to identify this issue.\\nv5 Added results using the new 16-layer network architec-\\nture from Simonyan and Zisserman [43] to Section 3.3 and\\nTable 3.\\nReferences\\n[1] B. Alexe, T. Deselaers, and V . Ferrari. Measuring the object-\\nness of image windows. TPAMI, 2012. 2\\n[2] P. Arbel ´aez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and\\nJ. Malik. Semantic segmentation using regions and parts. In\\nCVPR, 2012. 10, 11\\n[3] P. Arbel ´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-\\nlik. Multiscale combinatorial grouping. In CVPR, 2014. 3\\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\\nmantic segmentation with second-order pooling. In ECCV,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='lik. Multiscale combinatorial grouping. In CVPR, 2014. 3\\n[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\\nmantic segmentation with second-order pooling. In ECCV,\\n2012. 4, 10, 11, 13, 14\\n[5] J. Carreira and C. Sminchisescu. CPMC: Automatic ob-\\nject segmentation using constrained parametric min-cuts.\\nTPAMI, 2012. 2, 3\\n[6] D. Cires ¸an, A. Giusti, L. Gambardella, and J. Schmidhu-\\nber. Mitosis detection in breast cancer histology images with\\ndeep neural networks. In MICCAI, 2013. 3\\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In CVPR, 2005. 1\\n[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, and J. Yagnik. Fast, accurate detection of\\n100,000 object classes on a single machine. In CVPR, 2013.\\n3\\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\\nFei. ImageNet Large Scale Visual Recognition Competition\\n2012 (ILSVRC2012). http://www.image-net.org/\\nchallenges/LSVRC/2012/. 1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='3\\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\\nFei. ImageNet Large Scale Visual Recognition Competition\\n2012 (ILSVRC2012). http://www.image-net.org/\\nchallenges/LSVRC/2012/. 1\\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. ImageNet: A large-scale hierarchical image database.\\nIn CVPR, 2009. 1\\n[11] J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. C.\\nBerg, and L. Fei-Fei. Scalable multi-label annotation. In\\nCHI, 2014. 8\\n[12] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,\\nE. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional\\nActivation Feature for Generic Visual Recognition. InICML,\\n2014. 2\\n[13] M. Douze, H. J ´egou, H. Sandhawalia, L. Amsaleg, and\\nC. Schmid. Evaluation of gist descriptors for web-scale im-\\nage search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='age search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\\nChallenge. IJCV, 2010. 1, 4\\n[16] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. TPAMI, 2013. 10\\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. TPAMI, 2010. 2, 4, 7, 12\\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up\\nsegmentation for top-down detection. In CVPR, 2013. 4, 5\\n[19] K. Fukushima. Neocognitron: A self-organizing neu-\\nral network model for a mechanism of pattern recogni-\\ntion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='tion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-\\nnatively trained deformable part models, release 5. http:\\n//www.cs.berkeley.edu/˜rbg/latent-v5/. 2,\\n5, 6, 7\\n[21] C. Gu, J. J. Lim, P. Arbel ´aez, and J. Malik. Recognition\\nusing regions. In CVPR, 2009. 2\\n[22] B. Hariharan, P. Arbel ´aez, L. Bourdev, S. Maji, and J. Malik.\\nSemantic contours from inverse detectors. In ICCV, 2011.\\n10\\n[23] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In ECCV. 2012. 2, 7, 8\\n[24] Y . Jia. Caffe: An open source convolutional archi-\\ntecture for fast feature embedding. http://caffe.\\nberkeleyvision.org/, 2013. 3\\n[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\\nsiﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 3, 4, 7\\n[26] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\\nW. Hubbard, and L. Jackel. Backpropagation applied to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14'}, page_content='siﬁcation with deep convolutional neural networks. In NIPS,\\n2012. 1, 3, 4, 7\\n[26] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\\nW. Hubbard, and L. Jackel. Backpropagation applied to\\nhandwritten zip code recognition. Neural Comp., 1989. 1\\n[27] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-\\nbased learning applied to document recognition. Proc. of the\\nIEEE, 1998. 1\\n[28] J. J. Lim, C. L. Zitnick, and P. Doll ´ar. Sketch tokens: A\\nlearned mid-level representation for contour and object de-\\ntection. In CVPR, 2013. 6, 7\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='class AP class AP class AP class AP class AP\\naccordion 50.8 centipede 30.4 hair spray 13.8 pencil box 11.4 snowplow 69.2\\nairplane 50.0 chain saw 14.1 hamburger 34.2 pencil sharpener 9.0 soap dispenser 16.8\\nant 31.8 chair 19.5 hammer 9.9 perfume 32.8 soccer ball 43.7\\nantelope 53.8 chime 24.6 hamster 46.0 person 41.7 sofa 16.3\\napple 30.9 cocktail shaker 46.2 harmonica 12.6 piano 20.5 spatula 6.8\\narmadillo 54.0 coffee maker 21.5 harp 50.4 pineapple 22.6 squirrel 31.3\\nartichoke 45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball 21.0 starﬁsh 45.1\\naxe 11.8 computer mouse 21.2 head cabbage 17.4 pitcher 19.2 stethoscope 18.3\\nbaby bed 42.0 corkscrew 24.2 helmet 33.4 pizza 43.7 stove 8.1\\nbackpack 2.8 cream 29.9 hippopotamus 38.0 plastic bag 6.4 strainer 9.9\\nbagel 37.5 croquet ball 30.0 horizontal bar 7.0 plate rack 15.2 strawberry 26.8\\nbalance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2\\nbanana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='balance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2\\nbanana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8\\nband aid 17.4 cup or mug 34.0 iPod 59.2 porcupine 37.2 swimming trunks 9.1\\nbanjo 55.3 diaper 10.1 isopod 19.5 power drill 7.9 swine 45.3\\nbaseball 41.8 digital clock 18.5 jellyﬁsh 23.7 pretzel 24.8 syringe 5.7\\nbasketball 65.3 dishwasher 19.9 koala bear 44.3 printer 21.3 table 21.7\\nbathing cap 37.2 dog 76.8 ladle 3.0 puck 14.1 tape player 21.4\\nbeaker 11.3 domestic cat 44.1 ladybug 58.4 punching bag 29.4 tennis ball 59.1\\nbear 62.7 dragonﬂy 27.8 lamp 9.1 purse 8.0 tick 42.6\\nbee 52.9 drum 19.9 laptop 35.4 rabbit 71.0 tie 24.6\\nbell pepper 38.8 dumbbell 14.1 lemon 33.3 racket 16.2 tiger 61.8\\nbench 12.7 electric fan 35.0 lion 51.3 ray 41.1 toaster 29.2\\nbicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 trafﬁc light 24.7\\nbinder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='bicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 trafﬁc light 24.7\\nbinder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8\\nbird 70.9 ﬁg 44.5 lobster 32.4 remote control 41.6 trombone 13.8\\nbookshelf 19.3 ﬁling cabinet 20.6 maillot 31.0 rubber eraser 2.5 trumpet 14.4\\nbow tie 38.8 ﬂower pot 20.2 maraca 30.1 rugby ball 34.5 turtle 59.1\\nbow 9.0 ﬂute 4.9 microphone 4.0 ruler 11.5 tv or monitor 41.7\\nbowl 26.7 fox 59.3 microwave 40.1 salt or pepper shaker 24.6 unicycle 27.2\\nbrassiere 31.2 french horn 24.2 milk can 33.3 saxophone 40.8 vacuum 19.5\\nburrito 25.7 frog 64.1 miniskirt 14.9 scorpion 57.3 violin 13.7\\nbus 57.5 frying pan 21.5 monkey 49.6 screwdriver 10.6 volleyball 59.7\\nbutterﬂy 88.5 giant panda 42.5 motorcycle 42.2 seal 20.9 wafﬂe iron 24.0\\ncamel 37.6 goldﬁsh 28.6 mushroom 31.8 sheep 48.9 washer 39.8\\ncan opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1\\ncar 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15'}, page_content='camel 37.6 goldﬁsh 28.6 mushroom 31.8 sheep 48.9 washer 39.8\\ncan opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1\\ncar 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9\\ncart 48.0 guacamole 32.3 oboe 27.5 snail 36.2 whale 48.6\\ncattle 32.3 guitar 33.1 orange 38.8 snake 33.8 wine bottle 31.2\\ncello 28.9 hair dryer 13.0 otter 22.2 snowmobile 58.8 zebra 49.6\\nTable 8: Per-class average precision (%) on the ILSVRC2013 detection test set.\\n[29] D. Lowe. Distinctive image features from scale-invariant\\nkeypoints. IJCV, 2004. 1\\n[30] A. Oliva and A. Torralba. Modeling the shape of the scene:\\nA holistic representation of the spatial envelope.IJCV, 2001.\\n13\\n[31] X. Ren and D. Ramanan. Histograms of sparse codes for\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='lemon 0.79\\nlemon 0.70\\nlemon 0.56lemon 0.50\\nperson 0.88\\nperson 0.72\\ncocktail shaker 0.56\\ndog 0.97dog 0.85 dog 0.57\\nbird 0.63\\ndog 0.97dog 0.95\\ndog 0.64\\nhelmet 0.65\\nhelmet 0.52\\nmotorcycle 0.65\\nperson 0.75\\nperson 0.58\\nsnowmobile 0.83\\nsnowmobile 0.83\\nbow tie 0.86\\nperson 0.82\\nbird 0.61\\ndog 0.66\\ndog 0.61\\ndomestic cat 0.57\\nbird 0.96\\ndog 0.91\\ndog 0.77\\nsofa 0.71\\ndog 0.95\\ndog 0.55\\nladybug 1.00\\nperson 0.87\\ncar 0.96 car 0.66car 0.63\\nbird 0.98\\nperson 0.65\\nwatercraft 1.00\\nwatercraft 0.69\\npretzel 0.78\\ncar 0.96\\nperson 0.65person 0.58person 0.52\\nperson 0.52\\nbird 0.99 bird 0.91\\nbird 0.75\\ndog 0.98\\nflower pot 0.62\\ndog 0.97dog 0.56\\ntrain 1.00\\ntrain 0.53\\narmadillo 1.00\\narmadillo 0.56\\nbird 0.93\\ndog 0.92\\nswine 0.88\\nbird 1.00\\nbutterfly 0.96\\nperson 0.90\\nflower pot 0.62\\nsnake 0.70\\nturtle 0.54\\nbell pepper 0.81\\nbell pepper 0.62\\nbell pepper 0.54\\nruler 1.00\\nantelope 0.53\\nmushroom 0.93\\ntv or monitor 0.82\\ntv or monitor 0.76tv or monitor 0.54\\nbird 0.89\\nlipstick 0.80\\nlipstick 0.61\\nperson 0.58\\ndog 0.97\\nsoccer ball 0.90'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16'}, page_content='bell pepper 0.54\\nruler 1.00\\nantelope 0.53\\nmushroom 0.93\\ntv or monitor 0.82\\ntv or monitor 0.76tv or monitor 0.54\\nbird 0.89\\nlipstick 0.80\\nlipstick 0.61\\nperson 0.58\\ndog 0.97\\nsoccer ball 0.90\\nFigure 8: Example detections on the val2 set from the conﬁguration that achieved 31.0% mAP on val2. Each image was sampled randomly\\n(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the\\nprecision value of that detection from the detector’s precision-recall curve. Viewing digitally with zoom is recommended.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17'}, page_content='baby bed 0.55helmet 0.51\\npitcher 0.57\\ndog 0.98\\nhat with a wide brim 0.78\\nperson 0.86\\nbird 0.52table 0.60\\nmonkey 0.97\\ntable 0.68\\nwatercraft 0.55\\nperson 0.88\\ncar 0.61\\nperson 0.87\\nperson 0.51\\nsunglasses 0.51\\ndog 0.94dog 0.55\\nbird 0.52\\nmonkey 0.87\\nmonkey 0.81\\nswine 0.50\\ndog 0.97\\nhat with a wide brim 0.96\\nsnake 0.74\\ndog 0.93\\nperson 0.77\\ndog 0.97\\nguacamole 0.64\\npretzel 0.69\\ntable 0.54\\ndog 0.71\\nperson 0.85\\nladybug 0.90\\nperson 0.52\\nzebra 0.83 zebra 0.80\\nzebra 0.55\\nzebra 0.52\\ndog 0.98\\nhat with a wide brim 0.60person 0.85\\nperson 0.81 person 0.73\\nelephant 1.00\\nbird 0.99\\nperson 0.58\\ndog 0.98\\ncart 1.00\\nchair 0.79chair 0.64\\nperson 0.91person 0.87 person 0.57\\nperson 0.52\\ncomputer keyboard 0.52\\ndog 0.97 dog 0.92\\nperson 0.77\\nbird 0.94\\nbutterfly 0.98\\nperson 0.73\\nperson 0.61\\nbird 1.00\\nbird 0.78\\nperson 0.91 person 0.75\\nstethoscope 0.83\\nbird 0.83\\nFigure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='person 0.81\\nperson 0.57\\nperson 0.53\\nmotorcycle 0.64\\nperson 0.73\\nperson 0.51\\nbagel 0.57\\npineapple 1.00\\nbowl 0.63\\nguacamole 1.00tennis ball 0.60\\nlemon 0.88\\nlemon 0.86lemon 0.80\\nlemon 0.78\\norange 0.78\\norange 0.73\\norange 0.71\\ngolf ball 1.00\\ngolf ball 1.00\\ngolf ball 0.89\\ngolf ball 0.81\\ngolf ball 0.79\\ngolf ball 0.76golf ball 0.60\\ngolf ball 0.60\\ngolf ball 0.51\\nlemon 0.53\\nsoccer ball 0.67\\nlamp 0.61\\ntable 0.59\\nbee 0.85\\njellyfish 0.71\\nbowl 0.54\\nhamburger 0.78\\ndumbbell 1.00person 0.52\\nmicrophone 1.00\\nperson 0.85\\nhead cabbage 0.83\\nhead cabbage 0.75\\ndog 0.74\\ngoldfish 0.76\\nperson 0.57\\nguitar 1.00\\nguitar 1.00\\nguitar 0.88\\ntable 0.63\\ncomputer keyboard 0.78\\nmicrowave 0.60\\ntable 0.53\\ntick 0.64\\nlemon 0.80\\ntennis ball 0.67\\nrabbit 1.00\\ndog 0.98\\nperson 0.81\\nperson 0.92\\nsunglasses 0.52\\nwatercraft 0.86\\nmilk can 1.00\\nmilk can 1.00\\nbookshelf 0.50\\nchair 0.86\\ngiant panda 0.61\\nperson 0.87\\nantelope 0.74\\ncattle 0.81\\ndog 0.87\\nhorse 0.78\\npomegranate 1.00\\nchair 0.86\\ntv or monitor 0.52\\nantelope 0.68\\nbird 0.94\\nsnake 0.60'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18'}, page_content='milk can 1.00\\nbookshelf 0.50\\nchair 0.86\\ngiant panda 0.61\\nperson 0.87\\nantelope 0.74\\ncattle 0.81\\ndog 0.87\\nhorse 0.78\\npomegranate 1.00\\nchair 0.86\\ntv or monitor 0.52\\nantelope 0.68\\nbird 0.94\\nsnake 0.60\\ndog 0.98\\ndog 0.88\\nperson 0.79\\nsnake 0.76\\ntable 0.62\\ntv or monitor 0.80\\ntv or monitor 0.58\\ntv or monitor 0.54\\nlamp 0.86lamp 0.65\\ntable 0.83\\nmonkey 1.00monkey 1.00\\nmonkey 0.90\\nmonkey 0.88\\nmonkey 0.52\\ndog 0.88fox 1.00\\nfox 0.81\\nperson 0.88\\nwatercraft 0.91\\nwatercraft 0.56\\nbird 0.95\\nbird 0.78\\nisopod 0.56\\nbird 0.69\\nstarfish 0.67\\ndragonfly 0.70\\ndragonfly 0.60\\nhamburger 0.72\\nhamburger 0.60\\ncup or mug 0.72\\nelectric fan 1.00\\nelectric fan 0.83\\nelectric fan 0.78helmet 0.64\\nsoccer ball 0.63\\nFigure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing\\ndigitally with zoom is recommended.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='object detection. In CVPR, 2013. 6, 7\\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\\nbased face detection. TPAMI, 1998. 2\\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\\ning internal representations by error propagation. Parallel\\nDistributed Processing, 1:318–362, 1986. 1\\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\\ntion and Detection using Convolutional Networks. In ICLR,\\n2014. 1, 2, 4, 10\\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. In CVPR, 2013. 2\\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\\nfor visual object detection. In AAAI Technical Report, 4th\\nHuman Computation Workshop, 2012. 8\\n[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19'}, page_content='[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4\\n[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks\\nfor object detection. In NIPS, 2013. 2\\n[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\\nSelective search for object recognition. IJCV, 2013. 1, 2, 3,\\n4, 5, 9\\n[40] R. Vaillant, C. Monrocq, and Y . LeCun. Original approach\\nfor the localisation of objects in images. IEE Proc on Vision,\\nImage, and Signal Processing, 1994. 2\\n[41] X. Wang, M. Yang, S. Zhu, and Y . Lin. Regionlets for generic\\nobject detection. In ICCV, 2013. 3, 5\\n[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-\\ntional networks for mid and high level feature learning. In\\nCVPR, 2011. 4\\n[43] K. Simonyan and A. Zisserman. Very Deep Convolu-\\ntional Networks for Large-Scale Image Recognition. arXiv\\npreprint, arXiv:1409.1556, 2014. 6, 7, 14\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='person 0.82\\nsnake 0.76\\nfrog 0.78\\nbird 0.79\\ngoldfish 0.76\\ngoldfish 0.76\\ngoldfish 0.58\\nperson 0.94\\nstethoscope 0.56\\nperson 0.95person 0.92person 0.67\\nperson 0.60\\ntable 0.81\\njellyfish 0.67\\nlemon 0.52\\nperson 0.78\\nperson 0.65\\nwatercraft 0.55\\nbaseball 1.00\\nperson 0.94\\nperson 0.82\\nperson 0.80\\nperson 0.61\\nperson 0.55\\nperson 0.52\\ncomputer keyboard 0.81\\ndog 0.60 person 0.88\\nperson 0.79\\nperson 0.68\\nperson 0.59\\ntv or monitor 0.82\\nlizard 0.58\\nchair 0.50\\nperson 0.74\\ntable 0.82\\nperson 0.94\\nperson 0.94\\nperson 0.95\\nperson 0.81person 0.69\\nrugby ball 0.91\\nperson 0.84 person 0.59\\nvolleyball 0.70\\npineapple 1.00\\nbrassiere 0.71\\nperson 0.95 person 0.94person 0.94\\nperson 0.81 person 0.80person 0.80\\nperson 0.79\\nperson 0.79\\nperson 0.69\\nperson 0.66\\nperson 0.58\\nperson 0.56person 0.54\\nswimming trunks 0.56\\nbaseball 0.86\\nhelmet 0.74\\nperson 0.75\\nminiskirt 0.64\\nperson 0.92\\nvacuum 1.00\\ndog 0.98\\ndog 0.93\\nperson 0.94 person 0.75\\nperson 0.65\\nperson 0.53\\nski 0.80 ski 0.80\\nbird 0.55\\ntiger 1.00\\ntiger 0.67\\ntiger 0.59\\nbird 0.56'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='helmet 0.74\\nperson 0.75\\nminiskirt 0.64\\nperson 0.92\\nvacuum 1.00\\ndog 0.98\\ndog 0.93\\nperson 0.94 person 0.75\\nperson 0.65\\nperson 0.53\\nski 0.80 ski 0.80\\nbird 0.55\\ntiger 1.00\\ntiger 0.67\\ntiger 0.59\\nbird 0.56\\nwhale 1.00\\nchair 0.53\\nperson 0.92\\nperson 0.92\\nperson 0.82person 0.78\\nbowl 0.52\\nstrawberry 0.79strawberry 0.70\\nburrito 0.54\\ncroquet ball 0.91croquet ball 0.91croquet ball 0.91 croquet ball 0.91\\nmushroom 0.57\\nwatercraft 0.91\\nwatercraft 0.87\\nwatercraft 0.58\\nplastic bag 0.62\\nplastic bag 0.62\\nwhale 0.88\\ncar 0.70\\ndog 0.94\\ntv or monitor 0.57\\ncart 0.80\\nperson 0.79\\nperson 0.53\\nhat with a wide brim 0.89person 0.88\\nperson 0.82\\nperson 0.79\\nperson 0.56\\nperson 0.54\\ntraffic light 0.79\\nbird 0.59\\ncucumber 0.53\\ncucumber 0.52\\nantelope 1.00\\nantelope 1.00\\nantelope 0.94\\nantelope 0.73\\nantelope 0.63\\nantelope 0.63\\nfox 0.57\\nbalance beam 0.50horizontal bar 1.00\\nperson 0.80\\nperson 0.90\\nsnake 0.64\\ndog 0.98\\ndog 0.97\\nhelmet 0.69\\nhorse 0.92\\nhorse 0.69\\nperson 0.82\\nperson 0.72\\norange 0.79\\norange 0.71\\norange 0.66'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20'}, page_content='fox 0.57\\nbalance beam 0.50horizontal bar 1.00\\nperson 0.80\\nperson 0.90\\nsnake 0.64\\ndog 0.98\\ndog 0.97\\nhelmet 0.69\\nhorse 0.92\\nhorse 0.69\\nperson 0.82\\nperson 0.72\\norange 0.79\\norange 0.71\\norange 0.66\\norange 0.66\\norange 0.59\\norange 0.56\\nbird 0.97\\nbird 0.96\\nbird 0.96\\nbird 0.94\\nbird 0.89\\nbird 0.64\\nbird 0.56\\nbird 0.53bird 0.52\\nguitar 1.00\\nperson 0.82\\nbicycle 0.92\\nperson 0.90\\nperson 0.83\\ncar 1.00 car 0.97\\ndog 0.98dog 0.86\\ndog 0.85\\ndog 0.65dog 0.50\\nperson 0.83\\nperson 0.80\\nperson 0.74person 0.54\\nelephant 0.60\\nFigure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,1) (top 1 − 24)\\n1.0 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,2) (top 1 − 24)\\n1.0 0.9 0.9 0.9 0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,3) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,4) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,5) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,6) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,7) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,8) (top 1 − 24)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,7) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,8) (top 1 − 24)\\n0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,9) (top 1 − 24)\\n0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,10) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.5 0.5\\npool5 feature: (3,3,11) (top 1 − 24)\\n0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,12) (top 1 − 24)\\n0.9 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,13) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\npool5 feature: (3,3,14) (top 1 − 24)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,13) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\npool5 feature: (3,3,14) (top 1 − 24)\\n0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,15) (top 1 − 24)\\n0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8 0.8\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,16) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,17) (top 1 − 24)\\n0.9 0.9 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7\\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\\npool5 feature: (3,3,18) (top 1 − 24)\\n0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,19) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,20) (top 1 − 24)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2014-10-23T01:08:59+00:00', 'author': '', 'keywords': '', 'moddate': '2014-10-23T01:08:59+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21'}, page_content='pool5 feature: (3,3,19) (top 1 − 24)\\n0.9 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\npool5 feature: (3,3,20) (top 1 − 24)\\n1.0 0.9 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6\\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\\nFigure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly\\nactivate each of 20 units. Each montage is labeled by the unit’s (y, x, channel) position in the6 ×6 ×256 dimensional pool5 feature map.\\nEach image region is drawn with an overlay of the unit’s receptive ﬁeld in white. The activation value (which we normalize by dividing by\\nthe max activation value over all units in a channel) is shown in the receptive ﬁeld’s upper-left corner. Best viewed digitally with zoom.\\n21'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1'}, page_content='Full Terms & Conditions of access and use can be found at\\nhttps://www.tandfonline.com/action/journalInformation?journalCode=ttie20\\nTheoretical Issues in Ergonomics Science\\nISSN: 1463-922X (Print) 1464-536X (Online) Journal homepage: https://www.tandfonline.com/loi/ttie20\\nNeurophysiological measures of cognitive\\nworkload during human-computer interaction\\nAlan Gevins & Michael E. Smith\\nTo cite this article: Alan Gevins & Michael E. Smith (2003) Neurophysiological measures of\\ncognitive workload during human-computer interaction, Theoretical Issues in Ergonomics Science,\\n4:1-2, 113-131, DOI: 10.1080/14639220210159717\\nTo link to this article:  https://doi.org/10.1080/14639220210159717\\nPublished online: 26 Nov 2010.\\nSubmit your article to this journal \\nArticle views: 1622\\nView related articles \\nCiting articles: 82 View citing articles'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 1, 'page_label': '0'}, page_content='Neurophysiological measures of cognitive workload during human–\\ncomputer interaction\\nAlan Gevins* and Michael E. Smith\\nSan Francisco Brain Research Institute and SAM Technology, 425 Bush Street, 5th Floor, San\\nFrancisco, CA 94108, USA\\nKeywords: Human–computer interaction; adaptive automation; mental workload; working\\nmemory; attention; neurophysiological signals; EEG.\\nPerhaps the most basic issue in the study of cognitive workload is the problem of\\nhow to actually measure it. The electroencephalogram (EEG) continues to be the\\nclinical method of choice for monitoring brain function in assessing sleep dis-\\norders, level of anaesthesia and epilepsy. This preference reﬂects the EEG’s high\\nsensitivity to variations in alertness and attention, the unimposing conditions\\nunder which it can be recorded, and the low cost of the technology it requires.\\nThese characteristics also suggest that EEG-based monitoring methods might'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 1, 'page_label': '0'}, page_content='under which it can be recorded, and the low cost of the technology it requires.\\nThese characteristics also suggest that EEG-based monitoring methods might\\nprovide a useful tool in ergonomics. This paper reviews a long-term programme\\nof research aimed at developing cognitive workload monitoring methods based\\non EEG measures. This research programme began with basic studies of the way\\nneuroelectric signals change in response to highly controlled variations in task\\ndemands. The results yielded from such studies provided a basis on which to\\ndevelop appropriate signal processing methodologies to automatically diﬀeren-\\ntiate mental eﬀort-related changes in brain activity from artifactual contaminants\\nand for gauging relative magnitudes of mental eﬀort in diﬀerent task conditions.\\nThese methods were then evaluated in the context of more naturalistic computer-\\nbased work. The results obtained from these studies provide initial evidence for'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 1, 'page_label': '0'}, page_content='These methods were then evaluated in the context of more naturalistic computer-\\nbased work. The results obtained from these studies provide initial evidence for\\nthe scientiﬁc andtechnical feasibility of using EEG-based methods for monitoring\\ncognitive load during human–computer interaction.\\n1. Introduction\\nAlthough the EEG has limitations with respect to its use as a method for three-\\ndimensional anatomical localization of neurofunctional systems, it has clear advan-\\ntages relative to other neuroimaging techniques as a method for continuous mon-\\nitoring of brain function. Indeed, it is often the method of choice for some clinical\\nmonitoring tasks. For example, continuous EEG monitoring is an essential tool in\\nthe diagnostic evaluation of epilepsy (Thompson and Ebersole 1999) and in the\\nevaluation and treatment of sleep disorders (Carskadon and Rechtschaﬀen 1989).\\nIt is also coming to play an increasingly important role in neuro-intensive care unit'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 1, 'page_label': '0'}, page_content='evaluation and treatment of sleep disorders (Carskadon and Rechtschaﬀen 1989).\\nIt is also coming to play an increasingly important role in neuro-intensive care unit\\nmonitoring (Vespaetal. 1999) and in gauging level-of-awareness during anesthesia\\n(John etal. 2001, O’Connoretal. 2001).\\nFor many years, eﬀorts have also been under way to evaluate the extent to\\nwhich the EEG might be useful as a monitoring modality in applied work contexts.\\nTo be useful in such settings, a monitoring method should be robust enough to be\\nreliably measured under relatively unstructured task conditions, sensitive enough to\\nTheor. Issues in Ergon. Sci., 2003, vol. 4, nos. 1–2, 113–131\\nTheoreticalIssuesinErgonomicsScience ISSN 1463–922X print/ISSN 1464–536X online# 2003 Taylor & Francis Ltd\\nhttp://www.tandf.co.uk/journals\\nDOI 10.1080/14639220210159717\\n*Author for correspondence. e-mail: alan@eeg.com'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='consistently vary with some dimension of interest, unobtrusive enough to not inter-\\nfere with operator performance and inexpensive enough to eventually be deployable\\noutside of specialized laboratory environments. It should also have reasonably good\\ntime resolution to allow tracking of changes in mental status as complex behaviours\\nunfold. The EEG appears to meet such requirements. Furthermore, the compactness\\nof EEG technology also means that, unlike other functional neuroimaging modal-\\nities (which require massive machinery, large teams of technicians and complete\\nimmobilization of the subject), EEGs can even be collected from an ambulatory\\nsubject who is literally wearing the entire recording apparatus (Gilliametal. 1999).\\nIn recent years, we have been evaluating the potential of the EEG as a measure of\\ncognitive workload, primarily in individuals working at computers. Modern, com-\\nputer-based work environments demand sustained vigilance to multiple streams of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='cognitive workload, primarily in individuals working at computers. Modern, com-\\nputer-based work environments demand sustained vigilance to multiple streams of\\ninformation. Such conditions have the potential to exceed a human’s limited\\ncapacity to attend to and analyse information; cognitive overload has, thus, long\\nbeen recognized (Card et al.1983, Kieras 1988, Olson and Olson 1990) to be an\\nimportant source of performance errors during human–computer interaction. The\\npotential for overload is particularly acute in unskilled users, where unfamiliar pro-\\ncedures are likely to require greater commitment of cognitive resources (Anderson\\nand Boyle 1987, Carlsonetal. 1989). The ability to continuously monitor cognitive\\nworkload might, thus, be valuable in task analysis research and in eﬀorts to improve\\nthe usability of human–computer interfaces (Raskin 2000). Indeed, a central prob-\\nlem in interface design is to develop means to provide information to a user with'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='the usability of human–computer interfaces (Raskin 2000). Indeed, a central prob-\\nlem in interface design is to develop means to provide information to a user with\\nminimum disruption and distraction (Cadizetal. 2001).\\nBecause the problem of cognitive overload is widely recognized, it has been the\\ntopic of extensive empirical attention. Ironically, perhaps the most basic issue in the\\nstudy of cognitive workload is the problem of how to actually measure it. One\\npossibility is to use the EEG to directly measure the brain’s response to a particular\\nset of task demands. An EEG-based measurement of cognitive workload could help\\ncharacterize the success of eﬀorts to design suitable interfaces and interaction pro-\\ntocols. Such a tool might also aid in the design of appropriate adaptive-automation\\nstrategies (Morrison and Gluckman 1994, Byrne and Parasuraman 1996,\\nParasuraman etal. 2000).\\nWe have been taking a systematic approach towards developing EEG-based'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='strategies (Morrison and Gluckman 1994, Byrne and Parasuraman 1996,\\nParasuraman etal. 2000).\\nWe have been taking a systematic approach towards developing EEG-based\\nmethods for addressing this problem. Our ﬁrst eﬀorts revolved around identiﬁcation\\nand characterization of the properties of EEG signals sensitive to variations in the\\ndiﬃculty of highly controlled cognitive tasks. We also evaluated methods for analy-\\nsis of such signals that might be suitable for use in a continuous monitoring context.\\nMore recently, we have begun to generalize those methods to assess computer-based\\ntasks that are more naturalistic in character. In the following, we review the progress\\nof those eﬀorts.\\n2. Brain signals sensitive to variations in mental eﬀort\\nOur ﬁrst objective in this programme of research was to attempt to better character-\\nize the neurophysiological changes that accompany increases in cognitive workload\\nand the allocation of mental eﬀort. We have approached this issue in the context of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 2, 'page_label': '1'}, page_content='ize the neurophysiological changes that accompany increases in cognitive workload\\nand the allocation of mental eﬀort. We have approached this issue in the context of\\nEEG and event-related potential (ERP) studies of working memory (WM). WM can\\nbe construed as an outcome of the ability to control attention and sustain its focus\\non a particular active mental representation (or set of representations) in the face\\nof distracting inﬂuences (Engle et al.1999). In many ways, this notion is nearly\\n114 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='synonymous with what we commonly understand as the ability to eﬀortfully ‘con-\\ncentrate’ on task performance. This ability plays an important role in comprehen-\\nsion, reasoning, planning and learning (Baddeley 1992). Indeed, the eﬀortful use of\\nactive mental representations to guide performance appears critical to behavioural\\nﬂexibility (Goldman-Rakic 1987, 1988) and measures of it tend to be positively\\ncorrelated with performance on psychometric tests of cognitive ability and other\\nindices of scholastic aptitude (Carpenteret al.1990, Kyllonen and Christal 1990,\\nGevins and Smith 2000).\\nMost of our investigations related to the neurophysiological concomitants of\\nWM have required subjects to perform controlled ‘n-back’ style tasks (Gevinsetal.\\n1990, 1996, Gevins and Cutillo 1993) that demand sustained attention to a train of\\nstimuli. In these tasks, the load imposed on WM varies, while perceptual and motor'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='1990, 1996, Gevins and Cutillo 1993) that demand sustained attention to a train of\\nstimuli. In these tasks, the load imposed on WM varies, while perceptual and motor\\ndemands are kept relatively constant. For example, in a spatial variant of the n-back\\ntask we have often employed, stimuli are presented at diﬀerent spatial positions on a\\ncomputer monitor once every 4 or 5s while the subject maintains a central ﬁxation.\\nSubjects must compare the spatial location of each stimulus with that of a previous\\nstimulus, indicating whether a match criterion is met by making a key press response\\non a computer mouse or other device. In an easy, low load version of the task,\\nsubjects compare each stimulus to the ﬁrst stimulus presented in each block of\\ntrials (0-back task). In a more diﬃcult, higher load versions, subjects compare the\\nposition of the current stimulus with that presented one, two or even three trials'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='trials (0-back task). In a more diﬃcult, higher load versions, subjects compare the\\nposition of the current stimulus with that presented one, two or even three trials\\npreviously (1-, 2-, or 3-back tasks). These require constant updating of the informa-\\ntion stored in working memory on each trial, as well constant attention to new\\nstimuli and maintenance of previously presented information. To be successful in\\nsuch tasks when WM demands are high, subjects typically must make a signiﬁcant\\nand continuous mental eﬀort. Similar n-back tasks have recently been adopted in\\nmany other laboratories as a means to activate WM networks in a controlled fashion\\nin the context of conventional behavioural studies (McElree 2001), other electrophy-\\nsiological studies (Ross and Segalowitz 2000, Wintinket al.2001), studies of the\\neﬀects of magnetic ﬁelds on cognitive function (Koivistoet al.2000, Oliveri et al.\\n2001) and functional neuroimaging studies employing PET or fMRI methods'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='eﬀects of magnetic ﬁelds on cognitive function (Koivistoet al.2000, Oliveri et al.\\n2001) and functional neuroimaging studies employing PET or fMRI methods\\n(Jonides et al.1993, Cohen et al.1994, McCarthy et al.1994, Braver et al.1997,\\nJansma etal. 2000).\\nThe stimulus-locked ERPs recorded in such conditions in themselves provide an\\nintriguing picture of the transient, rapidly shifting, sub-second patterns of activation\\nthat characterize the neurofunctional networks that underlie task performance\\n(Gevins and Cutillo 1993, Gevins et al.1995, 1996, McEvoy et al.1998, 2001).\\nThere has also been a long and productive history of experimentation with such\\nmeasures as indices of task imposed cognitive workload (Isrealetal. 1980, Sirevaag\\netal. 1993, HumphreyandKramer1994, Wilson etal. 1994, Krameretal. 1995, Kok\\n2001, Ullsperger et al.2001). However, to compute such measures requires either\\nthat a primary task itself emit distinct and more-or-less regular stimuli that an ERP'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 3, 'page_label': '2'}, page_content='2001, Ullsperger et al.2001). However, to compute such measures requires either\\nthat a primary task itself emit distinct and more-or-less regular stimuli that an ERP\\nresponse can be reliably time-locked to (something lacking from most real-world\\nactivities), or that a task-irrelevant probe stimulus be added to the operator’s work\\nenvironment. In either case, the low signal-to-noise inherent in most single trial\\nERPs can often necessitate averaging the response over many similar events.\\nThe spectral composition of the ongoing EEG also displays regular patterns of\\nload-related modulation during n-back task performance. Some components of the\\nEEG spectrum could have signiﬁcant utility for continuous monitoring applications\\nEEGmonitoringofcognitiveworkload 115'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 4, 'page_label': '3'}, page_content='and, in contrast to ERP indices, can be measured either independently of speciﬁc\\ntask events or in temporal conjunction to them. Because of these properties, we have\\nfocused our eﬀorts to develop workload-monitoring methods on such spectral EEG\\nmeasures. For example, ﬁgure 1 displays spectral power in the 4–14Hz range at a\\nfrontal midline (Fz) and a parietal midline (Pz) scalp location computed from the\\ncontinuous EEG during performance of low load (0-back) and moderately high load\\n(2-back) versions of a spatial n-back task. The data represent the average response\\nfrom a group of 80 subjects in a large study of individual diﬀerences in cognitive\\nability (Gevins and Smith 2000), and show signiﬁcantdiﬀerences in spectral power as\\na function of task load that vary between electrode locations and frequency bands.\\nMore speciﬁcally, at the midline frontal site a 5–7Hz or/C18-band spectral peak is\\nincreased in power during the high load task relative to the low load task. This type'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 4, 'page_label': '3'}, page_content='More speciﬁcally, at the midline frontal site a 5–7Hz or/C18-band spectral peak is\\nincreased in power during the high load task relative to the low load task. This type\\nof frontal midline/C28-signal has frequently been reported to be enhanced in diﬃcult,\\nattention demanding tasks, particularly those requiring a sustained focus of con-\\ncentration (Mizukietal. 1980, Miyataetal. 1990, Yamamoto and Matsuoka 1990,\\nGundel and Wilson 1992, Gevins et al. 1997, 1998, Gevins and Smith 1999).\\nTopographic analyses have indicated that this task loading-related/C18-signal tends\\nto have a sharply deﬁned potential ﬁeld with a focus in the anterior midline\\n116 A.Gevins andM.E.Smith\\nFigure 1. Eﬀect of varying the diﬃculty of an n-back working memory task on the spectral\\npower of EEG signals. The ﬁgure illustrates spectral power in dB of the EEG in 4–14Hz\\nrange at frontal (Fz) and parietal (Pz) midline electrodes, averaged over all trials of the tasks'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 4, 'page_label': '3'}, page_content='power of EEG signals. The ﬁgure illustrates spectral power in dB of the EEG in 4–14Hz\\nrange at frontal (Fz) and parietal (Pz) midline electrodes, averaged over all trials of the tasks\\nand collapsed over 80 subjects. Data are from Gevins and Smith (2000).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='region of the scalp (Inouyeet al.1994, Gevinset al.1997); such a restricted topo-\\ngraphy is unlikely to result from distributed generators in dorsolateral cortical\\nregions. Instead, attempts to model the generating source of the frontal/C18-rhythm\\nfrom both EEG (Gevinsetal. 1997) and magnetoencephalographic (Ishiietal. 1999)\\ndata have implicated the anterior cingulate cortex as a likely region of origin. This\\ncortical region is thought to be part of an anterior brain network that is critical to\\nattention control mechanisms and that is activated by the performance of complex\\ncognitive tasks (Posner and Peterson 1990, Posner and Rothbart 1992). In a review\\nof over 100 positron emission tomography (PET) activation studies that examined\\nanterior cingulate cortex activity, Pausetal. (1998) found that the major source of\\nvariance that aﬀected activation in this region was associated with changes in task\\ndiﬃculty. The EEG results are, thus, consistent with these views, implying that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='variance that aﬀected activation in this region was associated with changes in task\\ndiﬃculty. The EEG results are, thus, consistent with these views, implying that\\nperformance of tasks that require signiﬁcant mental eﬀort places high demands on\\nfrontal brain circuits involved with attention control.\\nIn contrast, ﬁgure 1 also indicates that signals in the 8–12Hz or/C11-band tend to\\nbe attenuated in the high load task relative to the low load task. This inverse rela-\\ntionship between task diﬃculty and/C11-power has been observed in many studies in\\nwhich task diﬃculty has been systematically manipulated (Galinetal. 1978, Gundel\\nand Wilson 1992, Gevinsetal. 1997, 1998, Gevins and Smith 1999). Indeed, this task\\ncorrelate of the /C11-rhythm has been recognized for over 70 years (Berger 1929).\\nBecause of this load-related attenuation, the magnitude of/C11-activity during cogni-\\ntive tasks has been hypothesized to be inversely proportional to the fraction of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='Because of this load-related attenuation, the magnitude of/C11-activity during cogni-\\ntive tasks has been hypothesized to be inversely proportional to the fraction of\\ncortical neurons recruited into a transient functional network for purposes of task\\nperformance (Gevins and Schaﬀer 1980, Pfurtscheller and Klimesch 1992,\\nMulholland 1995). This hypothesis is consistent with current understanding of the\\nneural mechanisms underlying generation of the/C11-rhythm (reviewed in Smithetal.\\n2001). Convergent evidence for this view is also provided by observations of a\\nnegative correlation between /C11-power and regional brain activation as measured\\nwith PET (Larsonet al.1998, Sadato et al.1998), and the frequent ﬁnding from\\nneuroimaging studies of greater and more extensive brain activation during task\\nperformance when task diﬃculty increases (Bakeret al.1996, Bunge et al.2000,\\nCarpenter etal. 2000, Garavanetal. 2000).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='neuroimaging studies of greater and more extensive brain activation during task\\nperformance when task diﬃculty increases (Bakeret al.1996, Bunge et al.2000,\\nCarpenter etal. 2000, Garavanetal. 2000).\\nIn addition to signals in the/C18-a n d/C11-bands, other spectral components of the\\nEEG have also been reported to be sensitive to changes in eﬀortful attention. These\\ninclude slow wave activity in the/C14- (<3Hz) band (McCallumetal. 1988, Rockstroh\\net al.1989), high frequency activity in the/C12- (15–30Hz) and /C13- (30–50Hz) bands\\n(Sheer 1989) and rarely studied phenomenon such as the /C20-rhythm that occurs\\naround 8Hz in a small percentage of subjects (Kennedy et al.1948, Chapman\\net al.1962). Since such phenomena were observed relatively infrequently in our\\nseries of studies on working memory, we will not discuss them further, but they\\nmay, nonetheless, ultimately prove useful in eﬀorts to monitor cognitive workload\\nusing EEG measures.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 5, 'page_label': '4'}, page_content='series of studies on working memory, we will not discuss them further, but they\\nmay, nonetheless, ultimately prove useful in eﬀorts to monitor cognitive workload\\nusing EEG measures.\\n3. Automating detection of mental eﬀort-related changes in the EEG\\nAs the data reviewed above indicate, spectral components of the EEG do in fact vary\\nin a predictable fashion in response to variations in the cognitive demands of tasks.\\nWhile this is a necessary condition for the development of an EEG-based monitor of\\ncognitive workload, it is not suﬃcient. A number of other issues must also be\\naddressed if such laboratory observations are to be transitioned into practical\\nEEGmonitoringofcognitiveworkload 117'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='tools. Foremost among them is the problem of EEG artifact. That is, in addition to\\nbrain activity, signals recorded at the scalp include contaminating potentials from\\neye movements and blinks, muscle activity, head movements and other physiological\\nand instrumental sources of artifact. Such contaminants can easily mask cognition-\\nrelated EEG signals (Barlow 1986, Gevinset al.1979a, b, c, 1980). In laboratory\\nstudies, human experts can be used to actively identify artifacts in raw data and\\neliminate any contaminated EEG segments, to insure that data used in analyses\\nrepresent actual brain activity. For large amounts of data, this is an expensive,\\nlabour-intensive process which itself is both subjective and variable. To be practical\\nin more routine applied contexts such decisions must be made algorithmically.\\nA great deal of research has been directed towards the problem of automated\\nartifact detection. In previous work in our laboratory, we have developed and objec-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='A great deal of research has been directed towards the problem of automated\\nartifact detection. In previous work in our laboratory, we have developed and objec-\\ntively evaluated several generations of automatic artifact detection algorithms. These\\ninclude multi-criteria spectral detectors (Gevinset al.1975, 1977), sharp transient\\nwaveform detectors (Gevins et al. 1976), and detectors using neural networks\\n(Gevins and Morgan 1986, 1988). We have found that our most recent generations\\nof detection algorithms perform about as well as the consensus of expert human\\njudges. In a database of/C2440000 eye-movement, head/body movement and muscle\\nartifacts, the algorithms successfully detected 98.3% of the artifacts, with a false\\ndetection rate of 2.9%, whereas the average expert human judge found 96.5% of\\nthe artifacts, with a 1.7% false detection rate. Thus, while further work on the topic\\nis needed, it is reasonable to expect that the problem of automated artifact detection'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='the artifacts, with a 1.7% false detection rate. Thus, while further work on the topic\\nis needed, it is reasonable to expect that the problem of automated artifact detection\\nwill not be an insurmountable barrier to the development of an EEG-based cognitive\\nworkload monitor.\\nA closely related problem is the fact that, in subjects actively performing tasks\\nwith signiﬁcant perceptuomotor demands in a normal fashion, the incidence of data\\nsegments contaminated by artifacts can be high. As a result, it can be diﬃcult to\\nobtain enough artifact-free data segments for analysis. To minimize data loss, eﬀec-\\ntive digital signal processing methods must also be developed to ﬁlter contaminants\\nout of the EEG when possible. Our main approach to this problem has been to\\nimplement adaptive ﬁltering methods to decontaminate artifacts from EEG signals\\n(Du etal. 1994). We have found such methods to be eﬀective at recovering most of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='implement adaptive ﬁltering methods to decontaminate artifacts from EEG signals\\n(Du etal. 1994). We have found such methods to be eﬀective at recovering most of\\nthe artifact contaminated data recorded in our typical laboratory studies of subjects\\nworking on computer-based tasks. A variety of other methods have been employed\\nby diﬀerent investigators in response to this problem, including such techniques\\nas autoregressive modelling (Van den Berg-Lensssenet al.1989), source modeling\\napproaches(BergandScherg1994)andindependentcomponentsanalysis(Jung etal.\\n2000). As with the problem of artifact detection, continued progress in this area\\nsuggests that, at least under some conditions and for some types of artifacts, decon-\\ntamination strategies will evolve that will enable the automation of EEG processing\\nfor continuous monitoring applications.\\nPresuming, then, that automated pre-processing of the EEG can yield suﬃcient'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 6, 'page_label': '5'}, page_content='for continuous monitoring applications.\\nPresuming, then, that automated pre-processing of the EEG can yield suﬃcient\\ndata for subsequent analyses, questions still remain as to whether the type of load-\\nrelated changes in EEG signals can be measured in a reliable fashion in individual\\nsubjects and whether such measurements can be accomplished with a temporal\\ngranularity suitable for tracking complex behaviours. That is, in the experiments\\ndescribed above,changes in the/C18- and/C11-bands in response to variations in WM load\\nwere demonstrated by collapsing over many minutes of data recorded from a subject\\nat each load level, and then comparing the mean diﬀerences between load levels\\n118 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='across groups of subjects using conventional parametric statistical tests. Under\\nnormal waking conditions, such task-related EEG measures have high test–re-test\\nreliability when compared across a group of subjects measured during two sessions\\nwith 1 week between them (McEvoyetal. 2000). However, for the development of\\nautomated EEG analysis techniques suitable for monitoring applications, load-\\nrelated changes in the EEG would ideally also be replicable when computed over\\nshort segments of data, and would need to have high enough signal-to-noise ratios to\\nbe measurable within such segments.\\nPrior work has demonstrated that multivariate combinations of EEG variables\\ncan be used to accurately discriminate between speciﬁc cognitive states (Gevinsetal.\\n1979a, b, c, Wilson and Fisher 1995). Neural network-based pattern classiﬁcation\\nalgorithms trained on data from individual subjects could also be used to automa-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='1979a, b, c, Wilson and Fisher 1995). Neural network-based pattern classiﬁcation\\nalgorithms trained on data from individual subjects could also be used to automa-\\ntically discriminate data recorded during diﬀerent load levels of versions of the type\\nof n-back WM task described above. For example, we performed an experiment\\n(Gevins et al.1998) in which eight subjects performed both spatial and verbal ver-\\nsions of3-,2-and 1-back WMtasks on testsessionsconducted ondiﬀerent days. For\\neach single trial of data in each subject, spectral power estimates were computed in\\nthe /C18- and/C11-bands for each electrode site. Pattern recognition was performed with\\nthe classic Joseph-Viglione neural network algorithm (Joseph 1961, Viglione 1970,\\nGevins 1980, Gevins and Morgan 1986, 1988). This algorithm iteratively generates\\nand evaluates two-layered feed-forward neural networks from the set of signal fea-\\ntures, automatically identifying small sub-sets of features that produce the best'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='and evaluates two-layered feed-forward neural networks from the set of signal fea-\\ntures, automatically identifying small sub-sets of features that produce the best\\nclassiﬁcation of examples from the sample of data set aside for training. The result-\\ning classiﬁer networks were then cross-validated on the remaining data not included\\nin the training sample.\\nUtilizing these procedures, we found that test data segments from 3-back vs\\n1-back load levels were discriminated with over 95% (p< 0:001) accuracy. Over\\n80% (p< 0:05) of test data segments associated with a 2-back load could also be\\ndiscriminated from data segments in the 3-back or 1-back task loads. Such results\\nprovide initial evidence that, at least for these types of tasks, it is possible to develop\\nalgorithms capable of discriminating diﬀerent cognitive workload levels with a high\\ndegree of accuracy. Not surprisingly, they also indicated that relatively large diﬀer-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='algorithms capable of discriminating diﬀerent cognitive workload levels with a high\\ndegree of accuracy. Not surprisingly, they also indicated that relatively large diﬀer-\\nences in cognitive workload are easier to detect than smaller diﬀerences, and that\\nthere is an inherent trade-oﬀ between the accuracy of classiﬁer performance and the\\ntemporal length of the data segments being classiﬁed.\\nHigh levels of accurate classiﬁcation were also achieved when applying networks\\ntrained with data from one day to data from another day and when applying net-\\nworks trained with data from one task (e.g. spatial WM) to data from another task\\n(e.g. verbal WM). We also attempted to develop networks trained with data from a\\ngroup of subjects to data from new subjects. Such generic networks were found on\\naverage to yield statistically signiﬁcant classiﬁcation results when discriminating the\\n1-back from the 3-back task load conditions, but their accuracy was much reduced'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 7, 'page_label': '6'}, page_content='average to yield statistically signiﬁcant classiﬁcation results when discriminating the\\n1-back from the 3-back task load conditions, but their accuracy was much reduced\\nfrom that achievable with subject speciﬁc networks. On the one hand, such results\\nindicate that there is a fair amount of commonality across days, tasks and subjects in\\nthe particular set of EEG frequency-band measures that are sensitive to increases in\\ncognitive workload. Such commonalities can be exploited in eﬀorts to design eﬃcient\\nsensor montages and signal processing methods. Nonetheless, they also indicate that,\\nto achieve optimal performance using EEG-based cognitive load monitoring\\nmethods, it will likely be necessary to calibrate algorithms to accommodate\\nEEGmonitoringofcognitiveworkload 119'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='individual diﬀerences. Such conclusions are also consistent with the observation that\\npatterns of task-related EEG changes vary in conjunction with individual diﬀerences\\nin cognitive ability and cognitive style (Gevins and Smith 2000).\\nFinally, it is also worthwhile to brieﬂy mention another potential direction for\\nsuch methods. In examining changes in neurophysiological correlates of n-back task\\nperformance over time, we found substantial changes in both the magnitude and the\\ntopography of task-related modulation of EEG activity in the /C11-a n d/C18-bands\\nbetween the time when naı¨ve subjects were ﬁrst learning to perform versions of\\nthe n-back WM and after they had developed some skill at it (Gevinset al.1997,\\nSmith etal. 1999). To further analyse these data, we used neural network methods\\nanalogous to those described above in an attempt to discriminate the task-related\\nEEG signals recorded in the 3-back task when subjects were ﬁrst learning the task'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='analogous to those described above in an attempt to discriminate the task-related\\nEEG signals recorded in the 3-back task when subjects were ﬁrst learning the task\\nfrom signals recorded during skilled performance. Across subjects, we were able to\\nobtain very high levels of classiﬁcation accuracy (range: 96–100%,p< 0:001) for\\ndiscriminating naı¨ve from practiced states. Such results imply that the WM demands\\nof task performance changed with practice and that such changes could be auto-\\nmaticallydetectedwithEEG methods. Since overload of attentional orWM capacity\\nhas been found to be a limiting factor in the early stages of procedural skill acquisi-\\ntion (Woltz 1988, Kyllonen and Shute 1989), minimizing the potential of such over-\\nload is an important design guideline for the development of intelligent tutoring\\nsystems (Anderson and Boyle 1987, Carlson et al. 1989). The data described\\nabove, thus, suggest that it might be possible to utilize information provided by'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='systems (Anderson and Boyle 1987, Carlson et al. 1989). The data described\\nabove, thus, suggest that it might be possible to utilize information provided by\\nsuch monitoring methods to adapt a computer-aided instruction protocol to the\\ncognitive constraints and skill levels of individual students.\\n4. Extension of neurophysiology-based workload monitoring methods to\\n‘naturalistic’ HCI\\nThe types of results described above provide evidence for the basic feasibility of\\nusing EEG-based methods for unobtrusively monitoring cognitive task load in\\nindividuals engaged in computer-based work. However, the n-back WM task\\nmade minimal demands on perceptual and motor systems and it only required\\nthat a subject’s eﬀort be focused on a single repetitive activity. The ability to reliably\\nmeasure cognitive load in individual subjects under such constrained circumstances\\nmight in itself be useful. For example, it has been applied to the problem of assessing'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='measure cognitive load in individual subjects under such constrained circumstances\\nmight in itself be useful. For example, it has been applied to the problem of assessing\\nthe eﬀect of environmental stressors on cognitive functions (Gevins and Smith 1999).\\nEven so, in more naturalistic work environments, task demands are usually less\\nstructured and mental resources often must be divided between competing activities,\\nraising questions as to whether results obtained with the n-back task could generalize\\nto contexts that are more realistic.\\nRecent studies have demonstrated that more complicated forms of human–\\ncomputer interaction (such as videogame play) produce mental eﬀort-related mod-\\nulation of the EEG that is similar to that observed during n-back tasks (Pellouchoud\\netal. 1999, Smithetal. 1999). This implies that it might be possible to extend EEG-\\nbased multivariate methods for monitoring task load to such circumstances. To'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 8, 'page_label': '7'}, page_content='etal. 1999, Smithetal. 1999). This implies that it might be possible to extend EEG-\\nbased multivariate methods for monitoring task load to such circumstances. To\\nevaluate this possibility we performed a subsequent study (Smithet al.2001) in\\nwhich the EEG was recorded while subjects performed the Multi-Attribute Task\\nBattery (MATB; Comstock and Arnegard 1992). The MATB is a personal com-\\nputer-based multi-tasking environment that simulates some of the activities a pilot\\nmight be required to perform. It has been used in several prior studies of mental\\n120 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='workload and adaptive automation (Parasuramanetal. 1993, 1996, Fournieretal.\\n1999). The data collected during performance of the MATB were used to test\\nwhether it is possible to derive combinations of EEG features that can be used for\\nindexing task loading during a relatively complex form of human–computer inter-\\naction.\\nThe MATB task included four concurrently performed sub-tasks in separate\\nwindows on a computer screen (for graphic depictions of the MATB visual display,\\nsee Fournier et al.(1999) and Molloy and Parasuraman (1996)). These included a\\n‘systems monitoring task’ that required the operator to monitor and respond to\\nsimulated warning lights and gauges, a ‘resource management task’ in which fuel\\nlevels in two tanks had to be maintained at a certain level, a ‘communications task’\\nthat involved receiving audio messages and making frequency adjustments on virtual\\nradios, and a compensatory tracking task that simulated manual control of aircraft'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='that involved receiving audio messages and making frequency adjustments on virtual\\nradios, and a compensatory tracking task that simulated manual control of aircraft\\nposition. Manipulating the diﬃculty of each sub-task served to vary load; such\\nmanipulations were made in a between blocks fashion. Subjects learned to perform\\nlow-, medium- and high-load (LL, ML and HL) versions of the tasks. For compar-\\nison purposes, they also performed a ‘passive watching’ (PW) condition in which\\nthey observed the tasks unfolding without actively performing them.\\nSubjects engaged in extensive training on the tasks on one day, and then returned\\nto the laboratory on a subsequent day for testing. On the test day, subjects per-\\nformed multiple 5min blocks of each task diﬃculty level. Behavioural and subjective\\nworkload ratings provided evidence that, on average, workload did indeed increase\\nin a monotonic fashion across the PW, LL, ML and HL task conditions. This'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='workload ratings provided evidence that, on average, workload did indeed increase\\nin a monotonic fashion across the PW, LL, ML and HL task conditions. This\\nincrease in workload was associated with systematic changes in the EEG. In par-\\nticular, as in the prior study of workload changes in the n-back task paradigm,\\nfrontal /C18-band activity tended to increase with increasing task diﬃculty, whereas\\n/C11-band activity tended to decrease (ﬁgure 2). Such results indicated that the work-\\nload manipulations were successful, and that spectral features in the/C18- and/C11-range\\nmight be useful in attempting to automatically monitor changes in workload with\\nEEG measures.\\nSeparate blocks of data were, thus, used to derive and then independently vali-\\ndate subject-speciﬁc, EEG-based, multivariate cognitive workload functions. In con-\\ntrast to the two-class pattern detection functions that were employed to discriminate'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='date subject-speciﬁc, EEG-based, multivariate cognitive workload functions. In con-\\ntrast to the two-class pattern detection functions that were employed to discriminate\\nbetween diﬀerent task load levels in the prior study, we evaluated a diﬀerent tech-\\nnique that results in a single subject-speciﬁc function that produces a continuous\\nindex of cognitive workload and, hence, could be applied to data collected at each\\ndiﬃculty level of the task. In this procedure, the EEG data was ﬁrst decomposed into\\nshort windows and a set of spectral power estimates of activity in the/C18- and /C11-\\nfrequency ranges was extracted from each window. A unique multivariate function\\nwas then deﬁned for each subject that maximized the statistical distance or diver-\\ngence (Tou and Gonzalez 1974) between a small sample of data from low and high\\ntask load conditions. To cross-validate the function it was tested on new data seg-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 9, 'page_label': '8'}, page_content='gence (Tou and Gonzalez 1974) between a small sample of data from low and high\\ntask load conditions. To cross-validate the function it was tested on new data seg-\\nments from the same subject. Across subjects (ﬁgure 3), mean task load index values\\nwere found to increase systematically with increasing task diﬃculty, and diﬀered\\nsigniﬁcantly between the diﬀerent versions of the task (Smithet al.2001). These\\nresults provide encouraging initial evidence that EEG measures can indeed provide\\na modality for measuring cognitive workload during more complex forms of com-\\nputer interaction. Although complex, the signal processing and pattern classiﬁcation\\nalgorithms employed in this study were for real time implementation. A prototype\\nEEGmonitoringofcognitiveworkload 121'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 10, 'page_label': '9'}, page_content='online system running on a circa 1997 personal computer performed the requisite\\ncalculations on-line and provided an updated estimate of cognitive workload at 4s\\nintervals while subjects were engaged in task performance.\\nTo further evaluate the utility of the approach described above as a tool for\\nresearch on human–computer interaction, we also performed a small exploratory\\nstudy that involved more naturalistic computer tasks. In this experiment (Smith and\\n122 A.Gevins andM.E.Smith\\n \\n-1.5\\n-1\\n-0.5\\n0\\n0.5\\n1\\n1.5\\nPW                        LL                         ML                           HL\\nMean (s.e.m)  Power (z-scores)\\nFz Theta 6-7Hz\\nPz Alpha 8-10Hz\\nFigure 2. Mean (n¼ 16) EEG power for the frontal/C18- and parietal/C11-EEG signals during\\nperformance of the MATB ﬂight simulation task. Data have been normalized within each\\nsubject. Data are presented for each of four task versions (PW¼passive watch, L¼low'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 10, 'page_label': '9'}, page_content='performance of the MATB ﬂight simulation task. Data have been normalized within each\\nsubject. Data are presented for each of four task versions (PW¼passive watch, L¼low\\nload, ML¼moderate load, HL¼high load). Normalized spectral power for frontal/C18and\\nparietal /C11are plotted./C28-power increases from the PW to the HL task version, whereas/C18-\\npower decreases from the PW to the HL task version. Data are from Smithetal. (2001).\\n         \\nCognitive Workload Scores During MATB\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nPassive\\nWatching\\n     Low     \\nLoad\\nMedium\\nLoad\\n     High     \\nLoad\\nMATB Task Condition\\nIndex Value \\n \\nFigure 3. Mean and SEM (n = 16) EEG-based cognitive workload index values during\\nperformance of the MATB ﬂight simulation task. Data are presented for each of four\\ntask versions (PW¼passive watch, LL¼low load, ML¼moderate load, HL¼high load).\\nAverage cognitive workload index scores increased monotonically with increasing task\\ndiﬃculty. Data are from Smithetal. (2001b).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='Gevins, unpublished observations), EEG data was ﬁrst recorded from subjects as\\nthey performed multiple series of trials of easy (0-back) and diﬃcult (3-back) n-back\\ntasks, as well as when they rested quietly, in order to establish a baseline view of the\\nresponse of their brain to increased cognitive load. Data was then recorded while\\nsubjects performed more common computer-based tasks that were performed under\\ntime pressure and that were more-or-less intellectually demanding. These more nat-\\nuralistic activities required subjects to perform word processing, take a computer-\\nbased aptitude test, and search for information on the web. The word processing\\ntask requiredsubjects tocorrect asmanymisspellings and grammatical errors asthey\\ncould in the time allotted, working on a lengthy text sample using a popular word\\nprocessing program. The aptitude test was a practice version of the Computer-\\nAdaptive GMAT\\n1 test. Subjects were asked to solve as many ‘data suﬃciency’'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='processing program. The aptitude test was a practice version of the Computer-\\nAdaptive GMAT\\n1 test. Subjects were asked to solve as many ‘data suﬃciency’\\nproblems as possible in the time allotted; such problems make a high demand on\\nlogical and quantitative reasoning skills and require signiﬁcant mental eﬀort to\\ncomplete in a timely fashion. The web-searching task required subjects to use a\\npopular web browser and search engine to ﬁnd as many answers as possible in the\\ntime allotted to a list of trivia questions provided by the experimenter. For example,\\nsubjects were required to use the browser and search engine to ‘convert 98.6 degrees\\nFahrenheit into degrees Kelvin’, ‘ﬁnd the population of the 94105 area code in the\\n1990 US Census’ and ‘ﬁnd the monthly mortgage payment on a $349000, 30 year\\nmortgage with a 7.5% interest rate’. Each type of task was structured such that\\nsubjects would be unlikely to be able to complete it in the time allotted.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='mortgage with a 7.5% interest rate’. Each type of task was structured such that\\nsubjects would be unlikely to be able to complete it in the time allotted.\\nThe same basic analysis procedure described above that was applied to the EEG\\ndata recorded during MATB performance was also employed in this study. More\\nspeciﬁcally, a personalized continuous index of cognitive workload was ﬁrst devel-\\noped for each subject from a calibration set of data. In this case, the calibration data\\nused to create the subject-speciﬁc index of cognitive workload included samples of\\nthe subject’s 0-back and 3-back WM task EEG data. The resulting function was then\\napplied to windowed samples of that subject’s data from the quiet resting condition,\\nfrom samples of 0-back and 3-back data not included in the calibration data set,\\nand from samples of data during performance of the various naturalistic types of\\ncomputer-based work.\\nA summary of the results from these analyses, averaged across data segments'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='and from samples of data during performance of the various naturalistic types of\\ncomputer-based work.\\nA summary of the results from these analyses, averaged across data segments\\nwithin each task condition and compared between conditions, is presented in ﬁgure\\n4. These comparisons indicated that the cognitive load index performed in a pre-\\ndictable fashion. That is, the condition in which the subject was asked to sit quietly\\nand passively view a blank screen produced an average EEG-based cognitive work-\\nload around the zero point of the scale. Average index values during 0-back task\\nperformance were slightly higher than those during the resting condition, and aver-\\nage index values during the 3-back task were signiﬁcantly higher than those recorded\\neither during the 0-back WM task or during the resting state. All three naturalistic\\ntasks produced workload index values slightly higher than that obtained in the 3-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 11, 'page_label': '10'}, page_content='either during the 0-back WM task or during the resting state. All three naturalistic\\ntasks produced workload index values slightly higher than that obtained in the 3-\\nback task, which might be expected given that the n-back tasks had been practiced\\nand were repetitive in nature, whereas the other tasks were novel and required the\\nuse of strategies of information gathering, reasoning and responding that were less\\nstereotyped in form. Among the naturalistic tasks, the highest levels of cognitive\\nworkload were recorded during the computerized aptitude-testing task—the con-\\ndition that was also subjectively experienced as the most diﬃcult.\\nEEGmonitoringofcognitiveworkload 123'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 12, 'page_label': '11'}, page_content='This pattern of results is interesting, not only because it conforms witha priori\\nexpectations about how workload would vary among the diﬀerent tasks, but also\\nbecause it provides data relevant to the issue of how the workload measure is\\naﬀected by diﬀerences in perceptuomotor demands across conditions. Since in the\\nn-back tasks, stimuli and motor demands are kept constant between the 0-back and\\n3-back load levels, the observed EEG diﬀerences in those conditions are clearly\\nclosely related to diﬀerences in the amounts of mental work demanded by the two\\ntask variants rather than other factors. However, in the study of MATB task per-\\nformancedescribed above, thesource ofvariation inthe index is somewhat less clear.\\nOn the one hand, performance and subjective measures unambiguously indicated\\nthat the mental eﬀort required to perform the high load version of the MATB was\\nsubstantially greater than that required by the low load (or passive watching) ver-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 12, 'page_label': '11'}, page_content='that the mental eﬀort required to perform the high load version of the MATB was\\nsubstantially greater than that required by the low load (or passive watching) ver-\\nsions. On the other hand, the perceptuomotor requirements in the high load version\\nwere also substantially greater than those imposed by the other version. In this latter\\nexperiment, such confounds was less of a concern. Indeed, both the text editing task\\nand the web searching task required more eﬀortful visual search and more active\\nphysical responding than the aptitude test, whereas the aptitude test had little read-\\ning and less responding and instead required a great deal of thinking and mental\\nevaluation of possibilities. Thus, the fact that the average cognitive workload values\\nduring performance of the aptitude test were higher than those observed in the other\\ntasks provides convergent support for the notion that the subject-speciﬁc indices'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 12, 'page_label': '11'}, page_content='during performance of the aptitude test were higher than those observed in the other\\ntasks provides convergent support for the notion that the subject-speciﬁc indices\\nwere more closely tracking variations in mental demands rather than variations in\\nperceptuomotor demands in these instances.\\n124 A.Gevins andM.E.Smith\\nCOGNITIVE LOA D DURING HCI\\n-0.3\\n0\\n0.3\\n0.6\\n0.9\\n1.2\\n1.5\\neyes open\\nresting\\n0-back 3-back speeded\\ntext\\nediting\\naptitude\\ntest taking\\nspeeded\\nweb\\nsearching\\nindex value\\n Figure 4. Mean and SEM (n¼ 7) EEG-based cognitive workload index values during rest-\\ning conditions, easy and diﬃcult versions of the n-back WM tasks, and a few naturalistic\\ntypesofcomputer-based work(seetextforfulldescription oftasksandprocedure).Thedata\\nrepresent average index values over the course of each type of task. The easy WM and\\nresting conditions produced signiﬁcantly lower values than the more diﬃcult WM condition\\nor during the naturalistic tasks.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 13, 'page_label': '12'}, page_content='5. Conclusions\\nIn summary, the results reviewed above indicate that the EEG changes in a highly\\npredictable way in response to sustained changes in task load and associated changes\\nin the mental eﬀort required for task performance. It appears that such changes can\\nbe automatically detected and measured using algorithms that combine parameters\\nof the EEG power spectra into multivariate functions. Such methods can be eﬀective\\nboth in gauging the variations in cognitive workload imposed by highly controlled\\nlaboratory tasks and in monitoring diﬀerences in the mental eﬀort required to\\nperform tasks that more closely resemble those that an individual might encounter\\nin a real-world work environment.\\nThe results presented would beneﬁt from further replication, and are in need of\\nsigniﬁcant reﬁnement. For example, the data presented herein collapsed cognitive\\nactivity over ‘whole-tasks’, that is, the data were collapsed over many minutes of'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 13, 'page_label': '12'}, page_content='signiﬁcant reﬁnement. For example, the data presented herein collapsed cognitive\\nactivity over ‘whole-tasks’, that is, the data were collapsed over many minutes of\\nsustained performance. However, cognitive workload indices were calculated over\\ndata segments that were frequently updated and were of much shorter duration.\\nFuture work will need to try to identify how such momentary measures of cognitive\\nworkload vary with speciﬁc intra-task events. One possibility for using EEG spectral\\nmeasures to evaluate cognitive load in response to speciﬁc task events is to employ\\n‘event-related desynchonization’ (ERD) methods that compare post-stimulus power\\nto a pre-stimulus baseline measure and use degree of change in the spectra as a\\nload measure. However, past eﬀorts to evaluate such measurements in somewhat\\nnaturalistic tasks (in fact, during the MATB) have found that, although useful in\\nsingle tasks, the ERD is insensitive to workload variations in multi-tasking contexts'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 13, 'page_label': '12'}, page_content='naturalistic tasks (in fact, during the MATB) have found that, although useful in\\nsingle tasks, the ERD is insensitive to workload variations in multi-tasking contexts\\n(Fournier et al.1999). This failure likely reﬂects the fact that, in such contexts,\\nworkload is likely to be relatively high even in the pre-stimulus baseline period,\\nand so any stimulus related change in the EEG spectra is likely to be fairly small.\\nIt is an open question whether other types of EEG-based methods might be more\\nfruitfully applied in such circumstances.\\nAnother area of future reﬁnement is related to the current unitary nature of the\\ncognitive workload measures. That is, some views of the structure of the mental\\nresources that can be allocated to task performance posit a relative independence\\nofthe resources involvedwithcognitive processesand those involved with perceptual\\nprocessing and motor expression. Future development of such methods should, thus,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 13, 'page_label': '12'}, page_content='ofthe resources involvedwithcognitive processesand those involved with perceptual\\nprocessing and motor expression. Future development of such methods should, thus,\\nexplore the possibility of developing somewhat orthogonal physiological indices that\\ncan diﬀerentiate between the loading of one or another type of neural resource\\nsystem. While the need for such future reﬁnements is clear, the current results, none-\\ntheless, provide compelling initial evidence for the feasibility of creating EEG-based\\ntechnologies for monitoring cognitive workload during human–computer inter-\\naction.\\nAcknowledgements\\nThis research was supported by several agencies of the US Government including\\nthe Air Force Oﬃce of Scientiﬁc Research, the National Science Foundation and\\nthe National Aeronautics and Space Administration. We thank Drs Harrison\\nLeong, Robert Du and Linda McEvoy for their contributions to the studies reviewed\\nherein.\\nEEGmonitoringofcognitiveworkload 125'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='References\\nAnderson,J.R. and Boyle,S.F. 1987, Cognitive principles in the design of computer tutors,\\nin P. Morris (ed.),ModellingCognition (London: John Wiley & Sons Ltd), 93–133.\\nBaddeley,A. 1992, Working Memory,Science, 255, 556–559.\\nBaker,S.C.,Rogers,R.D.,Owen,A.M.,Frith,C.D.,Dolan,R.J.,Frackowiak,R.S.\\nand R o b b i n s ,T .W .1996, Neural systems engaged by planning: a PET study of the\\nTower of London task,Neuropsychologia, 34, 515–526.\\nBarlow,J.S. 1986, Artifact processing rejection and reduction in EEG data processing, in\\nF. H. Lopes da Silva, W. Storm van Leeuwen and A. Remond (eds),Handbook of\\nElectroencephalography and Clinical Neurophysiology Vol 2(Amsterdam: Elsevier),\\n15–65.\\nBerg,P. and Scherg,M. 1994, A multiple source approach to the correction of eye artifacts,\\nElectroencephalographyandClinicalNeurophysiology , 90, 229–241.\\nBerger, H.1929, Uber das Elektroenzephalogramm des Menschen,Archives of Psychiatry,\\n87, 527–570.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='ElectroencephalographyandClinicalNeurophysiology , 90, 229–241.\\nBerger, H.1929, Uber das Elektroenzephalogramm des Menschen,Archives of Psychiatry,\\n87, 527–570.\\nB r a v e r ,T .S . ,C o h e n ,J .D . ,N y s t r o m ,L .E . ,J o n i d e s ,J . ,S m i t h ,E .E .and Noll, D. C.\\n1997, A parametric study of prefrontal cortex involvement in human working\\nmemory, Neuroimage, 5, 49–62.\\nBunge,S.A.,Klingberg,T.,Jacobsen,R.B. and Gabrieli,J.D. 2000, A resource model\\nof the neural basis of executive working memory,ProceedingsoftheNationalAcademy\\nofSciences(USA) , 97, 3573–3578.\\nByrne, E. A. and Parasuraman, R. 1996, Psychophysiology and adaptive automation,\\nBiologicalPsychology, 42, 249–268.\\nCadiz,J.J.,Venolia,G.D.,Jancke,G. and Gupta,A., 2001, Sideshow:Providingperiph-\\neralawarenessofimportantInformation (MSR-TR-2001-83, Redmond, WA: Microsoft\\nResearch, Microsoft Corporation).\\nC a r d ,S .K . ,M o r a n ,T .P .and Newell, A. 1983, The Psychology of Human-Computer'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='eralawarenessofimportantInformation (MSR-TR-2001-83, Redmond, WA: Microsoft\\nResearch, Microsoft Corporation).\\nC a r d ,S .K . ,M o r a n ,T .P .and Newell, A. 1983, The Psychology of Human-Computer\\nInteraction(Hillsdale, NJ: Lawrence Erlbaum Associates, Inc).\\nCarlson,R.A.,Sullivan,M.A. and Schneider,W. 1989, Practice and working memory\\neﬀects in building procedural skill, Journal of Experimental Psychology: Learning,\\nMemoryandCognition , 3, 517–526.\\nCarpenter, P. A., Just, M. A.and Reichle, E. D.2000, Working memory and executive\\nfunction: evidence from neuroimaging,CurrentOpinioninNeurobiology , 10, 195–199.\\nCarpenter,P.A.,Just,M.A. and Shell,P. 1990, What one intelligence test measures: a\\ntheoretical account of the processing in the Raven Progressive Matrices Test,\\nPsychological Review, 97, 404–431.\\nCarskadon,M.A. and Rechtschaffen,A. 1989, Monitoring and staging human sleep, in\\nM. H. Kryger, T. Roth and W. C. Dement (eds), Principles and Practice of Sleep'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='Psychological Review, 97, 404–431.\\nCarskadon,M.A. and Rechtschaffen,A. 1989, Monitoring and staging human sleep, in\\nM. H. Kryger, T. Roth and W. C. Dement (eds), Principles and Practice of Sleep\\nMedicine,2 nd edn (Philadelphia: W.B. Saunders & Co), 943–960.\\nChapman, R. M., Armington, J. C.and Bragden, H. R.1962, A quantitative survey of\\nkappa and alpha EEG activity,Electroencephalography and Clinical Neurophysiology,\\n14, 858–868.\\nCohen, J. D., Forman, S. D., Braver, T. S., Casey, B. J., Servan-Schreiber, D.and\\nNoll, D. C.1994, Activation of prefrontal cortex in a non-spatial working memory\\ntask with functional MRI,HumanBrainMapping , 1, 293–304.\\nC o m s t o c k,J .R .and Arnegard, R. J.1992, The Multi-Attribute Task Battery for Human\\nOperator Workload and Strategic Behavior Research (104174: NASA Technical\\nMemorandum).\\nDu, W., Leong, H. M.and Gevins, A. S.1994, Ocular artifact minimization by adaptive\\nﬁltering, ProceedingsoftheSeventhIEEESPWorkshoponStatisticalSignalandArray'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 14, 'page_label': '13'}, page_content='Memorandum).\\nDu, W., Leong, H. M.and Gevins, A. S.1994, Ocular artifact minimization by adaptive\\nﬁltering, ProceedingsoftheSeventhIEEESPWorkshoponStatisticalSignalandArray\\nProcessing, Quebec City, Canada, 433–436.\\nEngle,R.W.,Tuholski,S. and Kane,M. 1999, Individual diﬀerences in working memory\\ncapacity and what they tell us about controlled attention, general ﬂuid intelligence and\\nfunctions of the prefrontal cortex, in A. Miyake and P. Shah (eds),ModelsofWorking\\nMemory(Cambridge: Cambridge University Press), 102–134.\\nF o u r n i e r ,L .R . ,W i l s o n ,G .F .and Swain, C. R.1999, Electrophysiological, behavioral,\\nand subjective indexes of workload when performing multiple tasks: manipulations of\\ntask diﬃculty and training,InternationalJournalofPsychophysiology , 31, 129–145.\\n126 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='Galin,D.,Johnstone,J. and Herron,J. 1978, Eﬀects of task diﬃculty on EEG measures of\\ncerebral engagement,Neuropsychologia, 16, 461–472.\\nGaravan,H.,Ross,T.J.,Li,S. and Stein,E.A. 2000, A parametric manipulation of central\\nexecutive functioning,CerebralCortex, 10, 585–592.\\nGevins, A. and Cutillo, B. 1993, Spatiotemporal dynamics of component processes in\\nhuman working memory, Electroencephalography and Clinical Neurophysiology, 87,\\n128–143.\\nGevins, A. and Morgan, N. 1986, Classiﬁer-directed signal processing in brain research,\\nIEEETransactionsonBiomedicalEngineering , 33, 1058–1064.\\nGevins, A. and Smith, M. E. 1999, Detecting transient cognitive impairment with EEG\\npattern recognition methods, Aviation Space and Environmental Medicine, 70, 1018–\\n1024.\\nGevins, A. and Smith, M. E.2000, Neurophysiological measures of working memory and\\nindividual diﬀerences in cognitive ability and cognitive style,CerebralCortex, 10, 829–\\n839.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='1024.\\nGevins, A. and Smith, M. E.2000, Neurophysiological measures of working memory and\\nindividual diﬀerences in cognitive ability and cognitive style,CerebralCortex, 10, 829–\\n839.\\nG e v i n s ,A . ,L e o n g ,H . ,S m i t h ,M .E . ,L e ,J .and Du, R. 1995, Mapping cognitive\\nbrain function with modern high-resolution electroencephalography, Trends in\\nNeurosciences, 18, 429–436.\\nGev ins,A .,S mith,M.E.,L eong,H.,M cEv oy ,L. ,Wh itfield,S .,Du,R.and Rush,G.\\n1998, Monitoring working memory load during computer-based tasks with EEG\\npattern recognition methods,HumanFactors, 40, 79–91.\\nGevins,A.,Smith,M.E.,McEvoy,L. and Yu,D. 1997, High-resolution EEG mapping of\\ncortical activation related to working memory: eﬀects of task diﬃculty, type of process-\\ning, and practice,CerebralCortex, 7, 374–385.\\nGevins,A.S. 1980, Pattern recognition of brain electrical potentials,IEEE Transactions on\\nPatternAnalysisandMachineIntelligence , 2, 383–404.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='ing, and practice,CerebralCortex, 7, 374–385.\\nGevins,A.S. 1980, Pattern recognition of brain electrical potentials,IEEE Transactions on\\nPatternAnalysisandMachineIntelligence , 2, 383–404.\\nGevins,A.S. and Morgan,N.H. 1988, Applications of neural-network (NN) signal proces-\\nsing in brain research,IEEETransactions onAcoustics, Speech, andSignalProcessing ,\\n36, 1152–1161.\\nGevins,A.S. and Schaffer,R.E. 1980, A critical review of electroencephalographic EEG\\ncorrelates of higher cortical functions,CRCCriticalReviewsinBioengineering , 4, 113–\\n164.\\nGevins,A.S.,Bressler,S.L.,Cutillo,B.A.,Illes,J.,Miller,J.C.,Stern,J. and Jex,\\nH. R. 1990, Eﬀects of prolonged mental work on functional brain topography,\\nElectroencephalographyandClinicalNeurophysiology , 76, 339–350.\\nGevins, A. S., Doyle, J. C., Schaffer, R. E., Callaway, E. and Yeager, C. 1980,\\nLateralized cognitive processes and the electroencephalogram,Science, 207, 1005–1008.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='Gevins, A. S., Doyle, J. C., Schaffer, R. E., Callaway, E. and Yeager, C. 1980,\\nLateralized cognitive processes and the electroencephalogram,Science, 207, 1005–1008.\\nGevins,A.S.,Smith,M.E.,Le,J.,Leong,H.,Bennett,J.,Martin,N.,McEvoy,L.,Du,\\nR. and Whitfield,S. 1996, High resolution evoked potential imaging of the cortical\\ndynamics of human working memory, Electroencephalography and Clinical Neuro-\\nphysiology, 98, 327-348.\\nGevins,A.S.,Yeager,C.L.,Diamond,S.L.,Spire,J.P.,Zeitlin,G.M. and Gevins,A.\\nH. 1975, Automated analysis of the electrical activity of the human brain (EEG): a\\nprogress report,ProceedingsoftheInstituteofElectricalandElectronicsEngineers , 63,\\n1382–1399.\\nGevins, A. S., Yeager, C. L., Diamond, S. L., Spire, J. P., Zeitlin, G. M.and Gevins,\\nA.H. 1976, Sharp-transient analysis and thresholded linear coherence spectra of par-\\noxysmal EEGs, in P. Kellaway and I. Petersen (eds), Quantitative Analytic Studies in\\nEpilepsy(New York: Raven Press), 463–481.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 15, 'page_label': '14'}, page_content='oxysmal EEGs, in P. Kellaway and I. Petersen (eds), Quantitative Analytic Studies in\\nEpilepsy(New York: Raven Press), 463–481.\\nGevins, A. S., Yeager, C. L., Zeitlin, G. M., Ancoli, S.and Dedon, M. 1977, On-line\\ncomputer rejection of EEG artifact, Electroencephalography and Clinical\\nNeurophysiology, 42, 267–274.\\nG e v i n s ,A .S . ,Z e i t l i n ,G .M . ,D o y l e ,J .C . ,S c h a f f e r ,R .E .and Callaway, E. 1979a,\\nEEG patterns during ‘cognitive’ tasks. II. Analysis of controlled tasks,\\nElectroencephalographyandClinicalNeurophysiology , 47, 704–710.\\nGevins, A. S., Zeitlin, G. M., Doyle, J. C., Yingling, C. D., Schaffer, R. E.,\\nCallaway, E. and Y e a g e r ,C .L .1979b, Electroencephalogram correlates of higher\\ncortical functions,Science, 203, 665–668.\\nEEGmonitoringofcognitiveworkload 127'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='Gevins, A. S., Zeitlin, G. M., Yingling, C. D., Doyle, J. C., Dedon, M. F., Schaffer,\\nR. E., Roumasset, J. T.and Yeager, C. L.1979c, EEG patterns during ‘cognitive’\\ntasks. I. Methodology and analysis of complex behaviors,Electroencephalography and\\nClinicalNeurophysiology, 47, 693–703.\\nGilliam,F.,Kuzniecky,R. and Faught,E. 1999, Ambulatory EEG monitoring,Journalof\\nClinicalNeurophysiology, 16, 111–115.\\nGoldman-Rakic,P. 1987, Circuitry of primate prefrontal cortex and regulation of behavior\\nby representational memory, in F. Plum and V. Mountcastle (ed.), Handbook of\\nPhysiology, The Nervous System—Higher Functions of the Brain,1 st edn, Vol. 5\\n(Bethesda, MD: American Physiological Society), 373–417.\\nGoldman-Rakic,P. 1988, Topography of cognition: parallel distributed networks in primate\\nassociation cortex,AnnualReviewofNeuroscience , 11, 137–156.\\nGundel,A. and Wilson,G.F. 1992, Topographical changes in the ongoing EEG related to\\nthe diﬃculty of mental tasks,BrainTopography, 5, 17–25.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='Gundel,A. and Wilson,G.F. 1992, Topographical changes in the ongoing EEG related to\\nthe diﬃculty of mental tasks,BrainTopography, 5, 17–25.\\nHumphrey, D. and K r a m e r ,A .F .1994, Toward a psychophysiological assessment of\\ndynamic changes in mental workload,HumanFactors, 36, 3–26.\\nInouye, T., Shinosaki, K., Iyama, A., Matsumoto, Y., Toi, S.and Ishihara, T. 1994,\\nPotential ﬂow of frontal midline theta activity during a mental task in the human\\nelectroencephalogram, NeuroscienceLetters, 169, 145–148.\\nIshii,R.,Shinosaki,K.,Ukai,S.,Inouye,T.,Ishihara,T.,Yoshimine,T.,Hirabuki,N.,\\nAsada, H., Kihara, T., Robinson, S. E.and Takeda, M. 1999, Medial prefrontal\\ncortex generates frontal midline theta rhythm,Neuroreport, 10, 675–679.\\nI s r e a l ,J .B . ,W i c ke n s ,C .D . ,C h e s n e y ,G .L .and Donchin, E. 1980, The event-related\\nbrain potential as an index of display-monitoring workload,Human Factors, 22, 211–\\n224.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='I s r e a l ,J .B . ,W i c ke n s ,C .D . ,C h e s n e y ,G .L .and Donchin, E. 1980, The event-related\\nbrain potential as an index of display-monitoring workload,Human Factors, 22, 211–\\n224.\\nJansma,J.M.,Ramsey,N.F.,Coppola,R. andKahn,R.S. 2000, Speciﬁc versus nonspeciﬁc\\nbrain activity in a parametric n-back task,Neuroimage, 12, 688–697.\\nJohn, E. R., Prichep, L. S., Kox, W., Valdes-Sosa, P., Bosch-Bayard, J., Aubert, E.,\\nTom,M.,diMichele,F. and Gugino,L.D. 2001, Invariant reversible QEEG eﬀects\\nof anesthetics,ConsciousnessandCognition , 10, 165–183.\\nJonides,J.,Smith,E.E.,Koeppe,R.A.,Awh,E.,Minoshima,S. and Mintun,M. 1993,\\nSpatial working memory in humans as revealed by PET,Nature, 363, 623–625.\\nJ o s e p h ,R .D .1961, Contributions of perceptron theory, unpublished PhD thesis, Cornell\\nUniversity, Ithaca, New York.\\nJ u n g ,T .P . ,M a ke i g ,S . ,H u m p h r i e s ,C . ,L e e ,T .W . ,M c K e o w n ,M .J . ,I r a g u i ,V .and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='University, Ithaca, New York.\\nJ u n g ,T .P . ,M a ke i g ,S . ,H u m p h r i e s ,C . ,L e e ,T .W . ,M c K e o w n ,M .J . ,I r a g u i ,V .and\\nSejnowski, T. J. 2000, Removing electroencephalographic artifacts by blind source\\nseparation, Psychophysiology, 37, 163–178.\\nKennedy, J. L., Gottsdanker, R. M., Arinington, J. C.and Gray, F. E.1948, A new\\nelectroencephalogram associated with thinking,Science, 108, 527.\\nKieras,D.E. 1988, Towards a practical GOMS model methodology for user interface design,\\nin M. Helander (ed.), The Handbook of Human–Computer Interaction(Amsterdam:\\nNorth-Holland), 135–158.\\nKoivisto, M., Krause, C. M., Revonsuo, A., Laine, M.and Hamalainen, H. 2000, The\\neﬀects of electromagnetic ﬁeld emitted by GSM phones on working memory,\\nNeuroreport, 11, 1641–1643.\\nKok, A. 2001, On the utility of P3 amplitude as a measure of processing capacity,\\nPsychophysiology, 38, 557–577.\\nKramer,A.F.,Trejo,L.J. and Humphrey,D. 1995, Assessment of mental workload with'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 16, 'page_label': '15'}, page_content='Kok, A. 2001, On the utility of P3 amplitude as a measure of processing capacity,\\nPsychophysiology, 38, 557–577.\\nKramer,A.F.,Trejo,L.J. and Humphrey,D. 1995, Assessment of mental workload with\\ntask-irrelevant auditory probes,BiologicalPsychology, 40, 83–100.\\nKyllonen, P. C.and Christal, R. E.1990, Reasoning ability is little more than working\\nmemory capacity?!,Intelligence, 14, 389–433.\\nKyllonen,P.C. and Shute,V.J. 1989, A taxonomy of learning skills, in P. L. Ackerman\\n(ed.), LearningandIndividualDiﬀerences (New York: Freeman), 117–163.\\nLarson, C. L., Davidson, R. J., Abercrombie, H. C., Ward, R. T., Schaefer, S. M.,\\nJackson, D. C., Holden, J. E.and Perlman, S. B.1998, Relations between PET-\\nderived measures of thalamic glucose metabolism and EEG alpha power,\\nPsychophysiology, 35, 162–169.\\n128 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='McCallum, W. C., Cooper, R.and Pocock, P. V.1988, Brain slow potential and ERP\\nchanges associated with operator load in a visual tracking task,Electroencephalography\\nandclinicalNeurophysiology , 69, 453–468.\\nM c C a r t h y ,G . ,B l a m i r e ,A .M . ,P u c e ,A . ,N o b r e ,A .C . ,B l o c h ,G . ,H y d e r ,F . ,\\nGoldman-Rakic, P. and Shulman, R. G. 1994, Functional magnetic resonance\\nimaging of human prefrontal cortex activation during a spatial working memory\\ntask, ProceedingsoftheNationalAcademyofScience(USA) , 91, 8690–8694.\\nMcElree, B. 2001, Working memory and focal attention, Journal of Experimental\\nPsychology:Learning, MemoryandCognition , 27, 817–835.\\nMcEvoy,L.K.,Pellouchoud,E.,Smith,M.E. and Gevins,A. 2001, Neurophysiological\\nsignals of working memory in normal aging,CognitiveBrainResearch , 11, 363–376.\\nMcEvoy,L.K.,Smith,M.E. and Gevins,A. 1998, Dynamic cortical networks of verbal and\\nspatial working memory: eﬀects of memory load and task practice,CerebralCortex, 8,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='McEvoy,L.K.,Smith,M.E. and Gevins,A. 1998, Dynamic cortical networks of verbal and\\nspatial working memory: eﬀects of memory load and task practice,CerebralCortex, 8,\\n563–574.\\nMcEvoy,L.K.,Smith,M.E. and Gevins,A. 2000, Test-retest reliability of cognitive EEG,\\nClinicalNeurophysiology, 111, 457–463.\\nMiyata, Y., Tanaka, Y.and Hono, T.1990, Long term observation on Fm-theta during\\nmental eﬀort,Neuroscience, 16, 145–148.\\nMizuki,Y.,Tanaka,M.,Iogaki,H.,Nishijima,H. andInanaga,K. 1980, Periodic appear-\\nances of theta rhythm in the frontal midline area during performance of a mental task,\\nElectroencephalographyandClinicalNeurophysiology , 49, 345–351.\\nMolloy, R. and Parasuraman, R. 1996, Monitoring an automated system for a single\\nfailure: vigilance and task complexity eﬀects,HumanFactors, 38, 311–322.\\nMorrison,J.G. and Gluckman,J.P. 1994, Deﬁnitions and prospective guidelines for the\\napplication of automation, in M. Mouloua and R. Parasuraman (eds),Human perfor-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='Morrison,J.G. and Gluckman,J.P. 1994, Deﬁnitions and prospective guidelines for the\\napplication of automation, in M. Mouloua and R. Parasuraman (eds),Human perfor-\\nmance in automated systems: Current research and trends(Hillsdale, NJ: Lawrence\\nErlbaum Associates), 256–263.\\nMulholland, T. 1995, Human EEG, behavioral stillness and biofeedback, International\\nJournalofPsychology , 19, 263–279.\\nO’Connor,M.F.,Daves,S.M.,Tung,A.,Cook,R.I.,Thisted,R. and Appelbaum,J.\\n2001, BIS monitoring to prevent awareness during general anesthesia,Anesthesiology,\\n94, 520–522.\\nO l i v e r i ,M . ,T u r r i z i a n i ,P . ,C a r l e s i m o ,G .A . ,K o c h ,G . ,T o m a i u o l o ,F . ,P a n e l l a ,M .\\nand Caltagirone, C. 2001, Parieto-frontal interactions in visual-object and visual-\\nspatial working memory: evidence for transcranial magnetic stimulation, Cerebral\\nCortex, 11, 606–618.\\nOlson,J.R. and Olson,G.M. 1990, The growth of cognitive modeling in human-computer'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='spatial working memory: evidence for transcranial magnetic stimulation, Cerebral\\nCortex, 11, 606–618.\\nOlson,J.R. and Olson,G.M. 1990, The growth of cognitive modeling in human-computer\\ninteraction since GOMS,Human-ComputerInteraction, 5, 221-265.\\nParasuraman,R.,Molloy,R. and Singh,I.L. 1993, Performance consequences of auto-\\nmation-induced ‘complacency’,InternationalJournalofAviationPsychology , 3, 1–23.\\nParasuraman,R.,Mouloua,M. and Molloy,R. 1996, Eﬀects of adaptive task allocation\\non monitoring of automated systems,HumanFactors, 38, 665–679.\\nParasuraman,R.,Sheridan,T.B. and Wickens,C.D. 2000, A model for types and levels\\nof human interaction with automation, IEEE Transactions Systems, Man, and\\nCybernetics-Part A:SystemsandHumans , 30, 286–297.\\nPaus, T., Koski, L., Caramanos, Z.and Westbury, C. 1998, Regional diﬀerences in the\\neﬀects of task diﬃculty and motor output on blood ﬂow response in the human anterior'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 17, 'page_label': '16'}, page_content='Paus, T., Koski, L., Caramanos, Z.and Westbury, C. 1998, Regional diﬀerences in the\\neﬀects of task diﬃculty and motor output on blood ﬂow response in the human anterior\\ncingulate cortex: a review of 107 PET activation studies,Neuroreport, 9, R37–R47.\\nP e l l o u c h o u d ,E . ,S m i t h ,M .E . ,M c E v o y ,L .and Gevins, A.1999, Mental eﬀort-related\\nEEG modulation during video-game play: comparison between juvenile subjects with\\nepilepsy and normal control subjects,Epilepsia, 40 (Suppl 4), 38–43.\\nPfurtscheller, G. and Klimesch, W. 1992, Functional topography during a visuoverbal\\njudgment task studied with event-related desynchronization mapping, Journal of\\nClinicalNeurophysiology, 9, 120–131.\\nPosner,M.I. and Peterson,S.E. 1990, The attention system of the human brain,Annual\\nReview ofNeuroscience, 13, 25–42.\\nEEGmonitoringofcognitiveworkload 129'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='Posner,M.I. andRothbart,M.K. 1992, Attentional mechanisms and conscious experience,\\nin A. D. Milner and M. D. Rugg (eds),The Neuropsychology of Consciousness(San\\nDiego: Academic Press), 91–111.\\nRaskin,J. 2000, HumaneInterface:NewDirectionsforDesigningInteractiveSystems (Boston,\\nMA: Addison-Wesley).\\nRockstroh, B., Elbert, T., Canavan, A., Lutzenberger, W.and Birbaumer, N. 1989,\\nSlowcorticalpotentialsandbehavior (Baltimore: Urban & Schwarzenberg).\\nRoss, P. and Segalowitz, S. J.2000, An EEG coherence test of the frontal dorsal versus\\nventral hypothesis of n-back working memory,BrainandCognition , 43, 375–379.\\nSadato, N., Nakamura, S., Oohashi, T., Nishina, E., Fuwamoto, Y., Waki, A. and\\nYonekura, Y. 1998, Neural networks for generation and suppression of alpha\\nrhythm: a PET study,Neuroreport, 30, 893–897.\\nSheer,D.E. 1989, Sensory and cognitive 40Hz event-related potentials, in E. Basar and T. H.\\nBullock (eds),BrainDynamics, Vol. 2 (Berlin: Springer), 339–374.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='Sheer,D.E. 1989, Sensory and cognitive 40Hz event-related potentials, in E. Basar and T. H.\\nBullock (eds),BrainDynamics, Vol. 2 (Berlin: Springer), 339–374.\\nSirevaag,E.J.,Kramer,A.F. and Wickens,C.D. 1993, Assessment of pilot performance\\nand mental workload in rotary wing aircraft,Ergonomics, 36, 1121–1140.\\nSmith,M.E.,Gevins,A.,Brown,H.,Karnik,A. and Du,R. 2001, Monitoring task load\\nwith multivariate EEG measures during complex forms of human computer interaction,\\nHumanFactors, 43, 366–380.\\nSmith,M.E.,McEvoy,L.K. and Gevins,A. 1999, Neurophysiological indices of strategy\\ndevelopment and skill acquisition,BrainResearchCognitiveBrainResearch , 7, 389–404.\\nThompson,J.L. and Ebersole,J.S. 1999, Longterm inpatient audiovisual scalp EEG mon-\\nitoring, JournalofClinicalNeurophysiology , 16, 91–99.\\nTou, J. T. and G o n z a l e z ,R .C .1974, Pattern Recognition Principles(Reading, MA:\\nAddison-Wesley Publishing Co).'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='itoring, JournalofClinicalNeurophysiology , 16, 91–99.\\nTou, J. T. and G o n z a l e z ,R .C .1974, Pattern Recognition Principles(Reading, MA:\\nAddison-Wesley Publishing Co).\\nUllsperger,P.,Freude,G. and Erdmann,U. 2001, Auditory probe sensitivity to mental\\nworkload changes—an event-related potential study, International Journal of\\nPsychophysiology, 40, 201–209.\\nVandenBerg-Lensssen,M.M.,Brunia,C.H. and Blom,J.A. 1989, Correction of ocular\\nartifacts in EEGs using an autoregressive model to describe the EEG: a pilot study,\\nElectroencephalographyandClinicalNeurophysiology , 73, 72–83.\\nVespa,P.,Nenov,V. and Nuwer,M.R. 1999, Continuous EEG monitoring in the intensive\\ncare unit: early ﬁndings and clinical eﬃcacy,Journal of Clinical Neurophysiology, 16,\\n1–13.\\nViglione, S.S. 1970, Applications of pattern recognition technology, in J. M. Mendel and\\nK. S. Fu (eds), Adaptive Learning and Pattern Recognition Systems(New York:\\nAcademic Press), 115–161.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='1–13.\\nViglione, S.S. 1970, Applications of pattern recognition technology, in J. M. Mendel and\\nK. S. Fu (eds), Adaptive Learning and Pattern Recognition Systems(New York:\\nAcademic Press), 115–161.\\nWilson, G. F.and Fisher, F. 1995, Cognitive task classiﬁcation based upon topographic\\nEEG data,BiologicalPsychology, 40, 239–250.\\nWilson,G.F.,Fullenkamp,B.S. and Davis,I. 1994, Evoked potential, cardiac, blink, and\\nrespiration measures of pilot workload in air-to-ground missions,Aviation, Space and\\nEnvironmentalMedicine, 65, 100–105.\\nWintink,A.J.,Segalowitz,S.J. and Cudmore,L.J. 2001, Task complexity and habitua-\\ntion eﬀects on frontal P300 topography,BrainandCognition , 46, 307–311.\\nW o l t z ,D .J .1988, An investigation of the role of working memory in procedural skill\\nacquisition, JournalofExperimentalPsychology:General , 117, 319–331.\\nYamamoto,S. and Matsuoka,S. 1990, Topographic EEG study of visual display terminal'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 18, 'page_label': '17'}, page_content='acquisition, JournalofExperimentalPsychology:General , 117, 319–331.\\nYamamoto,S. and Matsuoka,S. 1990, Topographic EEG study of visual display terminal\\nVDT performance with special reference to frontal midline theta waves, Brain\\nTopography, 2, 257–267.\\nAbout the authors\\nAlan Gevinsis the founder and Executive Director of the San Francisco Brain Research\\nInstitute and the founder and President of SAM Technology, Inc., both in San Francisco.\\nHe is internationally known for developing algorithms and systems for analysing human brain\\nfunction, and for basic science studies of human neurocognitive functions. He is the author of\\nover 125 scientiﬁc papers and of 16 US patents.\\n130 A.Gevins andM.E.Smith'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': '3B2 Total Publishing System 7.00a/W', 'creationdate': '2002-11-26T08:07:44+00:00', 'moddate': '2010-11-18T07:59:47+05:30', 'title': 'Neurophysiological measures of cognitive workload during human-computer interaction', 'source': '..\\\\data\\\\pdf\\\\Paper 20_12ec27b43cf1539e32703e09d476e429.pdf', 'total_pages': 20, 'page': 19, 'page_label': '18'}, page_content='MichaelE.Smith is a cognitive neuroscientist who specializes in basic and applied research on\\nthe neural systems mediating human attention and memory. He has authored over 50 scientiﬁc\\npapers on related topics. He holds an undergraduate degree from the University of Michigan,\\na PhD from the University of California, Los Angeles, and an MBA from the University of\\nCalifornia, Berkeley. He currently directs the research department of SAM Technology, Inc.\\nEEGmonitoringofcognitiveworkload 131'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}, page_content=\"R-CNN: Revolutionizing \\nObject Detection\\nThis presentation introduces R-CNN (Regions with Convolutional Neural \\nNetworks), a pioneering deep learning approach that transformed object \\ndetection. We'll explore its innovative architecture and the profound impact it \\nhad on the field.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}, page_content='The Fundamental Challenge: \\n\"What + Where\"\\nObject detection is more than just classification; it\\'s about simultaneously \\nidentifying \"what\" objects are present in an image and \"where\" they are located. \\nBefore R-CNN, performance in object localization struggled to advance, facing a \\nsignificant plateau. The key challenge was efficiently locating objects within an \\nimage using deep learning.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 2, 'page_label': '3'}, page_content='Evolution of Approaches\\nBrute Force: The Sliding Window\\nEarly methods relied on a sliding window approach, \\nexhaustively checking every possible region. While conceptually \\nsimple, this method proved computationally prohibitive for \\ncomplex deep learning models like CNNs due to the immense \\nnumber of regions to process.\\nSmarter Approach: Region Proposals\\nR-CNN introduced a paradigm shift: Region Proposals. Instead \\nof brute force, it generates a sparse set of potential object \\nlocations, dramatically reducing computational load. Combined \\nwith deep learning, this paved the way for modern object \\ndetection.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 3, 'page_label': '4'}, page_content=\"R-CNN: A Three-Module Pipeline\\nThe R-CNN architecture is elegantly structured into three distinct, yet interconnected, modules. This pipeline processes an input image to \\nultimately identify and precisely localize objects. Let's delve into each module to understand its contribution.\\n01\\n1. Region Proposals\\nGenerating candidate object locations.\\n02\\n2. Feature Extraction\\nExtracting rich features for each proposal.\\n03\\n3. Classification & Refinement\\nClassifying objects and fine-tuning their \\npositions.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 4, 'page_label': '5'}, page_content='Module 1: Region Proposals & IoU\\nGenerating Candidate Regions with Selective \\nSearch\\nThe first module employs Selective Search to generate approximately \\n2000 region proposals per image. This algorithm intelligently groups \\nsimilar pixels into potential object segments, acting as \"intelligent \\nguesses\" for object locations.\\nIntersection over Union (IoU)\\nT o evaluate the quality of these proposals and the accuracy of our \\ndetections, we use Intersection over Union (IoU). This metric quantifies \\nthe overlap between a predicted bounding box and the ground-truth \\nbounding box.\\nIoU is crucial for both training (determining \\npositive/negative samples) and evaluation (assessing \\ndetection accuracy).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 5, 'page_label': '6'}, page_content='Module 2: Feature Extraction \\nwith AlexNet\\nOnce region proposals are generated, they are warped to a fixed size and fed \\ninto a powerful Convolutional Neural Network (CNN). R-CNN famously \\nleveraged AlexNet, a groundbreaking CNN architecture.\\nTransfer Learning: AlexNet, pre-trained on the vast ImageNet dataset, was \\nfine-tuned on the Pascal VOC dataset for object detection.\\nFeature Vector Generation: Each warped region proposal is passed through \\nthe CNN, yielding a 4096-dimensional feature vector. These high-level \\nfeatures capture semantic information about the potential object.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 6, 'page_label': '7'}, page_content=\"Module 3: Classification & Bounding Box Regression\\nObject Classification with SoftMax\\nThe 4096-dimensional feature vector for each region proposal \\nis then fed into a class-specific linear SVM classifier. This \\nclassifier determines the presence and specific class of an \\nobject within that proposal. A SoftMax layer outputs the \\nprobability distribution over all possible object classes.\\nBounding Box Regression for Precision\\nT o achieve highly accurate localization, R-CNN incorporates a \\nbounding box regressor. This linear regression model refines \\nthe initial region proposal's coordinates, predicting offsets that \\nadjust the box to tightly enclose the object. This step \\nsignificantly improves localization precision.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 7, 'page_label': '8'}, page_content=\"Datasets & Strategic Training\\nR-CNN's success was greatly attributed to its sophisticated training strategy, \\nleveraging large, diverse datasets and the power of transfer learning.\\nImageNet (1.2 Million Images): Initial pre-training of the AlexNet CNN on \\nthis massive dataset allowed the model to learn robust, generalizable visual \\nfeatures.\\nPascal VOC (10,000 Images): This smaller, object detection-specific dataset \\nwas then used for fine-tuning the pre-trained CNN, adapting its learned \\nfeatures to the nuances of object localization.\\nTransfer learning was a critical component, enabling R-CNN to achieve high \\naccuracy with relatively less training data compared to training from scratch.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 8, 'page_label': '9'}, page_content='Impact & Legacy: A New Era of Detection\\n53.7%\\nmAP Improvement\\nR-CNN achieved a remarkable 53.7% mean Average Precision \\n(mAP) on the Pascal VOC 2012 dataset, a massive 30% relative \\nimprovement over prior state-of-the-art methods.\\n4\\nFoundational Work\\nThis groundbreaking performance cemented R-CNN as a \\nfoundational work, inspiring a wave of subsequent research and \\nforming the basis for modern object detection architectures like \\nFast R-CNN and Faster R-CNN.\\nR-CNN demonstrated that deep learning could effectively tackle the complex problem of object detection, setting a new benchmark and \\nopening up vast research avenues.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.56.1', 'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)', 'creationdate': \"D:20251127102928Z00'00'\", 'moddate': \"D:20251127102928Z00'00'\", 'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf', 'total_pages': 10, 'page': 9, 'page_label': '10'}, page_content='Further Exploration\\nThe techniques introduced in R-CNN, particularly region proposals and fine-\\ntuning CNNs for detection, remain highly influential.\\n\"Rich feature hierarchies for accurate object detection and semantic \\nsegmentation\" (R. Girshick et al.)\\nWe encourage you to explore the original research paper for a deeper dive into \\nthe mathematical and implementation details of this seminal work.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dd140",
   "metadata": {},
   "source": [
    "### Embedding and vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa6e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from chromadb.config import Settings\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce0e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "class EmbeddingManager:\n",
    "    \"\"\" Handels document embedding generation using SentenceTransformation\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializing the EmbeddingManager\n",
    "\n",
    "        Args:\n",
    "            model_name : HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Load embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimention: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings (self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate Embeddings for list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model Not Loaded\")\n",
    "        \n",
    "        print(f\"Generating Embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated Embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f4a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimention: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x25ac57757f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb550025",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7099b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Initialized. Collection:pdf_document\n",
      "Existing document in Collection:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x25ac72e5400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.trychroma.com/docs/run-chroma/persistent-client\n",
    "class VectorStore:\n",
    "    \"\"\"Manages document Embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_document\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "\n",
    "        Args:\n",
    "            collection_name: name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        # Initialize ChromaDB Client and collection\n",
    "        try:\n",
    "            # Create Persistant ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create a collection\n",
    "            self.collection = self.client.create_collection(name=self.collection_name,metadata={\"description\":\"PDF document embeddings for RAG\"})\n",
    "            print(f\"Vector Store Initialized. Collection:{self.collection_name}\")\n",
    "            print(f\"Existing document in Collection:{self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_document(self, documents:List[Any], embeddings:np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents should match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} to the vector store...\")\n",
    "\n",
    "        # Prepare data for chromadb\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            # Generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Preparing metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #Embeddings\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids= ids,\n",
    "                embeddings= embeddings_list,\n",
    "                metadatas= metadatas,\n",
    "                documents= documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a59309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Embeddings for 203 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:10<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape: (203, 384)\n",
      "Adding 203 to the vector store...\n",
      "Successfully added 203 documents to vector store\n",
      "Total documents in collection: 203\n"
     ]
    }
   ],
   "source": [
    "# Converting the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store into the vector database\n",
    "vectorstore.add_document(chunks,embeddings) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b47a7",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb41f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "332fb9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Rich feature hierarchies for accurate object detection'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating Embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_ed72026b_202',\n",
       "  'content': 'Further Exploration\\nThe techniques introduced in R-CNN, particularly region proposals and fine-\\ntuning CNNs for detection, remain highly influential.\\n\"Rich feature hierarchies for accurate object detection and semantic \\nsegmentation\" (R. Girshick et al.)\\nWe encourage you to explore the original research paper for a deeper dive into \\nthe mathematical and implementation details of this seminal work.',\n",
       "  'metadata': {'page': 9,\n",
       "   'content_length': 400,\n",
       "   'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf',\n",
       "   'creationdate': \"D:20251127102928Z00'00'\",\n",
       "   'moddate': \"D:20251127102928Z00'00'\",\n",
       "   'producer': 'GPL Ghostscript 9.56.1',\n",
       "   'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)',\n",
       "   'page_label': '10',\n",
       "   'total_pages': 10,\n",
       "   'doc_index': 202},\n",
       "  'similarity_score': 0.39538371562957764,\n",
       "  'distance': 0.6046162843704224,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_d8a891cd_0',\n",
       "  'content': 'Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset, has plateaued in the last\\nfew years. The best-performing methods are complex en-\\nsemble systems that typically combine multiple low-level\\nimage features with high-level context. In this paper, we\\npropose a simple and scalable detection algorithm that im-\\nproves mean average precision (mAP) by more than 30%\\nrelative to the previous best result on VOC 2012—achieving\\na mAP of 53.3%. Our approach combines two key insights:\\n(1) one can apply high-capacity convolutional neural net-\\nworks (CNNs) to bottom-up region proposals in order to\\nlocalize and segment objects and (2) when labeled training\\ndata is scarce, supervised pre-training for an auxiliary task,',\n",
       "  'metadata': {'subject': '',\n",
       "   'total_pages': 21,\n",
       "   'page': 0,\n",
       "   'author': '',\n",
       "   'page_label': '1',\n",
       "   'creationdate': '2014-10-23T01:08:59+00:00',\n",
       "   'content_length': 958,\n",
       "   'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'title': '',\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'keywords': '',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 0,\n",
       "   'moddate': '2014-10-23T01:08:59+00:00',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1'},\n",
       "  'similarity_score': 0.23331421613693237,\n",
       "  'distance': 0.7666857838630676,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_39f69747_89',\n",
       "  'content': 'age search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\\nChallenge. IJCV, 2010. 1, 4\\n[16] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\\nhierarchical features for scene labeling. TPAMI, 2013. 10\\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. TPAMI, 2010. 2, 4, 7, 12\\n[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up\\nsegmentation for top-down detection. In CVPR, 2013. 4, 5\\n[19] K. Fukushima. Neocognitron: A self-organizing neu-\\nral network model for a mechanism of pattern recogni-\\ntion unaffected by shift in position. Biological cybernetics,\\n36(4):193–202, 1980. 1\\n[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-',\n",
       "  'metadata': {'author': '',\n",
       "   'producer': 'pdfTeX-1.40.12',\n",
       "   'total_pages': 21,\n",
       "   'title': '',\n",
       "   'subject': '',\n",
       "   'moddate': '2014-10-23T01:08:59+00:00',\n",
       "   'trapped': '/False',\n",
       "   'page': 13,\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'doc_index': 89,\n",
       "   'creationdate': '2014-10-23T01:08:59+00:00',\n",
       "   'page_label': '14',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'keywords': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'content_length': 991},\n",
       "  'similarity_score': 0.16458022594451904,\n",
       "  'distance': 0.835419774055481,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_4d1ab1df_201',\n",
       "  'content': 'Impact & Legacy: A New Era of Detection\\n53.7%\\nmAP Improvement\\nR-CNN achieved a remarkable 53.7% mean Average Precision \\n(mAP) on the Pascal VOC 2012 dataset, a massive 30% relative \\nimprovement over prior state-of-the-art methods.\\n4\\nFoundational Work\\nThis groundbreaking performance cemented R-CNN as a \\nfoundational work, inspiring a wave of subsequent research and \\nforming the basis for modern object detection architectures like \\nFast R-CNN and Faster R-CNN.\\nR-CNN demonstrated that deep learning could effectively tackle the complex problem of object detection, setting a new benchmark and \\nopening up vast research avenues.',\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf',\n",
       "   'producer': 'GPL Ghostscript 9.56.1',\n",
       "   'page': 8,\n",
       "   'moddate': \"D:20251127102928Z00'00'\",\n",
       "   'creator': 'pdf-lib (https://github.com/Hopding/pdf-lib)',\n",
       "   'content_length': 629,\n",
       "   'page_label': '9',\n",
       "   'doc_index': 201,\n",
       "   'creationdate': \"D:20251127102928Z00'00'\",\n",
       "   'total_pages': 10},\n",
       "  'similarity_score': 0.12998533248901367,\n",
       "  'distance': 0.8700146675109863,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_542792fe_101',\n",
       "  'content': 'object detection. In CVPR, 2013. 6, 7\\n[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-\\nbased face detection. TPAMI, 1998. 2\\n[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-\\ning internal representations by error propagation. Parallel\\nDistributed Processing, 1:318–362, 1986. 1\\n[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. OverFeat: Integrated Recognition, Localiza-\\ntion and Detection using Convolutional Networks. In ICLR,\\n2014. 1, 2, 4, 10\\n[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y . LeCun.\\nPedestrian detection with unsupervised multi-stage feature\\nlearning. In CVPR, 2013. 2\\n[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations\\nfor visual object detection. In AAAI Technical Report, 4th\\nHuman Computation Workshop, 2012. 8\\n[37] K. Sung and T. Poggio. Example-based learning for view-\\nbased human face detection. Technical Report A.I. Memo\\nNo. 1521, Massachussets Institute of Technology, 1994. 4',\n",
       "  'metadata': {'author': '',\n",
       "   'title': '',\n",
       "   'page': 18,\n",
       "   'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'doc_index': 101,\n",
       "   'total_pages': 21,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       "   'creationdate': '2014-10-23T01:08:59+00:00',\n",
       "   'moddate': '2014-10-23T01:08:59+00:00',\n",
       "   'trapped': '/False',\n",
       "   'keywords': '',\n",
       "   'page_label': '19',\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref package',\n",
       "   'content_length': 976,\n",
       "   'producer': 'pdfTeX-1.40.12'},\n",
       "  'similarity_score': 0.12650376558303833,\n",
       "  'distance': 0.8734962344169617,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Rich feature hierarchies for accurate object detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37eaa41",
   "metadata": {},
   "source": [
    "## Integration Vector Context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9cb1e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaLLM(model='llama3.2')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Initialize llm model \n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c715747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    # Retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results]) if results else \"\"\n",
    "    \n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question\"\n",
    "    \n",
    "    # Generate the answer using LLM\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Use the following context to answer the question concisely.\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = prompt.format(context=context, query=query)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    # Handle different response types\n",
    "    if hasattr(response, 'content'):\n",
    "        # LangChain/ChatModel response\n",
    "        return response.content\n",
    "    elif isinstance(response, str):\n",
    "        # Raw string response (like from Ollama)\n",
    "        return response\n",
    "    elif hasattr(response, 'text'):\n",
    "        # Some LLMs return objects with .text attribute\n",
    "        return response.text\n",
    "    else:\n",
    "        # Fallback: convert to string\n",
    "        return str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4c94d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Rich feature hierarchies for accurate object detection'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating Embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 73.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is Rich feature hierarchies for accurate object detection\",rag_retriever,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88c903d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rich feature hierarchies for accurate object detection refer to the use of high-capacity convolutional neural networks (CNNs) to extract features from bottom-up region proposals, improving mean average precision (mAP) by more than 30% relative to previous best results on object detection datasets.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c9499",
   "metadata": {},
   "source": [
    "## Enhance the RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88c906b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.1, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    # 1. Retrieve documents\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    \n",
    "    if not results:\n",
    "        return {\n",
    "            'answer': 'No relevant context found.',\n",
    "            'sources': [],\n",
    "            'confidence': 0.0,\n",
    "            'context': ''\n",
    "        }\n",
    "    \n",
    "    # 2. Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    \n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...' if len(doc['content']) > 300 else doc['content']\n",
    "    } for doc in results]\n",
    "    \n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # 3. Generate answer\n",
    "    prompt_text = f\"\"\"\n",
    "    Use the following context to answer the question concisely.\n",
    "    \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get LLM response\n",
    "    response = llm.invoke(prompt_text)\n",
    "    \n",
    "    # 4. Extract answer text based on response type\n",
    "    if isinstance(response, str):\n",
    "        answer = response\n",
    "    elif hasattr(response, 'content'):\n",
    "        answer = response.content\n",
    "    elif hasattr(response, 'text'):\n",
    "        answer = response.text\n",
    "    else:\n",
    "        answer = str(response)\n",
    "    \n",
    "    # 5. Prepare output\n",
    "    output = {\n",
    "        'answer': answer.strip(),\n",
    "        'sources': sources,\n",
    "        'confidence': float(confidence)  # Ensure it's a float\n",
    "    }\n",
    "    \n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e418ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Rich feature hierarchies for accurate object detection'\n",
      "Top K: 5, Score threshold: 0.1\n",
      "Generating Embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_advanced(\"What is Rich feature hierarchies for accurate object detection\",rag_retriever,llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "903056fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Rich Feature Hierarchies for Accurate Object Detection refers to an approach proposed by Ross Girshick et al. in their seminal work, which combines high-capacity convolutional neural networks (CNNs) with bottom-up region proposals to improve object detection performance. This approach achieves a significant improvement over prior state-of-the-art methods, particularly on the PASCAL VOC 2012 dataset.',\n",
       " 'sources': [{'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf',\n",
       "   'page': 9,\n",
       "   'score': 0.38349807262420654,\n",
       "   'preview': 'Further Exploration\\nThe techniques introduced in R-CNN, particularly region proposals and fine-\\ntuning CNNs for detection, remain highly influential.\\n\"Rich feature hierarchies for accurate object detection and semantic \\nsegmentation\" (R. Girshick et al.)\\nWe encourage you to explore the original rese...'},\n",
       "  {'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'page': 0,\n",
       "   'score': 0.21446001529693604,\n",
       "   'preview': 'Rich feature hierarchies for accurate object detection and semantic segmentation\\nTech report (v5)\\nRoss Girshick Jeff Donahue Trevor Darrell Jitendra Malik\\nUC Berkeley\\n{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu\\nAbstract\\nObject detection performance, as measured on the\\ncanonical PASCAL VOC dataset,...'},\n",
       "  {'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'page': 0,\n",
       "   'score': 0.15330874919891357,\n",
       "   'preview': 'Figure 1: Object detection system overview. Our system (1)\\ntakes an input image, (2) extracts around 2000 bottom-up region\\nproposals, (3) computes features for each proposal using a large\\nconvolutional neural network (CNN), and then (4) classiﬁes each\\nregion using class-speciﬁc linear SVMs. R-CNN ac...'},\n",
       "  {'source': '..\\\\data\\\\pdf\\\\1311.2524v5.pdf',\n",
       "   'page': 13,\n",
       "   'score': 0.13106554746627808,\n",
       "   'preview': 'age search. In Proc. of the ACM International Conference on\\nImage and Video Retrieval, 2009. 13\\n[14] I. Endres and D. Hoiem. Category independent object pro-\\nposals. In ECCV, 2010. 3\\n[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\\nA. Zisserman. The PASCAL Visual Object Classes (VOC)...'},\n",
       "  {'source': '..\\\\data\\\\pdf\\\\R-CNN-Revolutionizing-Object-Detection.pdf',\n",
       "   'page': 8,\n",
       "   'score': 0.11804437637329102,\n",
       "   'preview': 'Impact & Legacy: A New Era of Detection\\n53.7%\\nmAP Improvement\\nR-CNN achieved a remarkable 53.7% mean Average Precision \\n(mAP) on the Pascal VOC 2012 dataset, a massive 30% relative \\nimprovement over prior state-of-the-art methods.\\n4\\nFoundational Work\\nThis groundbreaking performance cemented R-CNN as...'}],\n",
       " 'confidence': 0.38349807262420654}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e42a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
